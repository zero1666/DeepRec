{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaeb7060-a190-43e3-a652-f578b4ce1956",
   "metadata": {},
   "source": [
    "## DSSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21084bbf-8032-4df1-a78c-7668c4be7ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import namedtuple, OrderedDict\n",
    "from copy import copy\n",
    "from itertools import chain\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46850b0b-98f4-483f-96fd-b0769cff6d50",
   "metadata": {},
   "source": [
    "### moivelens 1M 数据集处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1618c5a-7b84-4ba4-9b21-3decc2e2b3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"../../data/ml-1m/\"\n",
    "output_dir = \"../../data/ml-1m/dssm/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a3950c-5a64-4a5b-9669-bf4ecf0b2fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_path = os.path.join(input_dir, \"users.dat\")\n",
    "movies_path = os.path.join(input_dir, \"movies.dat\")\n",
    "ratings_path = os.path.join(input_dir, \"ratings.dat\")\n",
    "\n",
    "users = pd.read_csv(users_path, sep=\"::\", header=None, engine=\"python\",encoding=\"latin1\", names=\"UserID::Gender::Age::Occupation::Zip-code\".split(\"::\"))\n",
    "movies = pd.read_csv(movies_path, sep=\"::\", header=None, engine=\"python\",encoding=\"latin1\", names=\"MovieID::Title::Genres\".split(\"::\"))\n",
    "ratings = pd.read_csv(ratings_path, sep=\"::\", header=None, engine=\"python\",encoding=\"latin1\", names=\"UserID::MovieID::Rating::Timestamp\".split(\"::\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688fd02a-3ec3-40de-939c-a6b88a18d542",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"users:\", type(users), users.shape, '\\n', users.head(5))\n",
    "print(\"movies:\", type(movies), movies.shape, '\\n', movies.head(5))\n",
    "print(\"ratings:\", type(ratings), ratings.shape, '\\n', ratings.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9938dc-8916-49f1-97c6-4a3b2a4f952d",
   "metadata": {},
   "source": [
    "### 广告数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "812e7130-d72d-4aa1-8eb7-e008fffb059b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_data_file = \"../../data/tx_ad_data/train_sample.csv\"\n",
    "ad_data = pd.read_csv(ad_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "526f6731-26d8-44b2-9631-3460699b7bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ad_data: (100000, 3) \n",
      "     aid       uid  label\n",
      "0   411  40083340      0\n",
      "1  1119  28450328      0\n",
      "2   875  13700924      0\n",
      "3  1566  45588256      0\n",
      "4  1749  18791606      0\n"
     ]
    }
   ],
   "source": [
    "ad_data['label'] = ad_data['label'].replace(-1, 0)\n",
    "print('ad_data:', ad_data.shape, '\\n', ad_data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "719207ee-2292-4682-9402-e936a2b10f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_features = ['aid', 'uid']\n",
    "dense_features = []\n",
    "\n",
    "user_features=['aid']\n",
    "item_features = ['uid']\n",
    "target = ['label']\n",
    "\n",
    "ad_data[sparse_features] = ad_data[sparse_features].fillna('-1',)\n",
    "ad_data[dense_features] = ad_data[dense_features].fillna(0, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46629f55-4ca2-4b51-956a-d2901febfe9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative_Sample\n",
    "from collections import OrderedDict, Counter\n",
    "def Negative_Sample(data, user_col, item_col, label_col, ratio, method_id=2):\n",
    "    \"\"\"\n",
    "    :param data: training data\n",
    "    :param user_col: user column name\n",
    "    :param item_col: item column name for negative sampling\n",
    "    :param label_col: label column name\n",
    "    :param ratio: negative sample ratio, >= 1\n",
    "    :param method_id: {0 : \"random sampling\", 1: \"sampling method used in word2vec\", 2: \"tencent RALM sampling\"}\n",
    "    :return: new_dataframe, (user_id, item_id, label)\n",
    "    \"\"\"\n",
    "    if not isinstance(ratio, int) or ratio < 1:\n",
    "        raise ValueError(\"ratio means neg/pos, it should be greater than or equal to 1\")\n",
    "    items_cnt = Counter(data[item_col])\n",
    "    items_cnt_order = OrderedDict(sorted((items_cnt.items()), key=lambda x:x[1], reverse=True))\n",
    "    #print(items_cnt_order)\n",
    "    user_pos_item = data[data[label_col]==1].drop(label_col, axis=1).groupby(user_col).agg(list).reset_index()\n",
    "    \n",
    "    if method_id == 0:\n",
    "        def sample(row):\n",
    "            neg_items = np.random.choice(list(items_cnt.keys()), size=ratio, replace=False)\n",
    "            neg_items = [neg for neg in neg_items if neg not in row[item_col]]\n",
    "            return neg_items\n",
    "        user_pos_item['neg_' + item_col] = user_pos_item.apply(sample, axis=1)\n",
    "    elif method_id == 1:\n",
    "        items_cnt_freq = {item: count/len(items_cnt) for item,count in items_cnt_order.items()}\n",
    "        p_sel = {item: np.sqrt(1e-5/items_cnt_freq[item]) for item in items_cnt_order}\n",
    "        p_value = np.array(list(p_sel.values()))/sum(p_sel.values())\n",
    "        def sample(row):\n",
    "            neg_items = np.random.choice(list(items_cnt.keys()), size=ratio, replace=False, p=p_value)\n",
    "            neg_items = [neg for neg in neg_items if neg not in row[item_col]]\n",
    "            return neg_items\n",
    "        user_pos_item['neg_' + item_col] = user_pos_item.apply(sample, axis=1)\n",
    "    elif method_id == 2:\n",
    "        p_sel = {item: (np.log(k + 2) - np.log(k + 1) / np.log(len(items_cnt_order) + 1)) for item, k in\n",
    "                 items_cnt_order.items()}\n",
    "        p_value = np.array(list(p_sel.values())) / sum(p_sel.values())\n",
    "        def sample(row):\n",
    "            neg_items = np.random.choice(list(items_cnt.keys()), size=ratio, replace=False, p=p_value)\n",
    "            neg_items = [neg for neg in neg_items if neg not in row[item_col]]\n",
    "            return neg_items\n",
    "        user_pos_item['neg_'+item_col] = user_pos_item.apply(sample, axis=1)\n",
    "    else:\n",
    "        raise ValueError(\"method id should in (0,1,2)\")\n",
    "        \n",
    "    #print(user_pos_item)\n",
    "\n",
    "    neg_data = pd.DataFrame({user_col: user_pos_item[user_col], 'neg_'+item_col: user_pos_item['neg_'+item_col]})\n",
    "    #print('neg_data', neg_data)\n",
    "    neg_data = neg_data.rename(columns={'neg_' + item_col: item_col}, inplace=False)\n",
    "    #print('neg_data', neg_data)\n",
    "\n",
    "    pos_data = pd.DataFrame({user_col: user_pos_item[user_col], item_col: user_pos_item[item_col]})\n",
    "    #print('pos_data', pos_data)\n",
    "\n",
    "    pos_data[label_col] = 1\n",
    "    neg_data[label_col] = 0\n",
    "    neg_data = neg_data.explode('uid')\n",
    "    \n",
    "    pos_data = pos_data.explode('uid')\n",
    "    #print('neg_data', neg_data)\n",
    "    #print('pos_data', pos_data)\n",
    "\n",
    "    return pd.concat([pos_data, neg_data])\n",
    "\n",
    "def Cosine_Similarity(query, candidate, gamma=1, axis=-1):\n",
    "    query_norm = tf.norm(query, axis=axis)\n",
    "    candidate_norm = tf.norm(candidate, axis=axis)\n",
    "    cosine_score = tf.reduce_sum(tf.multiply(query, candidate), -1)\n",
    "    cosine_score = tf.divide(cosine_score, query_norm*candidate_norm+1e-8)\n",
    "    cosine_score = tf.clip_by_value(cosine_score, -1, 1.0)*gamma\n",
    "    return cosine_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de7ba6a5-d6fe-41c2-81ed-f81ae6453bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs_list(inputs):\n",
    "    return list(chain(*list(map(lambda x: x.values(), filter(lambda x: x is not None, inputs)))))\n",
    "\n",
    "\n",
    "def create_embedding_dict(sparse_feature_columns, varlen_sparse_feature_columns, seed, l2_reg,\n",
    "                          prefix='sparse_', seq_mask_zero=True):\n",
    "    sparse_embedding = {}\n",
    "    for feat in sparse_feature_columns:\n",
    "        emb = tf.keras.layers.Embedding(feat.vocabulary_size, feat.embedding_dim,\n",
    "                        embeddings_initializer=feat.embeddings_initializer,\n",
    "                        embeddings_regularizer=l2(l2_reg),\n",
    "                        name=prefix + '_emb_' + feat.embedding_name)\n",
    "        emb.trainable = feat.trainable\n",
    "        sparse_embedding[feat.embedding_name] = emb\n",
    "\n",
    "    if varlen_sparse_feature_columns and len(varlen_sparse_feature_columns) > 0:\n",
    "        for feat in varlen_sparse_feature_columns:\n",
    "            # if feat.name not in sparse_embedding:\n",
    "            emb = tf.keras.layers.Embedding(feat.vocabulary_size, feat.embedding_dim,\n",
    "                            embeddings_initializer=feat.embeddings_initializer,\n",
    "                            embeddings_regularizer=l2(\n",
    "                                l2_reg),\n",
    "                            name=prefix + '_seq_emb_' + feat.name,\n",
    "                            mask_zero=seq_mask_zero)\n",
    "            emb.trainable = feat.trainable\n",
    "            sparse_embedding[feat.embedding_name] = emb\n",
    "    return sparse_embedding\n",
    "\n",
    "def get_embedding_vec_list(embedding_dict, input_dict, sparse_feature_columns, return_feat_list=(), mask_feat_list=()):\n",
    "    embedding_vec_list = []\n",
    "    for fg in sparse_feature_columns:\n",
    "        feat_name = fg.name\n",
    "        if len(return_feat_list) == 0 or feat_name in return_feat_list:\n",
    "            if fg.use_hash:\n",
    "                lookup_idx = Hash(fg.vocabulary_size, mask_zero=(feat_name in mask_feat_list), vocabulary_path=fg.vocabulary_path)(input_dict[feat_name])\n",
    "            else:\n",
    "                lookup_idx = input_dict[feat_name]\n",
    "\n",
    "            embedding_vec_list.append(embedding_dict[feat_name](lookup_idx))\n",
    "\n",
    "    return embedding_vec_list\n",
    "\n",
    "\n",
    "def create_embedding_matrix(feature_columns, l2_reg, seed, prefix=\"\", seq_mask_zero=True):\n",
    "\n",
    "    sparse_feature_columns = list(\n",
    "        filter(lambda x: isinstance(x, SparseFeat), feature_columns)) if feature_columns else []\n",
    "    varlen_sparse_feature_columns = list(\n",
    "        filter(lambda x: isinstance(x, VarLenSparseFeat), feature_columns)) if feature_columns else []\n",
    "    \n",
    "    sparse_emb_dict = create_embedding_dict(sparse_feature_columns, varlen_sparse_feature_columns, seed,\n",
    "                                            l2_reg, prefix=str(prefix) + \"sparse\", seq_mask_zero=seq_mask_zero)\n",
    "    return sparse_emb_dict\n",
    "\n",
    "\n",
    "def embedding_lookup(sparse_embedding_dict, sparse_input_dict, sparse_feature_columns, return_feat_list=(),\n",
    "                     mask_feat_list=(), to_list=False):\n",
    "    group_embedding_dict = defaultdict(list)\n",
    "    for fc in sparse_feature_columns:\n",
    "        feature_name = fc.name\n",
    "        embedding_name = fc.embedding_name\n",
    "        if (len(return_feat_list) == 0 or feature_name in return_feat_list):\n",
    "            if fc.use_hash:\n",
    "                lookup_idx = Hash(fc.vocabulary_size, mask_zero=(feature_name in mask_feat_list), vocabulary_path=fc.vocabulary_path)(\n",
    "                    sparse_input_dict[feature_name])\n",
    "            else:\n",
    "                lookup_idx = sparse_input_dict[feature_name]\n",
    "\n",
    "            group_embedding_dict[fc.group_name].append(sparse_embedding_dict[embedding_name](lookup_idx))\n",
    "    if to_list:\n",
    "        return list(chain.from_iterable(group_embedding_dict.values()))\n",
    "    return group_embedding_dict\n",
    "\n",
    "\n",
    "def varlen_embedding_lookup(embedding_dict, sequence_input_dict, varlen_sparse_feature_columns):\n",
    "    varlen_embedding_vec_dict = {}\n",
    "    for fc in varlen_sparse_feature_columns:\n",
    "        feature_name = fc.name\n",
    "        embedding_name = fc.embedding_name\n",
    "        if fc.use_hash:\n",
    "            lookup_idx = Hash(fc.vocabulary_size, mask_zero=True, vocabulary_path=fc.vocabulary_path)(sequence_input_dict[feature_name])\n",
    "        else:\n",
    "            lookup_idx = sequence_input_dict[feature_name]\n",
    "        varlen_embedding_vec_dict[feature_name] = embedding_dict[embedding_name](lookup_idx)\n",
    "    return varlen_embedding_vec_dict\n",
    "\n",
    "\n",
    "def get_varlen_pooling_list(embedding_dict, features, varlen_sparse_feature_columns, to_list=False):\n",
    "    pooling_vec_list = defaultdict(list)\n",
    "    for fc in varlen_sparse_feature_columns:\n",
    "        feature_name = fc.name\n",
    "        combiner = fc.combiner\n",
    "        feature_length_name = fc.length_name\n",
    "        if feature_length_name is not None:\n",
    "            if fc.weight_name is not None:\n",
    "                seq_input = WeightedSequenceLayer(weight_normalization=fc.weight_norm)(\n",
    "                    [embedding_dict[feature_name], features[feature_length_name], features[fc.weight_name]])\n",
    "            else:\n",
    "                seq_input = embedding_dict[feature_name]\n",
    "            vec = SequencePoolingLayer(combiner, supports_masking=False)(\n",
    "                [seq_input, features[feature_length_name]])\n",
    "        else:\n",
    "            if fc.weight_name is not None:\n",
    "                seq_input = WeightedSequenceLayer(weight_normalization=fc.weight_norm, supports_masking=True)(\n",
    "                    [embedding_dict[feature_name], features[fc.weight_name]])\n",
    "            else:\n",
    "                seq_input = embedding_dict[feature_name]\n",
    "            vec = SequencePoolingLayer(combiner, supports_masking=True)(\n",
    "                seq_input)\n",
    "        pooling_vec_list[fc.group_name].append(vec)\n",
    "    if to_list:\n",
    "        return chain.from_iterable(pooling_vec_list.values())\n",
    "    return pooling_vec_list\n",
    "\n",
    "\n",
    "def get_dense_input(features, feature_columns):\n",
    "    \n",
    "    dense_feature_columns = list(\n",
    "        filter(lambda x: isinstance(x, DenseFeat), feature_columns)) if feature_columns else []\n",
    "    dense_input_list = []\n",
    "    for fc in dense_feature_columns:\n",
    "        if fc.transform_fn is None:\n",
    "            dense_input_list.append(features[fc.name])\n",
    "        else:\n",
    "            transform_result = tf.keras.initializers.Lambda(fc.transform_fn)(features[fc.name])\n",
    "            dense_input_list.append(transform_result)\n",
    "    return dense_input_list\n",
    "\n",
    "\n",
    "def mergeDict(a, b):\n",
    "    c = defaultdict(list)\n",
    "    for k, v in a.items():\n",
    "        c[k].extend(v)\n",
    "    for k, v in b.items():\n",
    "        c[k].extend(v)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "529d3b1c-a7db-4c99-8b15-6cb718734441",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_GROUP_NAME = \"default_group\"\n",
    "\n",
    "\n",
    "class SparseFeat(namedtuple('SparseFeat',\n",
    "                            ['name', 'vocabulary_size', 'embedding_dim', 'use_hash', 'vocabulary_path', 'dtype', 'embeddings_initializer',\n",
    "                             'embedding_name',\n",
    "                             'group_name', 'trainable'])):\n",
    "    __slots__ = ()\n",
    "\n",
    "    def __new__(cls, name, vocabulary_size, embedding_dim=4, use_hash=False, vocabulary_path=None, dtype=\"int32\", embeddings_initializer=None,\n",
    "                embedding_name=None,\n",
    "                group_name=DEFAULT_GROUP_NAME, trainable=True):\n",
    "\n",
    "        if embedding_dim == \"auto\":\n",
    "            embedding_dim = 6 * int(pow(vocabulary_size, 0.25))\n",
    "        if embeddings_initializer is None:\n",
    "            embeddings_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.0001, seed=2020)\n",
    "\n",
    "        if embedding_name is None:\n",
    "            embedding_name = name\n",
    "\n",
    "        return super(SparseFeat, cls).__new__(cls, name, vocabulary_size, embedding_dim, use_hash, vocabulary_path, dtype,\n",
    "                                              embeddings_initializer,\n",
    "                                              embedding_name, group_name, trainable)\n",
    "\n",
    "    def __hash__(self):\n",
    "        return self.name.__hash__()\n",
    "\n",
    "\n",
    "class VarLenSparseFeat(namedtuple('VarLenSparseFeat',\n",
    "                                  ['sparsefeat', 'maxlen', 'combiner', 'length_name', 'weight_name', 'weight_norm'])):\n",
    "    __slots__ = ()\n",
    "\n",
    "    def __new__(cls, sparsefeat, maxlen, combiner=\"mean\", length_name=None, weight_name=None, weight_norm=True):\n",
    "        return super(VarLenSparseFeat, cls).__new__(cls, sparsefeat, maxlen, combiner, length_name, weight_name,\n",
    "                                                    weight_norm)\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self.sparsefeat.name\n",
    "\n",
    "    @property\n",
    "    def vocabulary_size(self):\n",
    "        return self.sparsefeat.vocabulary_size\n",
    "\n",
    "    @property\n",
    "    def embedding_dim(self):\n",
    "        return self.sparsefeat.embedding_dim\n",
    "\n",
    "    @property\n",
    "    def use_hash(self):\n",
    "        return self.sparsefeat.use_hash\n",
    "\n",
    "    @property\n",
    "    def vocabulary_path(self):\n",
    "        return self.sparsefeat.vocabulary_path\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self.sparsefeat.dtype\n",
    "\n",
    "    @property\n",
    "    def embeddings_initializer(self):\n",
    "        return self.sparsefeat.embeddings_initializer\n",
    "\n",
    "    @property\n",
    "    def embedding_name(self):\n",
    "        return self.sparsefeat.embedding_name\n",
    "\n",
    "    @property\n",
    "    def group_name(self):\n",
    "        return self.sparsefeat.group_name\n",
    "\n",
    "    @property\n",
    "    def trainable(self):\n",
    "        return self.sparsefeat.trainable\n",
    "\n",
    "    def __hash__(self):\n",
    "        return self.name.__hash__()\n",
    "\n",
    "\n",
    "class DenseFeat(namedtuple('DenseFeat', ['name', 'dimension', 'dtype', 'transform_fn'])):\n",
    "    \"\"\" Dense feature\n",
    "    Args:\n",
    "        name: feature name,\n",
    "        dimension: dimension of the feature, default = 1.\n",
    "        dtype: dtype of the feature, default=\"float32\".\n",
    "        transform_fn: If not `None` , a function that can be used to transform\n",
    "        values of the feature.  the function takes the input Tensor as its\n",
    "        argument, and returns the output Tensor.\n",
    "        (e.g. lambda x: (x - 3.0) / 4.2).\n",
    "    \"\"\"\n",
    "    __slots__ = ()\n",
    "\n",
    "    def __new__(cls, name, dimension=1, dtype=\"float32\", transform_fn=None):\n",
    "        return super(DenseFeat, cls).__new__(cls, name, dimension, dtype, transform_fn)\n",
    "\n",
    "    def __hash__(self):\n",
    "        return self.name.__hash__()\n",
    "\n",
    "    # def __eq__(self, other):\n",
    "    #     if self.name == other.name:\n",
    "    #         return True\n",
    "    #     return False\n",
    "\n",
    "    # def __repr__(self):\n",
    "    #     return 'DenseFeat:'+self.name\n",
    "\n",
    "\n",
    "def get_feature_names(feature_columns):\n",
    "    features = build_input_features(feature_columns)\n",
    "    return list(features.keys())\n",
    "\n",
    "\n",
    "def build_input_features(feature_columns, prefix=''):\n",
    "    input_features = OrderedDict()\n",
    "    for fc in feature_columns:\n",
    "        if isinstance(fc, SparseFeat):\n",
    "            input_features[fc.name] = tf.keras.Input(\n",
    "                shape=(1,), name=prefix + fc.name, dtype=fc.dtype)\n",
    "        elif isinstance(fc, DenseFeat):\n",
    "            input_features[fc.name] = tf.keras.Input(\n",
    "                shape=(fc.dimension,), name=prefix + fc.name, dtype=fc.dtype)\n",
    "        elif isinstance(fc, VarLenSparseFeat):\n",
    "            input_features[fc.name] = tf.keras.Input(shape=(fc.maxlen,), name=prefix + fc.name,\n",
    "                                            dtype=fc.dtype)\n",
    "            if fc.weight_name is not None:\n",
    "                input_features[fc.weight_name] = tf.keras.Input(shape=(fc.maxlen, 1), name=prefix + fc.weight_name,\n",
    "                                                       dtype=\"float32\")\n",
    "            if fc.length_name is not None:\n",
    "                input_features[fc.length_name] = tf.keras.Input((1,), name=prefix + fc.length_name, dtype='int32')\n",
    "\n",
    "        else:\n",
    "            raise TypeError(\"Invalid feature column type,got\", type(fc))\n",
    "\n",
    "    return input_features\n",
    "\n",
    "\n",
    "def get_linear_logit(features, feature_columns, units=1, use_bias=False, seed=1024, prefix='linear',\n",
    "                     l2_reg=0, sparse_feat_refine_weight=None):\n",
    "    linear_feature_columns = copy(feature_columns)\n",
    "    for i in range(len(linear_feature_columns)):\n",
    "        if isinstance(linear_feature_columns[i], SparseFeat):\n",
    "            linear_feature_columns[i] = linear_feature_columns[i]._replace(embedding_dim=1,\n",
    "                                                                           embeddings_initializer=tf.keras.initializers.Zeros())\n",
    "        if isinstance(linear_feature_columns[i], VarLenSparseFeat):\n",
    "            linear_feature_columns[i] = linear_feature_columns[i]._replace(\n",
    "                sparsefeat=linear_feature_columns[i].sparsefeat._replace(embedding_dim=1,\n",
    "                                                                         embeddings_initializer=tf.keras.initializers.Zeros()))\n",
    "\n",
    "    linear_emb_list = [input_from_feature_columns(features, linear_feature_columns, l2_reg, seed,\n",
    "                                                  prefix=prefix + str(i))[0] for i in range(units)]\n",
    "    _, dense_input_list = input_from_feature_columns(features, linear_feature_columns, l2_reg, seed, prefix=prefix)\n",
    "\n",
    "    linear_logit_list = []\n",
    "    for i in range(units):\n",
    "\n",
    "        if len(linear_emb_list[i]) > 0 and len(dense_input_list) > 0:\n",
    "            sparse_input = concat_func(linear_emb_list[i])\n",
    "            dense_input = concat_func(dense_input_list)\n",
    "            if sparse_feat_refine_weight is not None:\n",
    "                sparse_input = tf.keras.initializers.Lambda(lambda x: x[0] * tf.expand_dims(x[1], axis=1))(\n",
    "                    [sparse_input, sparse_feat_refine_weight])\n",
    "            linear_logit = Linear(l2_reg, mode=2, use_bias=use_bias, seed=seed)([sparse_input, dense_input])\n",
    "        elif len(linear_emb_list[i]) > 0:\n",
    "            sparse_input = concat_func(linear_emb_list[i])\n",
    "            if sparse_feat_refine_weight is not None:\n",
    "                sparse_input = tf.keras.initializers.Lambda(lambda x: x[0] * tf.expand_dims(x[1], axis=1))(\n",
    "                    [sparse_input, sparse_feat_refine_weight])\n",
    "            linear_logit = Linear(l2_reg, mode=0, use_bias=use_bias, seed=seed)(sparse_input)\n",
    "        elif len(dense_input_list) > 0:\n",
    "            dense_input = concat_func(dense_input_list)\n",
    "            linear_logit = Linear(l2_reg, mode=1, use_bias=use_bias, seed=seed)(dense_input)\n",
    "        else:   #empty feature_columns\n",
    "            return tf.keras.initializers.Lambda(lambda x: tf.constant([[0.0]]))(list(features.values())[0])\n",
    "        linear_logit_list.append(linear_logit)\n",
    "\n",
    "    return concat_func(linear_logit_list)\n",
    "\n",
    "\n",
    "def input_from_feature_columns(features, feature_columns, l2_reg, seed, prefix='', seq_mask_zero=True,\n",
    "                               support_dense=True, support_group=False):\n",
    "    sparse_feature_columns = list(\n",
    "        filter(lambda x: isinstance(x, SparseFeat), feature_columns)) if feature_columns else []\n",
    "    varlen_sparse_feature_columns = list(\n",
    "        filter(lambda x: isinstance(x, VarLenSparseFeat), feature_columns)) if feature_columns else []\n",
    "\n",
    "    embedding_matrix_dict = create_embedding_matrix(feature_columns, l2_reg, seed, prefix=prefix,\n",
    "                                                    seq_mask_zero=seq_mask_zero)\n",
    "    group_sparse_embedding_dict = embedding_lookup(embedding_matrix_dict, features, sparse_feature_columns)\n",
    "    dense_value_list = get_dense_input(features, feature_columns)\n",
    "    if not support_dense and len(dense_value_list) > 0:\n",
    "        raise ValueError(\"DenseFeat is not supported in dnn_feature_columns\")\n",
    "\n",
    "    sequence_embed_dict = varlen_embedding_lookup(embedding_matrix_dict, features, varlen_sparse_feature_columns)\n",
    "    group_varlen_sparse_embedding_dict = get_varlen_pooling_list(sequence_embed_dict, features,\n",
    "                                                                 varlen_sparse_feature_columns)\n",
    "    group_embedding_dict = mergeDict(group_sparse_embedding_dict, group_varlen_sparse_embedding_dict)\n",
    "    if not support_group:\n",
    "        group_embedding_dict = list(chain.from_iterable(group_embedding_dict.values()))\n",
    "    return group_embedding_dict, dense_value_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "228caad3-b98a-4f34-8a75-2d8478252700",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "try:\n",
    "    from tensorflow.python.ops.init_ops import Zeros\n",
    "except ImportError:\n",
    "    from tensorflow.python.ops.init_ops_v2 import Zeros\n",
    "from tensorflow.python.keras.layers import Layer, Activation\n",
    "\n",
    "try:\n",
    "    from tensorflow.python.keras.layers import BatchNormalization\n",
    "except ImportError:\n",
    "    BatchNormalization = tf.keras.layers.BatchNormalization\n",
    "\n",
    "try:\n",
    "    unicode\n",
    "except NameError:\n",
    "    unicode = str\n",
    "\n",
    "\n",
    "class Dice(Layer):\n",
    "    \"\"\"The Data Adaptive Activation Function in DIN,which can be viewed as a generalization of PReLu and can adaptively adjust the rectified point according to distribution of input data.\n",
    "      Input shape\n",
    "        - Arbitrary. Use the keyword argument `input_shape` (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model.\n",
    "      Output shape\n",
    "        - Same shape as the input.\n",
    "      Arguments\n",
    "        - **axis** : Integer, the axis that should be used to compute data distribution (typically the features axis).\n",
    "        - **epsilon** : Small float added to variance to avoid dividing by zero.\n",
    "      References\n",
    "        - [Zhou G, Zhu X, Song C, et al. Deep interest network for click-through rate prediction[C]//Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. ACM, 2018: 1059-1068.](https://arxiv.org/pdf/1706.06978.pdf)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, axis=-1, epsilon=1e-9, **kwargs):\n",
    "        self.axis = axis\n",
    "        self.epsilon = epsilon\n",
    "        super(Dice, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.bn = BatchNormalization(\n",
    "            axis=self.axis, epsilon=self.epsilon, center=False, scale=False)\n",
    "        self.alphas = self.add_weight(shape=(input_shape[-1],), initializer=Zeros(\n",
    "        ), dtype=tf.float32, name='dice_alpha')  # name='alpha_'+self.name\n",
    "        super(Dice, self).build(input_shape)  # Be sure to call this somewhere!\n",
    "        self.uses_learning_phase = True\n",
    "\n",
    "    def call(self, inputs, training=None, **kwargs):\n",
    "        inputs_normed = self.bn(inputs, training=training)\n",
    "        # tf.layers.batch_normalization(\n",
    "        # inputs, axis=self.axis, epsilon=self.epsilon, center=False, scale=False)\n",
    "        x_p = tf.sigmoid(inputs_normed)\n",
    "        return self.alphas * (1.0 - x_p) * inputs + x_p * inputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    def get_config(self, ):\n",
    "        config = {'axis': self.axis, 'epsilon': self.epsilon}\n",
    "        base_config = super(Dice, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "def activation_layer(activation):\n",
    "    print('activation_layer',activation)\n",
    "    if activation in (\"dice\", \"Dice\"):\n",
    "        act_layer = Dice()\n",
    "    elif isinstance(activation, (str, unicode)):\n",
    "        act_layer = Activation(activation)\n",
    "    elif issubclass(activation, Layer):\n",
    "        act_layer = activation()\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Invalid activation,found %s.You should use a str or a Activation Layer Class.\" % (activation))\n",
    "    return act_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa91ed6c-19d0-408b-bf70-1c0ade47e62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "try:\n",
    "    from tensorflow.python.ops.init_ops_v2 import Zeros, glorot_normal\n",
    "except ImportError:\n",
    "    from tensorflow.python.ops.init_ops import Zeros, glorot_normal_initializer as glorot_normal\n",
    "\n",
    "from tensorflow.keras.layers import Layer, Dropout\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "from tensorflow.python.keras.regularizers import l2\n",
    "\n",
    "class DNN(Layer):\n",
    "    \"\"\"The Multi Layer Percetron\n",
    "      Input shape\n",
    "        - nD tensor with shape: ``(batch_size, ..., input_dim)``. The most common situation would be a 2D input with shape ``(batch_size, input_dim)``.\n",
    "      Output shape\n",
    "        - nD tensor with shape: ``(batch_size, ..., hidden_size[-1])``. For instance, for a 2D input with shape ``(batch_size, input_dim)``, the output would have shape ``(batch_size, hidden_size[-1])``.\n",
    "      Arguments\n",
    "        - **hidden_units**:list of positive integer, the layer number and units in each layer.\n",
    "        - **activation**: Activation function to use.\n",
    "        - **l2_reg**: float between 0 and 1. L2 regularizer strength applied to the kernel weights matrix.\n",
    "        - **dropout_rate**: float in [0,1). Fraction of the units to dropout.\n",
    "        - **use_bn**: bool. Whether use BatchNormalization before activation or not.\n",
    "        - **output_activation**: Activation function to use in the last layer.If ``None``,it will be same as ``activation``.\n",
    "        - **seed**: A Python integer to use as random seed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_units, activation='relu', l2_reg=0, dropout_rate=0, use_bn=False, output_activation=None,\n",
    "                 seed=1024, **kwargs):\n",
    "        self.hidden_units = hidden_units\n",
    "        self.activation = activation\n",
    "        self.l2_reg = l2_reg\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.use_bn = use_bn\n",
    "        self.output_activation = output_activation\n",
    "        self.seed = seed\n",
    "\n",
    "        super(DNN, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # if len(self.hidden_units) == 0:\n",
    "        #     raise ValueError(\"hidden_units is empty\")\n",
    "        input_size = input_shape[-1]\n",
    "        hidden_units = [int(input_size)] + list(self.hidden_units)\n",
    "        self.kernels = [self.add_weight(name='kernel' + str(i),\n",
    "                                        shape=(\n",
    "                                            hidden_units[i], hidden_units[i + 1]),\n",
    "                                        initializer=glorot_normal(\n",
    "                                            seed=self.seed),\n",
    "                                        regularizer=l2(self.l2_reg),\n",
    "                                        trainable=True) for i in range(len(self.hidden_units))]\n",
    "        self.bias = [self.add_weight(name='bias' + str(i),\n",
    "                                     shape=(self.hidden_units[i],),\n",
    "                                     initializer=Zeros(),\n",
    "                                     trainable=True) for i in range(len(self.hidden_units))]\n",
    "        if self.use_bn:\n",
    "            self.bn_layers = [BatchNormalization() for _ in range(len(self.hidden_units))]\n",
    "\n",
    "        self.dropout_layers = [Dropout(self.dropout_rate, seed=self.seed + i) for i in\n",
    "                               range(len(self.hidden_units))]\n",
    "\n",
    "        self.activation_layers = [activation_layer(self.activation) for _ in range(len(self.hidden_units))]\n",
    "\n",
    "        if self.output_activation:\n",
    "            self.activation_layers[-1] = activation_layer(self.output_activation)\n",
    "\n",
    "        super(DNN, self).build(input_shape)  # Be sure to call this somewhere!\n",
    "\n",
    "    def call(self, inputs, training=None, **kwargs):\n",
    "\n",
    "        deep_input = inputs\n",
    "\n",
    "        for i in range(len(self.hidden_units)):\n",
    "            fc = tf.nn.bias_add(tf.tensordot(\n",
    "                deep_input, self.kernels[i], axes=(-1, 0)), self.bias[i])\n",
    "\n",
    "            if self.use_bn:\n",
    "                fc = self.bn_layers[i](fc, training=training)\n",
    "            try:\n",
    "                fc = self.activation_layers[i](fc, training=training)\n",
    "            except TypeError as e:  # TypeError: call() got an unexpected keyword argument 'training'\n",
    "                print(\"make sure the activation function use training flag properly\", e)\n",
    "                fc = self.activation_layers[i](fc)\n",
    "\n",
    "            fc = self.dropout_layers[i](fc, training=training)\n",
    "            deep_input = fc\n",
    "\n",
    "        return deep_input\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if len(self.hidden_units) > 0:\n",
    "            shape = input_shape[:-1] + (self.hidden_units[-1],)\n",
    "        else:\n",
    "            shape = input_shape\n",
    "\n",
    "        return tuple(shape)\n",
    "\n",
    "    def get_config(self, ):\n",
    "        config = {'activation': self.activation, 'hidden_units': self.hidden_units,\n",
    "                  'l2_reg': self.l2_reg, 'use_bn': self.use_bn, 'dropout_rate': self.dropout_rate,\n",
    "                  'output_activation': self.output_activation, 'seed': self.seed}\n",
    "        base_config = super(DNN, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "class PredictionLayer(Layer):\n",
    "    \"\"\"\n",
    "      Arguments\n",
    "         - **task**: str, ``\"binary\"`` for  binary logloss or  ``\"regression\"`` for regression loss\n",
    "         - **use_bias**: bool.Whether add bias term or not.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, task='binary', use_bias=True, **kwargs):\n",
    "        if task not in [\"binary\", \"multiclass\", \"regression\"]:\n",
    "            raise ValueError(\"task must be binary,multiclass or regression\")\n",
    "        self.task = task\n",
    "        self.use_bias = use_bias\n",
    "        super(PredictionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.global_bias = self.add_weight(\n",
    "                shape=(1,), initializer=Zeros(), name=\"global_bias\")\n",
    "\n",
    "        # Be sure to call this somewhere!\n",
    "        super(PredictionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        x = inputs\n",
    "        if self.use_bias:\n",
    "            x = tf.nn.bias_add(x, self.global_bias, data_format='NHWC')\n",
    "        if self.task == \"binary\":\n",
    "            x = tf.sigmoid(x)\n",
    "\n",
    "        output = tf.reshape(x, (-1, 1))\n",
    "\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, 1)\n",
    "\n",
    "    def get_config(self, ):\n",
    "        config = {'task': self.task, 'use_bias': self.use_bias}\n",
    "        base_config = super(PredictionLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a57f2fe-6dad-412a-a589-5c5384b6cb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Flatten, Concatenate, Layer, Add\n",
    "class NoMask(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(NoMask, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Be sure to call this somewhere!\n",
    "        super(NoMask, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None, **kwargs):\n",
    "        return x\n",
    "\n",
    "    def compute_mask(self, inputs, mask):\n",
    "        return None\n",
    "\n",
    "def concat_func(inputs, axis=-1, mask=False):\n",
    "    print(len(inputs))\n",
    "    if not mask:\n",
    "        inputs = list(map(NoMask(), inputs))\n",
    "    if len(inputs) == 1:\n",
    "        return inputs[0]\n",
    "    else:\n",
    "        return Concatenate(axis=axis)(inputs)\n",
    "    \n",
    "def combined_dnn_input(sparse_embedding_list, dense_value_list):\n",
    "    if len(sparse_embedding_list) > 0 and len(dense_value_list) > 0:\n",
    "        sparse_dnn_input = Flatten()(concat_func(sparse_embedding_list))\n",
    "        dense_dnn_input = Flatten()(concat_func(dense_value_list))\n",
    "        return concat_func([sparse_dnn_input, dense_dnn_input])\n",
    "    elif len(sparse_embedding_list) > 0:\n",
    "        return Flatten()(concat_func(sparse_embedding_list))\n",
    "    elif len(dense_value_list) > 0:\n",
    "        return Flatten()(concat_func(dense_value_list))\n",
    "    else:\n",
    "        raise NotImplementedError(\"dnn_feature_columns can not be empty list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2401b0c-79c1-45ca-9f4a-b9498981974c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cbf9c1f-e68c-4bad-ad1f-e5f7a707a1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do Negative Sampler\n",
    "data = Negative_Sample(ad_data, 'aid', 'uid', 'label', 10, method_id=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91a64a66-e0e7-452f-90f0-4b3fdd8536b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.Label Encoding for sparse features,and do simple Transformation for dense features\n",
    "for feat in sparse_features:\n",
    "    lbe = LabelEncoder()\n",
    "    data[feat] = lbe.fit_transform(data[feat])\n",
    "    \n",
    "# 2.count #unique features for each sparse field,and record dense feature field name\n",
    "user_feature_columns = [SparseFeat(feat, vocabulary_size=data[feat].nunique(), embedding_dim=20) \n",
    "                        for i, feat in enumerate(user_features)]\n",
    "item_feature_columns = [SparseFeat(feat, vocabulary_size=data[feat].nunique(), embedding_dim=20)\n",
    "                           for i, feat in enumerate(item_features)]\n",
    "# 3.generate input data for model\n",
    "train, test = train_test_split(data, test_size=0.2)\n",
    "train_model_input = {name: train[name] for name in sparse_features}\n",
    "test_model_input = {name: test[name] for name in sparse_features}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5bc974cd-3fb3-4c95-8a77-e9b921904764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6480, 3)    aid   uid  label\n",
      "0    0  2880      1\n",
      "0    0   254      1\n",
      "0    0  1719      1\n",
      "0    0  6355      1\n",
      "0    0  2042      1\n",
      "<class 'list'> 1 [SparseFeat(name='aid', vocabulary_size=172, embedding_dim=20, use_hash=False, vocabulary_path=None, dtype='int32', embeddings_initializer=<keras.initializers.initializers_v2.RandomNormal object at 0x7f125d36ba50>, embedding_name='aid', group_name='default_group', trainable=True)]\n"
     ]
    }
   ],
   "source": [
    "print(data.shape, data.head(5))\n",
    "print(type(user_feature_columns), len(user_feature_columns), user_feature_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcdbcda5-d73f-4c16-9c03-a63e96711ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "\n",
    "def DSSM(user_dnn_feature_columns, item_dnn_feature_columns, gamma=1, dnn_use_bn=True, dnn_hidden_units=(300, 300, 128), dnn_activation='tanh',\n",
    "         l2_reg_dnn=0, l2_reg_embedding=1e-6, dnn_dropout=0, init_std=0.0001, seed=1024, task='binary'):\n",
    "\n",
    "    user_features = build_input_features(user_dnn_feature_columns)\n",
    "    user_inputs_list = list(user_features.values())\n",
    "    user_sparse_embedding_list, user_dense_value_list = input_from_feature_columns(user_features, user_dnn_feature_columns,\n",
    "                                                                         l2_reg_embedding, init_std, seed)\n",
    "    user_dnn_input = combined_dnn_input(user_sparse_embedding_list, user_dense_value_list)\n",
    "    #print(user_dnn_input)\n",
    "    \n",
    "    item_features = build_input_features(item_dnn_feature_columns)\n",
    "    item_inputs_list = list(item_features.values())\n",
    "    item_sparse_embedding_list, item_dense_value_list = input_from_feature_columns(item_features, item_dnn_feature_columns,\n",
    "                                                                         l2_reg_embedding, init_std, seed)\n",
    "    item_dnn_input = combined_dnn_input(item_sparse_embedding_list, item_dense_value_list)\n",
    "\n",
    "    user_dnn_out = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout,\n",
    "                  dnn_use_bn, seed=seed, name=\"user_embedding\")(user_dnn_input)\n",
    "    \n",
    "    item_dnn_out = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout,\n",
    "                  dnn_use_bn, seed=seed, name=\"item_embedding\")(item_dnn_input)\n",
    "\n",
    "    score = Cosine_Similarity(user_dnn_out, item_dnn_out, gamma=gamma)\n",
    "\n",
    "    output = PredictionLayer(task, False)(score)\n",
    "\n",
    "    model = Model(inputs=user_inputs_list+item_inputs_list, outputs=output)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2d62bc3-ef8d-405f-a3b4-34cdfc5cbe18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-24 12:14:14.467961: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib:/usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda-9.0/lib64:/usr/local/cuda-9.0/extras/CUPTI/lib64:/usr/local/cuda-8.0/lib64:/usr/local/cuda-8.0/extras/CUPTI/lib64:/usr/lib:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/lib:/usr/local/lib:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/lib:/usr/local/lib:/usr/local/cuda-11.0/lib64:/usr/local/cuda-11.0/extras/CUPTI/lib64/:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/lib:/usr/local/lib:/usr/local/cuda-11.0/lib64:/usr/local/cuda-11.0/extras/CUPTI/lib64/:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/lib:/usr/local/lib\n",
      "2022-08-24 12:14:14.468048: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-08-24 12:14:14.468096: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (9-223-245-158): /proc/driver/nvidia/version does not exist\n",
      "2022-08-24 12:14:14.468395: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "activation_layer tanh\n",
      "activation_layer tanh\n",
      "activation_layer tanh\n",
      "activation_layer tanh\n",
      "activation_layer tanh\n",
      "activation_layer tanh\n"
     ]
    }
   ],
   "source": [
    "# 4.Define Model,train,predict and evaluate\n",
    "model = DSSM(user_feature_columns, item_feature_columns, task='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f36a6eb-67cb-427f-a288-35b17c262320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "aid (InputLayer)                [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "uid (InputLayer)                [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "1024sparse_emb_aid (Embedding)  (None, 1, 20)        3440        aid[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "1024sparse_emb_uid (Embedding)  (None, 1, 20)        127940      uid[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "no_mask (NoMask)                (None, 1, 20)        0           1024sparse_emb_aid[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "no_mask_1 (NoMask)              (None, 1, 20)        0           1024sparse_emb_uid[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 20)           0           no_mask[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 20)           0           no_mask_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "user_embedding (DNN)            (None, 128)          138040      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "item_embedding (DNN)            (None, 128)          138040      flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.norm (TFOpLambda)  (None,)              0           user_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.norm_1 (TFOpLambda (None,)              0           item_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply (TFOpLambda)   (None, 128)          0           user_embedding[0][0]             \n",
      "                                                                 item_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_1 (TFOpLambda) (None,)              0           tf.compat.v1.norm[0][0]          \n",
      "                                                                 tf.compat.v1.norm_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_sum (TFOpLambda) (None,)              0           tf.math.multiply[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add (TFOpLambd (None,)              0           tf.math.multiply_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.truediv (TFOpLambda)    (None,)              0           tf.math.reduce_sum[0][0]         \n",
      "                                                                 tf.__operators__.add[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.clip_by_value (TFOpLambda)   (None,)              0           tf.math.truediv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_2 (TFOpLambda) (None,)              0           tf.clip_by_value[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "prediction_layer (PredictionLay (None, 1)            0           tf.math.multiply_2[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 407,460\n",
      "Trainable params: 404,548\n",
      "Non-trainable params: 2,912\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-24 12:14:15.164688: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 - 4s - loss: 0.7046 - binary_crossentropy: 0.7046 - val_loss: 0.8380 - val_binary_crossentropy: 0.8380\n",
      "Epoch 2/10\n",
      "17/17 - 1s - loss: 0.4414 - binary_crossentropy: 0.4414 - val_loss: 0.8031 - val_binary_crossentropy: 0.8031\n",
      "Epoch 3/10\n",
      "17/17 - 1s - loss: 0.3791 - binary_crossentropy: 0.3791 - val_loss: 0.8234 - val_binary_crossentropy: 0.8234\n",
      "Epoch 4/10\n",
      "17/17 - 1s - loss: 0.3576 - binary_crossentropy: 0.3576 - val_loss: 0.8302 - val_binary_crossentropy: 0.8302\n",
      "Epoch 5/10\n",
      "17/17 - 1s - loss: 0.3436 - binary_crossentropy: 0.3436 - val_loss: 0.8264 - val_binary_crossentropy: 0.8264\n",
      "Epoch 6/10\n",
      "17/17 - 1s - loss: 0.3346 - binary_crossentropy: 0.3346 - val_loss: 0.8114 - val_binary_crossentropy: 0.8114\n",
      "Epoch 7/10\n",
      "17/17 - 1s - loss: 0.3291 - binary_crossentropy: 0.3291 - val_loss: 0.7982 - val_binary_crossentropy: 0.7982\n",
      "Epoch 8/10\n",
      "17/17 - 1s - loss: 0.3254 - binary_crossentropy: 0.3254 - val_loss: 0.7844 - val_binary_crossentropy: 0.7844\n",
      "Epoch 9/10\n",
      "17/17 - 1s - loss: 0.3227 - binary_crossentropy: 0.3227 - val_loss: 0.7697 - val_binary_crossentropy: 0.7697\n",
      "Epoch 10/10\n",
      "17/17 - 1s - loss: 0.3216 - binary_crossentropy: 0.3216 - val_loss: 0.7517 - val_binary_crossentropy: 0.7517\n",
      "test LogLoss 0.7462\n",
      "test AUC 0.5816\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "model.compile(\"adam\", \"binary_crossentropy\", metrics=['binary_crossentropy'], )\n",
    "history = model.fit(train_model_input, train[target].values,\n",
    "                        batch_size=256, epochs=10, verbose=2, validation_split=0.2,)\n",
    "model.save_weights('../../data/saved_model/dssm.ckpt')\n",
    "pred_ans = model.predict(test_model_input, batch_size=256)\n",
    "print(\"test LogLoss\", round(log_loss(test[target].values, pred_ans), 4))\n",
    "print(\"test AUC\", round(roc_auc_score(test[target].values, pred_ans), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "565b9c24-c67b-4671-8907-d053e054cff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user embedding shape:  (1296, 128)\n",
      "item embedding shape:  (1296, 128)\n"
     ]
    }
   ],
   "source": [
    "user_embedding_model = Model(inputs=model.input, outputs=model.get_layer(\"user_embedding\").output)\n",
    "item_embedding_model = Model(inputs=model.input, outputs=model.get_layer(\"item_embedding\").output)\n",
    "user_embedding = user_embedding_model.predict(test_model_input)\n",
    "item_embedding = item_embedding_model.predict(test_model_input)\n",
    "\n",
    "print(\"user embedding shape: \", user_embedding.shape)\n",
    "print(\"item embedding shape: \", item_embedding.shape)\n",
    "\n",
    "np.save('../../data/saved_model/user_embedding.npy', user_embedding)\n",
    "np.save('../../data/saved_model/item_embedding.npy', item_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b247ac57-746d-402f-9b9e-8bb7ba2bf9bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_tf26_cpu",
   "language": "python",
   "name": "py37_tf26_cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
