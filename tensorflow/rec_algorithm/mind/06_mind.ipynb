{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae06f718-b87d-42e2-a155-7daa27ad0173",
   "metadata": {},
   "source": [
    "# MIND"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9af4c9c-060d-4306-b1d1-d231ce6ae363",
   "metadata": {},
   "source": [
    "## movielens 数据集处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2695a43b-b086-46c9-852e-03fedc9c2210",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.models import Model\n",
    "\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72999f22-17b3-456f-b3e6-025f5abfde3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"../../../data/ml-1m/\"\n",
    "output_dir = \"../../../data/ml-1m/mind/\"\n",
    "\n",
    "train_path = os.path.join(output_dir, \"train.txt\")\n",
    "test_path = os.path.join(output_dir, \"test.txt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7561bfa0-3428-4cf3-9d51-7543f4aef085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data_set(data, negsample=0):\n",
    "\n",
    "    data.sort_values(\"Timestamp\", inplace=True)\n",
    "    item_ids = data['MovieID'].unique()\n",
    "\n",
    "    train_set = []\n",
    "    test_set = []\n",
    "    for reviewerID, histlist in tqdm(data.groupby('UserID')):\n",
    "        pos_list = histlist['MovieID'].tolist()\n",
    "        rating_list = histlist['Rating'].tolist()\n",
    "\n",
    "        if negsample > 0:\n",
    "            candidate_set = list(set(item_ids) - set(pos_list))\n",
    "            neg_list = np.random.choice(candidate_set,size=len(pos_list)*negsample,replace=True)\n",
    "        for i in range(1, len(pos_list)):\n",
    "            hist = pos_list[:i]\n",
    "            if i != len(pos_list) - 1:\n",
    "                train_set.append((reviewerID, hist[::-1], pos_list[i], 1,len(hist[::-1]),rating_list[i]))\n",
    "                for negi in range(negsample):\n",
    "                    train_set.append((reviewerID, hist[::-1], neg_list[i*negsample+negi], 0,len(hist[::-1])))\n",
    "            else:\n",
    "                test_set.append((reviewerID, hist[::-1], pos_list[i],1,len(hist[::-1]),rating_list[i]))\n",
    "\n",
    "    random.shuffle(train_set)\n",
    "    random.shuffle(test_set)\n",
    "\n",
    "    print(len(train_set),len(test_set))\n",
    "\n",
    "    return train_set,test_set\n",
    "\n",
    "def gen_model_input(train_set,user_profile,seq_max_len):\n",
    "\n",
    "    train_uid = np.array([line[0] for line in train_set])\n",
    "    train_seq = [line[1] for line in train_set]\n",
    "    train_iid = np.array([line[2] for line in train_set])\n",
    "    train_label = np.array([line[3] for line in train_set])\n",
    "    train_hist_len = np.array([line[4] for line in train_set])\n",
    "\n",
    "    train_seq_pad = pad_sequences(train_seq, maxlen=seq_max_len, padding='post', value=0)\n",
    "    train_model_input = {\"UserID\": train_uid, \"MovieID\": train_iid, \"hist_movie_id\": train_seq_pad,\n",
    "                         \"hist_len\": train_hist_len}\n",
    "\n",
    "    for key in [\"Gender\", \"Age\", \"Occupation\", \"Zip-code\"]:\n",
    "        train_model_input[key] = user_profile.loc[train_model_input['UserID']][key].values\n",
    "\n",
    "    return train_model_input,train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b68aa5fa-361b-4312-a20b-3a8138ec8bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loda Data\n",
    "users_path = os.path.join(input_dir, \"users.dat\")\n",
    "movies_path = os.path.join(input_dir, \"movies.dat\")\n",
    "ratings_path = os.path.join(input_dir, \"ratings.dat\")\n",
    "users_fields = \"UserID::Gender::Age::Occupation::Zip-code\".split(\"::\")\n",
    "movies_fields = \"MovieID::Title::Genres\".split(\"::\")\n",
    "ratings_fields=\"UserID::MovieID::Rating::Timestamp\".split(\"::\")\n",
    "users = pd.read_csv(users_path, sep=\"::\", header=None, engine=\"python\",encoding=\"latin1\", names=users_fields)\n",
    "movies = pd.read_csv(movies_path, sep=\"::\", header=None, engine=\"python\",encoding=\"latin1\", names=movies_fields)\n",
    "ratings = pd.read_csv(ratings_path, sep=\"::\", header=None, engine=\"python\",encoding=\"latin1\", names=ratings_fields)\n",
    "\n",
    "data = pd.merge(pd.merge(ratings, movies), users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06484143-bb49-45bf-9087-2ac74bbb2108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: <class 'pandas.core.frame.DataFrame'> (1000209, 10) \n",
      "    UserID  MovieID  Rating  Timestamp                                   Title  \\\n",
      "0       1     1193       5  978300760  One Flew Over the Cuckoo's Nest (1975)   \n",
      "1       1      661       3  978302109        James and the Giant Peach (1996)   \n",
      "2       1      914       3  978301968                     My Fair Lady (1964)   \n",
      "\n",
      "                         Genres Gender  Age  Occupation Zip-code  \n",
      "0                         Drama      F    1          10    48067  \n",
      "1  Animation|Children's|Musical      F    1          10    48067  \n",
      "2               Musical|Romance      F    1          10    48067  \n"
     ]
    }
   ],
   "source": [
    "#print(\"users:\", type(users), users.shape, '\\n', users.head(3))\n",
    "#print(\"movies:\", type(movies), movies.shape, '\\n', movies.head(3))\n",
    "#print(\"ratings:\", type(ratings), ratings.shape, '\\n', ratings.head(3))\n",
    "print(\"data:\", type(data), data.shape, '\\n', data.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1a9c15b-e31a-4e56-81a4-9ef576f42027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: <class 'pandas.core.frame.DataFrame'> (1000209, 10) \n",
      "    UserID  MovieID  Rating  Timestamp                                   Title  \\\n",
      "0       1     1105       5  978300760  One Flew Over the Cuckoo's Nest (1975)   \n",
      "1       1      640       3  978302109        James and the Giant Peach (1996)   \n",
      "2       1      854       3  978301968                     My Fair Lady (1964)   \n",
      "\n",
      "                         Genres  Gender  Age  Occupation  Zip-code  \n",
      "0                         Drama       1    1          11      1589  \n",
      "1  Animation|Children's|Musical       1    1          11      1589  \n",
      "2               Musical|Romance       1    1          11      1589  \n"
     ]
    }
   ],
   "source": [
    "# 2. Label Encoding for sparse features, \n",
    "# and process sequence features with `gen_date_set` and `gen_model_input`\n",
    "sparse_features = [\"MovieID\", \"UserID\", \"Gender\", \"Age\", \"Occupation\", \"Zip-code\"]\n",
    "SEQ_LEN = 50\n",
    "negsample = 0\n",
    "\n",
    "\n",
    "features = ['UserID', 'MovieID', 'Gender', 'Age', 'Occupation', 'Zip-code']\n",
    "feature_max_idx = {}\n",
    "\n",
    "for feature in features:\n",
    "    lbe = LabelEncoder()\n",
    "    data[feature] = lbe.fit_transform(data[feature]) + 1\n",
    "    feature_max_idx[feature] = data[feature].max() + 1\n",
    "\n",
    "\n",
    "user_profile = data[[\"UserID\", \"Gender\", \"Age\", \"Occupation\", \"Zip-code\"]].drop_duplicates('UserID')\n",
    "item_profile = data[[\"MovieID\"]].drop_duplicates('MovieID')\n",
    "\n",
    "user_profile.set_index(\"UserID\", inplace=True)\n",
    "user_item_list = data.groupby(\"UserID\")['MovieID'].apply(list)\n",
    "print(\"data:\", type(data), data.shape, '\\n', data.head(3))\n",
    "#print(\"user_profile:\", type(user_profile), user_profile.shape, '\\n', user_profile.head(3))\n",
    "#print(\"item_profile:\", type(item_profile), item_profile.shape, '\\n', item_profile.head(3))\n",
    "#print(\"user_item_list:\", type(user_item_list), user_item_list.shape, '\\n', user_item_list.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63cd75af-d6ad-480b-9475-c108a03a0951",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6040/6040 [00:10<00:00, 566.28it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "988129 6040\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = gen_data_set(data, negsample)\n",
    "train_model_input, train_label = gen_model_input(train_set, user_profile, SEQ_LEN)\n",
    "test_model_input, test_label = gen_model_input(test_set, user_profile, SEQ_LEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "338123d4-4236-4519-9281-543c3a9ef6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_label <class 'numpy.ndarray'> 988129\n"
     ]
    }
   ],
   "source": [
    "#print('train_model_input', type(train_model_input), train_model_input)\n",
    "print('train_label',type(train_label),len(train_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eaa9dab8-0d53-46a9-8db3-6012cc87e019",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 988129/988129 [01:57<00:00, 8441.30it/s]\n",
      "100%|██████████| 6040/6040 [00:00<00:00, 8537.24it/s]\n"
     ]
    }
   ],
   "source": [
    "train_neg_sample_list = []\n",
    "test_neg_sample_list = []\n",
    "all_movie_list = set(data['MovieID'])\n",
    "neg_sample_num = 10\n",
    "\n",
    "for i in tqdm(range(len(train_label))):\n",
    "    a = set(train_model_input['hist_movie_id'][i] + train_model_input['MovieID'][i])\n",
    "    neg_list = random.sample(list(all_movie_list - a), neg_sample_num)\n",
    "    train_neg_sample_list.append(np.array(neg_list))\n",
    "    \n",
    "for i in tqdm(range(len(test_label))):\n",
    "    a = set(test_model_input['hist_movie_id'][i] + test_model_input['MovieID'][i])\n",
    "    neg_list = random.sample(list(all_movie_list - a), neg_sample_num)\n",
    "    test_neg_sample_list.append(np.array(neg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df1ed94b-0079-4eca-8678-9a920c82288d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_neg_sample_list <class 'list'> 988129 [1234  292  957  694 1480 1177 3304  465  415  238]\n"
     ]
    }
   ],
   "source": [
    "print('train_neg_sample_list', type(train_neg_sample_list), len(train_neg_sample_list),train_neg_sample_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "976c9445-f962-4ae0-bac9-b933d83bb11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Write to .txt\n",
    "train = open(train_path, \"w\")\n",
    "test = open(test_path, \"w\")\n",
    "\n",
    "for i in range(len(train_label)):\n",
    "    a = train_model_input[\"UserID\"][i]\n",
    "    b = train_model_input[\"Gender\"][i]\n",
    "    c = train_model_input[\"Age\"][i]\n",
    "    d = train_model_input[\"Occupation\"][i]\n",
    "    e = train_model_input[\"Zip-code\"][i]\n",
    "    f = train_model_input[\"hist_movie_id\"][i]\n",
    "    g = train_model_input[\"hist_len\"][i]\n",
    "    \n",
    "    h = train_model_input[\"MovieID\"][i]\n",
    "    m = train_neg_sample_list[i]\n",
    "    \n",
    "    train.write(\"%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\n\"\\\n",
    "               %(str(a), str(b), str(c), str(d), str(e), ','.join([str(ii) for ii in f]), str(g), str(h), ','.join([str(ii) for ii in m])))\n",
    "    \n",
    "train.close()\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(test_label)):\n",
    "    a = test_model_input[\"UserID\"][i]\n",
    "    b = test_model_input[\"Gender\"][i]\n",
    "    c = test_model_input[\"Age\"][i]\n",
    "    d = test_model_input[\"Occupation\"][i]\n",
    "    e = test_model_input[\"Zip-code\"][i]\n",
    "    f = test_model_input[\"hist_movie_id\"][i]\n",
    "    g = test_model_input[\"hist_len\"][i]\n",
    "    \n",
    "    h = test_model_input[\"MovieID\"][i]\n",
    "    m = test_neg_sample_list[i]\n",
    "    \n",
    "    test.write(\"%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\n\"\\\n",
    "               %(str(a), str(b), str(c), str(d), str(e), ','.join([str(ii) for ii in f]), str(g), str(h), ','.join([str(ii) for ii in m])))\n",
    "    \n",
    "test.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f63382-f795-432d-a306-6647d0fbe057",
   "metadata": {},
   "source": [
    "#### 产出的数据格式如下\n",
    "第 1 列 user_id\t用户id\n",
    "\n",
    "第 2 列\tgender\t用户性别\n",
    "\n",
    "第 3 列\tage\t用户年龄\n",
    "\n",
    "第 4 列\toccupation\t用户工作\n",
    "\n",
    "第 5 列\tzip\t用户邮编\n",
    "\n",
    "第 6 列\thist_movie_id\t用户历史观看电影序列\n",
    "\n",
    "第 7 列\thist_len\t用户历史观看电影长度\n",
    "\n",
    "第 8 列\tpos_movie_id\t用户下一步观看的电影（正样本）\n",
    "\n",
    "第 9 列\tneg_movie_id\t用户下一步未观看的电影（抽样作为负样本）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1634d0b3-f758-472f-a4d1-7441768f8f4b",
   "metadata": {},
   "source": [
    "### 模型定义与训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e24314a9-00f5-4521-b0fd-b1a2a8835b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.initializers import RandomNormal, Zeros\n",
    "\n",
    "\n",
    "class SequencePoolingLayer(Layer):\n",
    "    \"\"\"The SequencePoolingLayer is used to apply pooling operation(sum,mean,max) on variable-length sequence feature/multi-value feature.\n",
    "\n",
    "      Input shape\n",
    "        - A list of two  tensor [seq_value,seq_len]\n",
    "\n",
    "        - seq_value is a 3D tensor with shape: ``(batch_size, T, embedding_size)``\n",
    "\n",
    "        - seq_len is a 2D tensor with shape : ``(batch_size, 1)``,indicate valid length of each sequence.\n",
    "\n",
    "      Output shape\n",
    "        - 3D tensor with shape: ``(batch_size, 1, embedding_size)``.\n",
    "\n",
    "      Arguments\n",
    "        - **mode**:str.Pooling operation to be used,can be sum,mean or max.\n",
    "\n",
    "        - **supports_masking**:If True,the input need to support masking.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mode='mean', supports_masking=False, **kwargs):\n",
    "\n",
    "        if mode not in ['sum', 'mean', 'max']:\n",
    "            raise ValueError(\"mode must be sum or mean\")\n",
    "        self.mode = mode\n",
    "        self.eps = tf.constant(1e-8, tf.float32)\n",
    "        super(SequencePoolingLayer, self).__init__(**kwargs)\n",
    "\n",
    "        self.supports_masking = supports_masking\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if not self.supports_masking:\n",
    "            self.seq_len_max = int(input_shape[0][1])\n",
    "        super(SequencePoolingLayer, self).build(input_shape)  # Be sure to call this somewhere!\n",
    "        print('input_shape',input_shape, self.seq_len_max)\n",
    "\n",
    "    def call(self, seq_value_len_list, mask=None, **kwargs):\n",
    "        if self.supports_masking:\n",
    "            if mask is None:\n",
    "                raise ValueError(\n",
    "                    \"When supports_masking=True,input must support masking\")\n",
    "            uiseq_embed_list = seq_value_len_list\n",
    "            mask = tf.cast(mask, tf.float32)  # tf.to_float(mask)\n",
    "            user_behavior_length = tf.reduce_sum(mask, axis=-1, keepdims=True)\n",
    "            mask = tf.expand_dims(mask, axis=2)\n",
    "        else:\n",
    "            uiseq_embed_list, user_behavior_length = seq_value_len_list\n",
    "\n",
    "            mask = tf.sequence_mask(user_behavior_length,\n",
    "                                    self.seq_len_max, dtype=tf.float32)\n",
    "            mask = tf.transpose(mask, (0, 2, 1))\n",
    "\n",
    "        embedding_size = uiseq_embed_list.shape[-1]\n",
    "\n",
    "        mask = tf.tile(mask, [1, 1, embedding_size])\n",
    "\n",
    "        if self.mode == \"max\":\n",
    "            hist = uiseq_embed_list - (1-mask) * 1e9\n",
    "            return tf.reduce_max(hist, 1, keepdims=True)\n",
    "\n",
    "        hist = tf.reduce_sum(uiseq_embed_list * mask, 1, keepdims=False)\n",
    "\n",
    "        if self.mode == \"mean\":\n",
    "            hist = tf.divide(hist, tf.cast(user_behavior_length, tf.float32) + self.eps)\n",
    "\n",
    "        hist = tf.expand_dims(hist, axis=1)\n",
    "        return hist\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.supports_masking:\n",
    "            return (None, 1, input_shape[-1])\n",
    "        else:\n",
    "            return (None, 1, input_shape[0][-1])\n",
    "\n",
    "    def compute_mask(self, inputs, mask):\n",
    "        return None\n",
    "\n",
    "    def get_config(self, ):\n",
    "        config = {'mode': self.mode, 'supports_masking': self.supports_masking}\n",
    "        base_config = super(SequencePoolingLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "        \n",
    "        \n",
    "\n",
    "class LabelAwareAttention(Layer):\n",
    "    def __init__(self, k_max, pow_p=1, **kwargs):\n",
    "        self.k_max = k_max\n",
    "        self.pow_p = pow_p\n",
    "        super(LabelAwareAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Be sure to call this somewhere!\n",
    "        self.embedding_size = input_shape[0][-1]\n",
    "        super(LabelAwareAttention, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, training=None, **kwargs):\n",
    "        keys = inputs[0]\n",
    "        query = inputs[1]\n",
    "        weight = tf.reduce_sum(keys * query, axis=-1, keepdims=True)\n",
    "        weight = tf.pow(weight, self.pow_p)  # [x,k_max,1]\n",
    "        print('keys',keys)\n",
    "        print('query',query)\n",
    "        print('weight',weight)\n",
    "\n",
    "\n",
    "        if len(inputs) == 3:\n",
    "            k_user = tf.cast(tf.maximum(\n",
    "                1.,\n",
    "                tf.minimum(\n",
    "                    tf.cast(self.k_max, dtype=\"float32\"),  # k_max\n",
    "                    tf.math.log1p(tf.cast(inputs[2], dtype=\"float32\")) / tf.math.log(2.)  # hist_len\n",
    "                )\n",
    "            ), dtype=\"int64\")\n",
    "            print('k_user',k_user)\n",
    "            \n",
    "            seq_mask = tf.transpose(tf.sequence_mask(k_user, self.k_max), [0, 2, 1])\n",
    "            padding = tf.ones_like(seq_mask, dtype=tf.float32) * (-2 ** 32 + 1)  # [x,k_max,1]\n",
    "            weight = tf.where(seq_mask, weight, padding)\n",
    "            print('seq_mask',seq_mask)\n",
    "            print('padding',padding)\n",
    "            print('weight',weight)\n",
    "        \n",
    "        weight = tf.nn.softmax(weight, name=\"weight\")\n",
    "        output = tf.reduce_sum(keys * weight, axis=1)\n",
    "        print('attention output', output)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.embedding_size)\n",
    "\n",
    "    def get_config(self, ):\n",
    "        config = {'k_max': self.k_max, 'pow_p': self.pow_p}\n",
    "        base_config = super(LabelAwareAttention, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "        \n",
    "        \n",
    "\n",
    "class CapsuleLayer(Layer):\n",
    "    def __init__(self, input_units, out_units, max_len, k_max, iteration_times=3,\n",
    "                 init_std=1.0, **kwargs):\n",
    "        self.input_units = input_units\n",
    "        self.out_units = out_units\n",
    "        self.max_len = max_len\n",
    "        self.k_max = k_max\n",
    "        self.iteration_times = iteration_times\n",
    "        self.init_std = init_std\n",
    "        super(CapsuleLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        print('input_shape', type(input_shape),input_shape)\n",
    "        self.routing_logits = self.add_weight(shape=[1, self.k_max, self.max_len],\n",
    "                                              initializer=RandomNormal(stddev=self.init_std),\n",
    "                                              trainable=False, name=\"B\", dtype=tf.float32)\n",
    "        print('routing_logits', self.routing_logits.shape)\n",
    "        self.bilinear_mapping_matrix = self.add_weight(shape=[self.input_units, self.out_units],\n",
    "                                                       initializer=RandomNormal(stddev=self.init_std),\n",
    "                                                       name=\"S\", dtype=tf.float32)\n",
    "        print('bilinear_mapping_matrix', self.bilinear_mapping_matrix.shape)\n",
    "\n",
    "        super(CapsuleLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        behavior_embddings, seq_len = inputs\n",
    "        batch_size = tf.shape(behavior_embddings)[0]\n",
    "        seq_len_tile = tf.tile(seq_len, [1, self.k_max])\n",
    "        print('seq_len_tile', seq_len_tile.shape)\n",
    "\n",
    "\n",
    "        for i in range(self.iteration_times):\n",
    "            mask = tf.sequence_mask(seq_len_tile, self.max_len)\n",
    "            print('mask', mask.shape, mask)\n",
    "            pad = tf.ones_like(mask, dtype=tf.float32) * (-2 ** 32 + 1)\n",
    "            routing_logits_with_padding = tf.where(mask, tf.tile(self.routing_logits, [batch_size, 1, 1]), pad)\n",
    "            weight = tf.nn.softmax(routing_logits_with_padding)\n",
    "            behavior_embdding_mapping = tf.tensordot(behavior_embddings, self.bilinear_mapping_matrix, axes=1)\n",
    "            Z = tf.matmul(weight, behavior_embdding_mapping)\n",
    "            interest_capsules = squash(Z)\n",
    "            \n",
    "            delta_routing_logits = tf.reduce_sum(\n",
    "                tf.matmul(interest_capsules, tf.transpose(behavior_embdding_mapping, perm=[0, 2, 1])),\n",
    "                axis=0, keepdims=True\n",
    "            )\n",
    "            print('interest_capsules', interest_capsules.shape,interest_capsules)\n",
    "            print('behavior_embdding_mapping', behavior_embdding_mapping.shape,behavior_embdding_mapping)\n",
    "            print('delta_routing_logits', delta_routing_logits.shape,delta_routing_logits)\n",
    "            self.routing_logits.assign_add(delta_routing_logits)\n",
    "\n",
    "        interest_capsules = tf.reshape(interest_capsules, [-1, self.k_max, self.out_units])\n",
    "        return interest_capsules\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.k_max, self.out_units)\n",
    "\n",
    "    def get_config(self, ):\n",
    "        config = {'input_units': self.input_units, 'out_units': self.out_units, 'max_len': self.max_len,\n",
    "                  'k_max': self.k_max, 'iteration_times': self.iteration_times, \"init_std\": self.init_std}\n",
    "        base_config = super(CapsuleLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "    \n",
    "\n",
    "\n",
    "def squash(inputs):\n",
    "    vec_squared_norm = tf.reduce_sum(tf.square(inputs), axis=-1, keepdims=True)\n",
    "    scalar_factor = vec_squared_norm / (1 + vec_squared_norm) / tf.sqrt(vec_squared_norm + 1e-8)\n",
    "    vec_squashed = scalar_factor * inputs\n",
    "    \n",
    "    return vec_squashed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa438200-d285-4e13-9d9c-13af8f44b065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, concatenate, Flatten, Dense, Dropout\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "def tile_user_otherfeat(user_other_feature, k_max):\n",
    "        return tf.tile(tf.expand_dims(user_other_feature, -2), [1, k_max, 1])\n",
    "\n",
    "\n",
    "def mind(\n",
    "    sparse_input_length=1,\n",
    "    dense_input_length=1,\n",
    "    sparse_seq_input_length=50,\n",
    "    \n",
    "    embedding_dim = 64,\n",
    "    neg_sample_num = 10,\n",
    "    user_hidden_unit_list = [128, 64],\n",
    "    k_max = 5,\n",
    "    p = 1,\n",
    "    dynamic_k = True\n",
    "    ):\n",
    "    \n",
    "\n",
    "    \n",
    "    # 1. Input layer\n",
    "    user_id_input_layer = Input(shape=(sparse_input_length, ), name=\"user_id_input_layer\")\n",
    "    gender_input_layer = Input(shape=(sparse_input_length, ), name=\"gender_input_layer\")\n",
    "    age_input_layer = Input(shape=(sparse_input_length, ), name=\"age_input_layer\")\n",
    "    occupation_input_layer = Input(shape=(sparse_input_length, ), name=\"occupation_input_layer\")\n",
    "    zip_input_layer = Input(shape=(sparse_input_length, ), name=\"zip_input_layer\")\n",
    "    \n",
    "    \n",
    "    user_click_item_seq_input_layer = Input(shape=(sparse_seq_input_length, ), name=\"user_click_item_seq_input_layer\")\n",
    "    user_click_item_seq_length_input_layer = Input(shape=(sparse_input_length, ), name=\"user_click_item_seq_length_input_layer\")\n",
    "    \n",
    "    \n",
    "    pos_item_sample_input_layer = Input(shape=(sparse_input_length, ), name=\"pos_item_sample_input_layer\")\n",
    "    neg_item_sample_input_layer = Input(shape=(neg_sample_num, ), name=\"neg_item_sample_input_layer\")\n",
    "\n",
    "\n",
    "    \n",
    "    # 2. Embedding layer\n",
    "    user_id_embedding_layer = Embedding(6040+1, embedding_dim, mask_zero=True, name='user_id_embedding_layer')(user_id_input_layer)\n",
    "    gender_embedding_layer = Embedding(2+1, embedding_dim, mask_zero=True, name='gender_embedding_layer')(gender_input_layer)\n",
    "    age_embedding_layer = Embedding(7+1, embedding_dim, mask_zero=True, name='age_embedding_layer')(age_input_layer)\n",
    "    occupation_embedding_layer = Embedding(21+1, embedding_dim, mask_zero=True, name='occupation_embedding_layer')(occupation_input_layer)\n",
    "    zip_embedding_layer = Embedding(3439+1, embedding_dim, mask_zero=True, name='zip_embedding_layer')(zip_input_layer)\n",
    "    \n",
    "    item_id_embedding_layer = Embedding(3706+1, embedding_dim, mask_zero=True, name='item_id_embedding_layer')\n",
    "    pos_item_sample_embedding_layer = item_id_embedding_layer(pos_item_sample_input_layer)\n",
    "    neg_item_sample_embedding_layer = item_id_embedding_layer(neg_item_sample_input_layer)\n",
    "    \n",
    "    user_click_item_seq_embedding_layer = item_id_embedding_layer(user_click_item_seq_input_layer)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    ### ********** ###\n",
    "    # 3. user part\n",
    "    ### ********** ###\n",
    "    \n",
    "    # 3.1 pooling layer\n",
    "    user_click_item_seq_embedding_layer_pooling = SequencePoolingLayer()\\\n",
    "        ([user_click_item_seq_embedding_layer, user_click_item_seq_length_input_layer])\n",
    "    \n",
    "    print(\"user_click_item_seq_embedding_layer_pooling\", user_click_item_seq_embedding_layer_pooling)\n",
    "    \n",
    "    \n",
    "    # 3.2 capsule layer\n",
    "    high_capsule = CapsuleLayer(input_units=embedding_dim,\n",
    "                                out_units=embedding_dim, max_len=sparse_seq_input_length,\n",
    "                                k_max=k_max)\\\n",
    "                        ([user_click_item_seq_embedding_layer, user_click_item_seq_length_input_layer])\n",
    "    \n",
    "    print(\"high_capsule: \", high_capsule)\n",
    "    \n",
    "\n",
    "    # 3.3 Concat \"sparse\" embedding & \"sparse_seq\" embedding, and tile embedding\n",
    "    other_user_embedding_layer = concatenate([user_id_embedding_layer, gender_embedding_layer, \\\n",
    "                                                        age_embedding_layer, occupation_embedding_layer, \\\n",
    "                                                        zip_embedding_layer, user_click_item_seq_embedding_layer_pooling], \n",
    "                                       axis=-1)\n",
    "                                    \n",
    "    print('other_user_embedding_layer0', other_user_embedding_layer)\n",
    "\n",
    "    other_user_embedding_layer = tf.tile(other_user_embedding_layer, [1, k_max, 1])\n",
    "            \n",
    "    print(\"other_user_embedding_layer: \", other_user_embedding_layer)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 3.4 user dnn part\n",
    "    user_deep_input = concatenate([other_user_embedding_layer, high_capsule], axis=-1)\n",
    "    print(\"user_deep_input0: \", user_deep_input)\n",
    "\n",
    "    \n",
    "    for i, u in enumerate(user_hidden_unit_list):\n",
    "        user_deep_input = Dense(u, activation=\"relu\", name=\"FC_{0}\".format(i+1))(user_deep_input)\n",
    "        #user_deep_input = Dropout(0.3)(user_deep_input)\n",
    "        \n",
    "    print(\"user_deep_input: \", user_deep_input)\n",
    "    \n",
    "\n",
    "    if dynamic_k:\n",
    "        user_embedding_final = LabelAwareAttention(k_max=k_max, pow_p=p, )(\\\n",
    "                                    [user_deep_input, pos_item_sample_embedding_layer, user_click_item_seq_length_input_layer])\n",
    "    else:\n",
    "        user_embedding_final = LabelAwareAttention(k_max=k_max, pow_p=p, )(\\\n",
    "                                    [user_deep_input, pos_item_sample_embedding_layer])\n",
    "    \n",
    "    print(\"user_embedding_final0: \", user_embedding_final)\n",
    "\n",
    "    user_embedding_final = tf.expand_dims(user_embedding_final, 1)\n",
    "    print(\"user_embedding_final: \", user_embedding_final)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### ********** ###\n",
    "    # 4. item part\n",
    "    ### ********** ###\n",
    "\n",
    "    item_embedding_layer = concatenate([pos_item_sample_embedding_layer, neg_item_sample_embedding_layer], \\\n",
    "                                       axis=1)\n",
    "    \n",
    "    item_embedding_layer = tf.transpose(item_embedding_layer, [0,2,1])\n",
    "    \n",
    "    print(\"item_embedding_layer: \", item_embedding_layer)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ### ********** ###\n",
    "    # 5. Output\n",
    "    ### ********** ###\n",
    "    \n",
    "    dot_output = tf.matmul(user_embedding_final, item_embedding_layer)\n",
    "    dot_output = tf.nn.softmax(dot_output) # 输出11个值，index为0的值是正样本，负样本的索引位置为[1-10]\n",
    "    \n",
    "    print('dot_output', dot_output)\n",
    "    \n",
    "    user_inputs_list = [user_id_input_layer, gender_input_layer, age_input_layer, \\\n",
    "                        occupation_input_layer, zip_input_layer, \\\n",
    "                        user_click_item_seq_input_layer, user_click_item_seq_length_input_layer]\n",
    "    \n",
    "    item_inputs_list = [pos_item_sample_input_layer, neg_item_sample_input_layer]\n",
    "\n",
    "    model = Model(inputs = user_inputs_list + item_inputs_list,\n",
    "                  outputs = dot_output)\n",
    "    \n",
    "    \n",
    "    #print(model.summary())\n",
    "    #tf.keras.utils.plot_model(model, to_file='MIND_model.png', show_shapes=True)\n",
    "\n",
    "\n",
    "    model.__setattr__(\"user_input\", user_inputs_list)\n",
    "    model.__setattr__(\"user_embedding\", user_deep_input)\n",
    "    \n",
    "    model.__setattr__(\"item_input\", pos_item_sample_input_layer)\n",
    "    model.__setattr__(\"item_embedding\", pos_item_sample_embedding_layer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f3023c9-6c6c-4401-92be-06bbd637cc02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_train:  988129\n",
      "n_val:  6040\n",
      "steps_per_epoch:  989\n",
      "validation_steps:  7\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from data_generator import file_generator\n",
    "\n",
    "# 1. Load data\n",
    "\n",
    "train_path = train_path\n",
    "val_path = test_path\n",
    "batch_size = 1000\n",
    "\n",
    "n_train = sum([1 for i in open(train_path)])\n",
    "n_val = sum([1 for i in open(val_path)])\n",
    "\n",
    "train_steps = n_train / batch_size\n",
    "train_steps_ = n_train // batch_size\n",
    "validation_steps = n_val / batch_size\n",
    "validation_steps_ = n_val // batch_size\n",
    "\n",
    "\n",
    "train_generator = file_generator(train_path, batch_size)\n",
    "val_generator = file_generator(val_path, batch_size)\n",
    "\n",
    "steps_per_epoch = train_steps_ if train_steps==train_steps_ else train_steps_ + 1\n",
    "validation_steps = validation_steps_ if validation_steps==validation_steps_ else validation_steps_ + 1\n",
    "\n",
    "print(\"n_train: \", n_train)\n",
    "print(\"n_val: \", n_val)\n",
    "\n",
    "print(\"steps_per_epoch: \", steps_per_epoch)\n",
    "print(\"validation_steps: \", validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49d9ceae-9bfb-418e-bb71-e4501e2ea1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape [TensorShape([None, 50, 64]), TensorShape([None, 1])] 50\n",
      "user_click_item_seq_embedding_layer_pooling KerasTensor(type_spec=TensorSpec(shape=(None, 1, 64), dtype=tf.float32, name=None), name='sequence_pooling_layer_1/ExpandDims:0', description=\"created by layer 'sequence_pooling_layer_1'\")\n",
      "input_shape <class 'list'> [TensorShape([None, 50, 64]), TensorShape([None, 1])]\n",
      "routing_logits (1, 5, 50)\n",
      "bilinear_mapping_matrix (64, 64)\n",
      "seq_len_tile (None, 5)\n",
      "mask (None, 5, 50) Tensor(\"capsule_layer_1/SequenceMask/Less:0\", shape=(None, 5, 50), dtype=bool)\n",
      "interest_capsules (None, 5, 64) Tensor(\"capsule_layer_1/mul_1:0\", shape=(None, 5, 64), dtype=float32)\n",
      "behavior_embdding_mapping (None, 50, 64) Tensor(\"capsule_layer_1/Tensordot:0\", shape=(None, 50, 64), dtype=float32)\n",
      "delta_routing_logits (1, 5, 50) Tensor(\"capsule_layer_1/Sum_1:0\", shape=(1, 5, 50), dtype=float32)\n",
      "mask (None, 5, 50) Tensor(\"capsule_layer_1/SequenceMask_1/Less:0\", shape=(None, 5, 50), dtype=bool)\n",
      "interest_capsules (None, 5, 64) Tensor(\"capsule_layer_1/mul_3:0\", shape=(None, 5, 64), dtype=float32)\n",
      "behavior_embdding_mapping (None, 50, 64) Tensor(\"capsule_layer_1/Tensordot_1:0\", shape=(None, 50, 64), dtype=float32)\n",
      "delta_routing_logits (1, 5, 50) Tensor(\"capsule_layer_1/Sum_3:0\", shape=(1, 5, 50), dtype=float32)\n",
      "mask (None, 5, 50) Tensor(\"capsule_layer_1/SequenceMask_2/Less:0\", shape=(None, 5, 50), dtype=bool)\n",
      "interest_capsules (None, 5, 64) Tensor(\"capsule_layer_1/mul_5:0\", shape=(None, 5, 64), dtype=float32)\n",
      "behavior_embdding_mapping (None, 50, 64) Tensor(\"capsule_layer_1/Tensordot_2:0\", shape=(None, 50, 64), dtype=float32)\n",
      "delta_routing_logits (1, 5, 50) Tensor(\"capsule_layer_1/Sum_5:0\", shape=(1, 5, 50), dtype=float32)\n",
      "high_capsule:  KerasTensor(type_spec=TensorSpec(shape=(None, 5, 64), dtype=tf.float32, name=None), name='capsule_layer_1/Reshape:0', description=\"created by layer 'capsule_layer_1'\")\n",
      "other_user_embedding_layer0 KerasTensor(type_spec=TensorSpec(shape=(None, 1, 384), dtype=tf.float32, name=None), name='concatenate_3/concat:0', description=\"created by layer 'concatenate_3'\")\n",
      "other_user_embedding_layer:  KerasTensor(type_spec=TensorSpec(shape=(None, 5, 384), dtype=tf.float32, name=None), name='tf.tile_1/Tile:0', description=\"created by layer 'tf.tile_1'\")\n",
      "user_deep_input:  KerasTensor(type_spec=TensorSpec(shape=(None, 5, 448), dtype=tf.float32, name=None), name='concatenate_4/concat:0', description=\"created by layer 'concatenate_4'\")\n",
      "user_deep_input:  KerasTensor(type_spec=TensorSpec(shape=(None, 5, 64), dtype=tf.float32, name=None), name='FC_2/Relu:0', description=\"created by layer 'FC_2'\")\n",
      "keys Tensor(\"Placeholder:0\", shape=(None, 5, 64), dtype=float32)\n",
      "query Tensor(\"Placeholder_1:0\", shape=(None, 1, 64), dtype=float32)\n",
      "weight Tensor(\"label_aware_attention_1/Pow:0\", shape=(None, 5, 1), dtype=float32)\n",
      "k_user Tensor(\"label_aware_attention_1/Cast_1:0\", shape=(None, 1), dtype=int64)\n",
      "seq_mask Tensor(\"label_aware_attention_1/transpose:0\", shape=(None, 5, 1), dtype=bool)\n",
      "padding Tensor(\"label_aware_attention_1/mul_1:0\", shape=(None, 5, 1), dtype=float32)\n",
      "weight Tensor(\"label_aware_attention_1/SelectV2:0\", shape=(None, 5, 1), dtype=float32)\n",
      "attention output Tensor(\"label_aware_attention_1/Sum_1:0\", shape=(None, 64), dtype=float32)\n",
      "user_embedding_final0:  KerasTensor(type_spec=TensorSpec(shape=(None, 64), dtype=tf.float32, name=None), name='label_aware_attention_1/Sum_1:0', description=\"created by layer 'label_aware_attention_1'\")\n",
      "user_embedding_final:  KerasTensor(type_spec=TensorSpec(shape=(None, 1, 64), dtype=tf.float32, name=None), name='tf.expand_dims_1/ExpandDims:0', description=\"created by layer 'tf.expand_dims_1'\")\n",
      "item_embedding_layer:  KerasTensor(type_spec=TensorSpec(shape=(None, 64, 11), dtype=tf.float32, name=None), name='tf.compat.v1.transpose_1/transpose:0', description=\"created by layer 'tf.compat.v1.transpose_1'\")\n",
      "dot_output KerasTensor(type_spec=TensorSpec(shape=(None, 1, 11), dtype=tf.float32, name=None), name='tf.nn.softmax_1/Softmax:0', description=\"created by layer 'tf.nn.softmax_1'\")\n",
      "Epoch 1/2\n",
      "seq_len_tile (None, 5)\n",
      "mask (None, 5, 50) Tensor(\"model_1/capsule_layer_1/SequenceMask/Less:0\", shape=(None, 5, 50), dtype=bool)\n",
      "interest_capsules (None, 5, 64) Tensor(\"model_1/capsule_layer_1/mul_1:0\", shape=(None, 5, 64), dtype=float32)\n",
      "behavior_embdding_mapping (None, 50, 64) Tensor(\"model_1/capsule_layer_1/Tensordot:0\", shape=(None, 50, 64), dtype=float32)\n",
      "delta_routing_logits (1, 5, 50) Tensor(\"model_1/capsule_layer_1/Sum_1:0\", shape=(1, 5, 50), dtype=float32)\n",
      "mask (None, 5, 50) Tensor(\"model_1/capsule_layer_1/SequenceMask_1/Less:0\", shape=(None, 5, 50), dtype=bool)\n",
      "interest_capsules (None, 5, 64) Tensor(\"model_1/capsule_layer_1/mul_3:0\", shape=(None, 5, 64), dtype=float32)\n",
      "behavior_embdding_mapping (None, 50, 64) Tensor(\"model_1/capsule_layer_1/Tensordot_1:0\", shape=(None, 50, 64), dtype=float32)\n",
      "delta_routing_logits (1, 5, 50) Tensor(\"model_1/capsule_layer_1/Sum_3:0\", shape=(1, 5, 50), dtype=float32)\n",
      "mask (None, 5, 50) Tensor(\"model_1/capsule_layer_1/SequenceMask_2/Less:0\", shape=(None, 5, 50), dtype=bool)\n",
      "interest_capsules (None, 5, 64) Tensor(\"model_1/capsule_layer_1/mul_5:0\", shape=(None, 5, 64), dtype=float32)\n",
      "behavior_embdding_mapping (None, 50, 64) Tensor(\"model_1/capsule_layer_1/Tensordot_2:0\", shape=(None, 50, 64), dtype=float32)\n",
      "delta_routing_logits (1, 5, 50) Tensor(\"model_1/capsule_layer_1/Sum_5:0\", shape=(1, 5, 50), dtype=float32)\n",
      "keys Tensor(\"model_1/FC_2/Relu:0\", shape=(None, 5, 64), dtype=float32)\n",
      "query Tensor(\"model_1/item_id_embedding_layer/embedding_lookup_1/Identity_1:0\", shape=(None, 1, 64), dtype=float32)\n",
      "weight Tensor(\"model_1/label_aware_attention_1/Pow:0\", shape=(None, 5, 1), dtype=float32)\n",
      "k_user Tensor(\"model_1/label_aware_attention_1/Cast_1:0\", shape=(None, 1), dtype=int64)\n",
      "seq_mask Tensor(\"model_1/label_aware_attention_1/transpose:0\", shape=(None, 5, 1), dtype=bool)\n",
      "padding Tensor(\"model_1/label_aware_attention_1/mul_1:0\", shape=(None, 5, 1), dtype=float32)\n",
      "weight Tensor(\"model_1/label_aware_attention_1/SelectV2:0\", shape=(None, 5, 1), dtype=float32)\n",
      "attention output Tensor(\"model_1/label_aware_attention_1/Sum_1:0\", shape=(None, 64), dtype=float32)\n",
      "seq_len_tile (None, 5)\n",
      "mask (None, 5, 50) Tensor(\"model_1/capsule_layer_1/SequenceMask/Less:0\", shape=(None, 5, 50), dtype=bool)\n",
      "interest_capsules (None, 5, 64) Tensor(\"model_1/capsule_layer_1/mul_1:0\", shape=(None, 5, 64), dtype=float32)\n",
      "behavior_embdding_mapping (None, 50, 64) Tensor(\"model_1/capsule_layer_1/Tensordot:0\", shape=(None, 50, 64), dtype=float32)\n",
      "delta_routing_logits (1, 5, 50) Tensor(\"model_1/capsule_layer_1/Sum_1:0\", shape=(1, 5, 50), dtype=float32)\n",
      "mask (None, 5, 50) Tensor(\"model_1/capsule_layer_1/SequenceMask_1/Less:0\", shape=(None, 5, 50), dtype=bool)\n",
      "interest_capsules (None, 5, 64) Tensor(\"model_1/capsule_layer_1/mul_3:0\", shape=(None, 5, 64), dtype=float32)\n",
      "behavior_embdding_mapping (None, 50, 64) Tensor(\"model_1/capsule_layer_1/Tensordot_1:0\", shape=(None, 50, 64), dtype=float32)\n",
      "delta_routing_logits (1, 5, 50) Tensor(\"model_1/capsule_layer_1/Sum_3:0\", shape=(1, 5, 50), dtype=float32)\n",
      "mask (None, 5, 50) Tensor(\"model_1/capsule_layer_1/SequenceMask_2/Less:0\", shape=(None, 5, 50), dtype=bool)\n",
      "interest_capsules (None, 5, 64) Tensor(\"model_1/capsule_layer_1/mul_5:0\", shape=(None, 5, 64), dtype=float32)\n",
      "behavior_embdding_mapping (None, 50, 64) Tensor(\"model_1/capsule_layer_1/Tensordot_2:0\", shape=(None, 50, 64), dtype=float32)\n",
      "delta_routing_logits (1, 5, 50) Tensor(\"model_1/capsule_layer_1/Sum_5:0\", shape=(1, 5, 50), dtype=float32)\n",
      "keys Tensor(\"model_1/FC_2/Relu:0\", shape=(None, 5, 64), dtype=float32)\n",
      "query Tensor(\"model_1/item_id_embedding_layer/embedding_lookup_1/Identity_1:0\", shape=(None, 1, 64), dtype=float32)\n",
      "weight Tensor(\"model_1/label_aware_attention_1/Pow:0\", shape=(None, 5, 1), dtype=float32)\n",
      "k_user Tensor(\"model_1/label_aware_attention_1/Cast_1:0\", shape=(None, 1), dtype=int64)\n",
      "seq_mask Tensor(\"model_1/label_aware_attention_1/transpose:0\", shape=(None, 5, 1), dtype=bool)\n",
      "padding Tensor(\"model_1/label_aware_attention_1/mul_1:0\", shape=(None, 5, 1), dtype=float32)\n",
      "weight Tensor(\"model_1/label_aware_attention_1/SelectV2:0\", shape=(None, 5, 1), dtype=float32)\n",
      "attention output Tensor(\"model_1/label_aware_attention_1/Sum_1:0\", shape=(None, 64), dtype=float32)\n",
      "989/989 [==============================] - ETA: 0s - loss: 1.7563 - sparse_categorical_accuracy: 0.3612seq_len_tile (None, 5)\n",
      "mask (None, 5, 50) Tensor(\"model_1/capsule_layer_1/SequenceMask/Less:0\", shape=(None, 5, 50), dtype=bool)\n",
      "interest_capsules (None, 5, 64) Tensor(\"model_1/capsule_layer_1/mul_1:0\", shape=(None, 5, 64), dtype=float32)\n",
      "behavior_embdding_mapping (None, 50, 64) Tensor(\"model_1/capsule_layer_1/Tensordot:0\", shape=(None, 50, 64), dtype=float32)\n",
      "delta_routing_logits (1, 5, 50) Tensor(\"model_1/capsule_layer_1/Sum_1:0\", shape=(1, 5, 50), dtype=float32)\n",
      "mask (None, 5, 50) Tensor(\"model_1/capsule_layer_1/SequenceMask_1/Less:0\", shape=(None, 5, 50), dtype=bool)\n",
      "interest_capsules (None, 5, 64) Tensor(\"model_1/capsule_layer_1/mul_3:0\", shape=(None, 5, 64), dtype=float32)\n",
      "behavior_embdding_mapping (None, 50, 64) Tensor(\"model_1/capsule_layer_1/Tensordot_1:0\", shape=(None, 50, 64), dtype=float32)\n",
      "delta_routing_logits (1, 5, 50) Tensor(\"model_1/capsule_layer_1/Sum_3:0\", shape=(1, 5, 50), dtype=float32)\n",
      "mask (None, 5, 50) Tensor(\"model_1/capsule_layer_1/SequenceMask_2/Less:0\", shape=(None, 5, 50), dtype=bool)\n",
      "interest_capsules (None, 5, 64) Tensor(\"model_1/capsule_layer_1/mul_5:0\", shape=(None, 5, 64), dtype=float32)\n",
      "behavior_embdding_mapping (None, 50, 64) Tensor(\"model_1/capsule_layer_1/Tensordot_2:0\", shape=(None, 50, 64), dtype=float32)\n",
      "delta_routing_logits (1, 5, 50) Tensor(\"model_1/capsule_layer_1/Sum_5:0\", shape=(1, 5, 50), dtype=float32)\n",
      "keys Tensor(\"model_1/FC_2/Relu:0\", shape=(None, 5, 64), dtype=float32)\n",
      "query Tensor(\"model_1/item_id_embedding_layer/embedding_lookup_1/Identity_1:0\", shape=(None, 1, 64), dtype=float32)\n",
      "weight Tensor(\"model_1/label_aware_attention_1/Pow:0\", shape=(None, 5, 1), dtype=float32)\n",
      "k_user Tensor(\"model_1/label_aware_attention_1/Cast_1:0\", shape=(None, 1), dtype=int64)\n",
      "seq_mask Tensor(\"model_1/label_aware_attention_1/transpose:0\", shape=(None, 5, 1), dtype=bool)\n",
      "padding Tensor(\"model_1/label_aware_attention_1/mul_1:0\", shape=(None, 5, 1), dtype=float32)\n",
      "weight Tensor(\"model_1/label_aware_attention_1/SelectV2:0\", shape=(None, 5, 1), dtype=float32)\n",
      "attention output Tensor(\"model_1/label_aware_attention_1/Sum_1:0\", shape=(None, 64), dtype=float32)\n",
      "989/989 [==============================] - 68s 67ms/step - loss: 1.7561 - sparse_categorical_accuracy: 0.3612 - val_loss: 1.5639 - val_sparse_categorical_accuracy: 0.4243\n",
      "Epoch 2/2\n",
      "989/989 [==============================] - 67s 68ms/step - loss: 1.3859 - sparse_categorical_accuracy: 0.4813 - val_loss: 1.4674 - val_sparse_categorical_accuracy: 0.4724\n"
     ]
    }
   ],
   "source": [
    "# 2. Train model\n",
    "\n",
    "\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "callbacks = [early_stopping_cb]\n",
    "\n",
    "\n",
    "model = mind()\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', \\\n",
    "    optimizer=Adam(lr=1e-3), \\\n",
    "    metrics=['sparse_categorical_accuracy'])\n",
    "    \n",
    "# loss=\"sparse_categorical_accuracy\"的应用方式参见：https://mp.weixin.qq.com/s/H4ET0bO_xPm8TNqltMt3Fg\n",
    "\n",
    "\n",
    "\n",
    "history = model.fit(train_generator, \\\n",
    "                    epochs=2, \\\n",
    "                    steps_per_epoch = steps_per_epoch, \\\n",
    "                    callbacks = callbacks, \n",
    "                    validation_data = val_generator, \\\n",
    "                    validation_steps = validation_steps, \\\n",
    "                    shuffle=True\n",
    "                   )\n",
    "                   \n",
    "                   \n",
    "model.save_weights('mind_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c6fc95-2393-4b44-ab0c-2c1f3b2f5017",
   "metadata": {},
   "source": [
    "## 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "513d81d1-72eb-4e3e-b72d-81b5469452fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape [TensorShape([None, 50, 64]), TensorShape([None, 1])] 50\n",
      "user_click_item_seq_embedding_layer_pooling KerasTensor(type_spec=TensorSpec(shape=(None, 1, 64), dtype=tf.float32, name=None), name='sequence_pooling_layer_3/ExpandDims:0', description=\"created by layer 'sequence_pooling_layer_3'\")\n",
      "input_shape <class 'list'> [TensorShape([None, 50, 64]), TensorShape([None, 1])]\n",
      "routing_logits (1, 5, 50)\n",
      "bilinear_mapping_matrix (64, 64)\n",
      "seq_len_tile (None, 5)\n",
      "mask (None, 5, 50) Tensor(\"capsule_layer_3/SequenceMask/Less:0\", shape=(None, 5, 50), dtype=bool)\n",
      "interest_capsules (None, 5, 64) Tensor(\"capsule_layer_3/mul_1:0\", shape=(None, 5, 64), dtype=float32)\n",
      "behavior_embdding_mapping (None, 50, 64) Tensor(\"capsule_layer_3/Tensordot:0\", shape=(None, 50, 64), dtype=float32)\n",
      "delta_routing_logits (1, 5, 50) Tensor(\"capsule_layer_3/Sum_1:0\", shape=(1, 5, 50), dtype=float32)\n",
      "mask (None, 5, 50) Tensor(\"capsule_layer_3/SequenceMask_1/Less:0\", shape=(None, 5, 50), dtype=bool)\n",
      "interest_capsules (None, 5, 64) Tensor(\"capsule_layer_3/mul_3:0\", shape=(None, 5, 64), dtype=float32)\n",
      "behavior_embdding_mapping (None, 50, 64) Tensor(\"capsule_layer_3/Tensordot_1:0\", shape=(None, 50, 64), dtype=float32)\n",
      "delta_routing_logits (1, 5, 50) Tensor(\"capsule_layer_3/Sum_3:0\", shape=(1, 5, 50), dtype=float32)\n",
      "mask (None, 5, 50) Tensor(\"capsule_layer_3/SequenceMask_2/Less:0\", shape=(None, 5, 50), dtype=bool)\n",
      "interest_capsules (None, 5, 64) Tensor(\"capsule_layer_3/mul_5:0\", shape=(None, 5, 64), dtype=float32)\n",
      "behavior_embdding_mapping (None, 50, 64) Tensor(\"capsule_layer_3/Tensordot_2:0\", shape=(None, 50, 64), dtype=float32)\n",
      "delta_routing_logits (1, 5, 50) Tensor(\"capsule_layer_3/Sum_5:0\", shape=(1, 5, 50), dtype=float32)\n",
      "high_capsule:  KerasTensor(type_spec=TensorSpec(shape=(None, 5, 64), dtype=tf.float32, name=None), name='capsule_layer_3/Reshape:0', description=\"created by layer 'capsule_layer_3'\")\n",
      "other_user_embedding_layer0 KerasTensor(type_spec=TensorSpec(shape=(None, 1, 384), dtype=tf.float32, name=None), name='concatenate_9/concat:0', description=\"created by layer 'concatenate_9'\")\n",
      "other_user_embedding_layer:  KerasTensor(type_spec=TensorSpec(shape=(None, 5, 384), dtype=tf.float32, name=None), name='tf.tile_3/Tile:0', description=\"created by layer 'tf.tile_3'\")\n",
      "user_deep_input:  KerasTensor(type_spec=TensorSpec(shape=(None, 5, 448), dtype=tf.float32, name=None), name='concatenate_10/concat:0', description=\"created by layer 'concatenate_10'\")\n",
      "user_deep_input:  KerasTensor(type_spec=TensorSpec(shape=(None, 5, 64), dtype=tf.float32, name=None), name='FC_2/Relu:0', description=\"created by layer 'FC_2'\")\n",
      "keys Tensor(\"Placeholder:0\", shape=(None, 5, 64), dtype=float32)\n",
      "query Tensor(\"Placeholder_1:0\", shape=(None, 1, 64), dtype=float32)\n",
      "weight Tensor(\"label_aware_attention_3/Pow:0\", shape=(None, 5, 1), dtype=float32)\n",
      "k_user Tensor(\"label_aware_attention_3/Cast_1:0\", shape=(None, 1), dtype=int64)\n",
      "seq_mask Tensor(\"label_aware_attention_3/transpose:0\", shape=(None, 5, 1), dtype=bool)\n",
      "padding Tensor(\"label_aware_attention_3/mul_1:0\", shape=(None, 5, 1), dtype=float32)\n",
      "weight Tensor(\"label_aware_attention_3/SelectV2:0\", shape=(None, 5, 1), dtype=float32)\n",
      "attention output Tensor(\"label_aware_attention_3/Sum_1:0\", shape=(None, 64), dtype=float32)\n",
      "user_embedding_final0:  KerasTensor(type_spec=TensorSpec(shape=(None, 64), dtype=tf.float32, name=None), name='label_aware_attention_3/Sum_1:0', description=\"created by layer 'label_aware_attention_3'\")\n",
      "user_embedding_final:  KerasTensor(type_spec=TensorSpec(shape=(None, 1, 64), dtype=tf.float32, name=None), name='tf.expand_dims_3/ExpandDims:0', description=\"created by layer 'tf.expand_dims_3'\")\n",
      "item_embedding_layer:  KerasTensor(type_spec=TensorSpec(shape=(None, 64, 11), dtype=tf.float32, name=None), name='tf.compat.v1.transpose_3/transpose:0', description=\"created by layer 'tf.compat.v1.transpose_3'\")\n",
      "dot_output KerasTensor(type_spec=TensorSpec(shape=(None, 1, 11), dtype=tf.float32, name=None), name='tf.nn.softmax_3/Softmax:0', description=\"created by layer 'tf.nn.softmax_3'\")\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "user_click_item_seq_input_layer [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_id_input_layer (InputLayer [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gender_input_layer (InputLayer) [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "age_input_layer (InputLayer)    [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "occupation_input_layer (InputLa [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zip_input_layer (InputLayer)    [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_id_embedding_layer (Embedd multiple             237248      pos_item_sample_input_layer[0][0]\n",
      "                                                                 neg_item_sample_input_layer[0][0]\n",
      "                                                                 user_click_item_seq_input_layer[0\n",
      "__________________________________________________________________________________________________\n",
      "user_click_item_seq_length_inpu [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_id_embedding_layer (Embedd (None, 1, 64)        386624      user_id_input_layer[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "gender_embedding_layer (Embeddi (None, 1, 64)        192         gender_input_layer[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "age_embedding_layer (Embedding) (None, 1, 64)        512         age_input_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "occupation_embedding_layer (Emb (None, 1, 64)        1408        occupation_input_layer[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zip_embedding_layer (Embedding) (None, 1, 64)        220160      zip_input_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sequence_pooling_layer_3 (Seque (None, 1, 64)        0           item_id_embedding_layer[2][0]    \n",
      "                                                                 user_click_item_seq_length_input_\n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 1, 384)       0           user_id_embedding_layer[0][0]    \n",
      "                                                                 gender_embedding_layer[0][0]     \n",
      "                                                                 age_embedding_layer[0][0]        \n",
      "                                                                 occupation_embedding_layer[0][0] \n",
      "                                                                 zip_embedding_layer[0][0]        \n",
      "                                                                 sequence_pooling_layer_3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf.tile_3 (TFOpLambda)          (None, 5, 384)       0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "capsule_layer_3 (CapsuleLayer)  (None, 5, 64)        4346        item_id_embedding_layer[2][0]    \n",
      "                                                                 user_click_item_seq_length_input_\n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 5, 448)       0           tf.tile_3[0][0]                  \n",
      "                                                                 capsule_layer_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "FC_1 (Dense)                    (None, 5, 128)       57472       concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "pos_item_sample_input_layer (In [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "neg_item_sample_input_layer (In [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FC_2 (Dense)                    (None, 5, 64)        8256        FC_1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "label_aware_attention_3 (LabelA (None, 64)           0           FC_2[0][0]                       \n",
      "                                                                 item_id_embedding_layer[0][0]    \n",
      "                                                                 user_click_item_seq_length_input_\n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 11, 64)       0           item_id_embedding_layer[0][0]    \n",
      "                                                                 item_id_embedding_layer[1][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_3 (TFOpLambda)   (None, 1, 64)        0           label_aware_attention_3[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose_3 (TFOpL (None, 64, 11)       0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf.linalg.matmul_3 (TFOpLambda) (None, 1, 11)        0           tf.expand_dims_3[0][0]           \n",
      "                                                                 tf.compat.v1.transpose_3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.softmax_3 (TFOpLambda)    (None, 1, 11)        0           tf.linalg.matmul_3[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 916,218\n",
      "Trainable params: 915,968\n",
      "Non-trainable params: 250\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "seq_len_tile (None, 5)\n",
      "mask (None, 5, 50) Tensor(\"model_4/capsule_layer_3/SequenceMask/Less:0\", shape=(None, 5, 50), dtype=bool)\n",
      "interest_capsules (None, 5, 64) Tensor(\"model_4/capsule_layer_3/mul_1:0\", shape=(None, 5, 64), dtype=float32)\n",
      "behavior_embdding_mapping (None, 50, 64) Tensor(\"model_4/capsule_layer_3/Tensordot:0\", shape=(None, 50, 64), dtype=float32)\n",
      "delta_routing_logits (1, 5, 50) Tensor(\"model_4/capsule_layer_3/Sum_1:0\", shape=(1, 5, 50), dtype=float32)\n",
      "mask (None, 5, 50) Tensor(\"model_4/capsule_layer_3/SequenceMask_1/Less:0\", shape=(None, 5, 50), dtype=bool)\n",
      "interest_capsules (None, 5, 64) Tensor(\"model_4/capsule_layer_3/mul_3:0\", shape=(None, 5, 64), dtype=float32)\n",
      "behavior_embdding_mapping (None, 50, 64) Tensor(\"model_4/capsule_layer_3/Tensordot_1:0\", shape=(None, 50, 64), dtype=float32)\n",
      "delta_routing_logits (1, 5, 50) Tensor(\"model_4/capsule_layer_3/Sum_3:0\", shape=(1, 5, 50), dtype=float32)\n",
      "mask (None, 5, 50) Tensor(\"model_4/capsule_layer_3/SequenceMask_2/Less:0\", shape=(None, 5, 50), dtype=bool)\n",
      "interest_capsules (None, 5, 64) Tensor(\"model_4/capsule_layer_3/mul_5:0\", shape=(None, 5, 64), dtype=float32)\n",
      "behavior_embdding_mapping (None, 50, 64) Tensor(\"model_4/capsule_layer_3/Tensordot_2:0\", shape=(None, 50, 64), dtype=float32)\n",
      "delta_routing_logits (1, 5, 50) Tensor(\"model_4/capsule_layer_3/Sum_5:0\", shape=(1, 5, 50), dtype=float32)\n",
      "(6040, 5, 64)\n",
      "(3707, 1, 64)\n",
      "[[0.         0.20871578 0.         0.         0.03589481 0.\n",
      "  0.         0.05215715 0.         0.09314882 0.         0.6770794\n",
      "  0.11630171 0.408412   0.03777155 0.39042467 0.         0.\n",
      "  0.         0.08782773 0.         0.         0.03318495 0.\n",
      "  0.         0.25599673 0.39499834 0.65712076 0.11369517 0.\n",
      "  0.         0.         0.3420964  0.46303645 0.         0.\n",
      "  0.07366497 0.         0.29438615 0.0760963  0.         0.\n",
      "  0.01230427 0.         0.         0.         0.29715106 0.\n",
      "  0.36271635 0.         0.         0.         0.07745764 0.18170162\n",
      "  0.00634064 0.         0.9200518  0.1799115  0.         0.16000095\n",
      "  0.33340588 0.14461489 0.         0.37726238]\n",
      " [0.05527842 0.         0.         0.         0.         0.\n",
      "  0.         0.19234872 0.         0.         0.20009531 0.7901488\n",
      "  0.07544748 0.3666886  0.11759795 0.410903   0.         0.08796233\n",
      "  0.         0.         0.         0.04264273 0.         0.08689511\n",
      "  0.         0.27569613 0.04196722 0.50486815 0.         0.\n",
      "  0.         0.         0.09251625 0.4679188  0.         0.\n",
      "  0.16150756 0.10648215 0.34384802 0.07691985 0.         0.00112855\n",
      "  0.07588112 0.10930634 0.0729043  0.         0.3361654  0.\n",
      "  0.18500184 0.         0.1602327  0.         0.32493547 0.38840717\n",
      "  0.         0.         0.7887809  0.05445872 0.         0.1084094\n",
      "  0.08329149 0.12695032 0.         0.07892305]]\n",
      "(3707, 64)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from data_generator import init_output\n",
    "\n",
    "\n",
    "\n",
    "# 1. Load model\n",
    "\n",
    "re_model = mind()\n",
    "re_model.load_weights('mind_model.h5')\n",
    "\n",
    "print(re_model.summary())\n",
    "\n",
    "\n",
    "\n",
    "# 2. Load data\n",
    "\n",
    "user_id, gender, age, occupation, zip, \\\n",
    "        hist_movie_id, hist_len, pos_movie_id, neg_movie_id = init_output()\n",
    "\n",
    "with open(\"test.txt\", 'r') as f:\n",
    "    for line in f.readlines():\n",
    "\n",
    "        buf = line.strip().split('\\t')\n",
    "\n",
    "        user_id.append(int(buf[0]))\n",
    "        gender.append(int(buf[1]))\n",
    "        age.append(int(buf[2]))\n",
    "        occupation.append(int(buf[3]))\n",
    "        zip.append(int(buf[4]))\n",
    "        hist_movie_id.append(np.array([int(i) for i in buf[5].strip().split(\",\")]))\n",
    "        hist_len.append(int(buf[6]))\n",
    "        pos_movie_id.append(int(buf[7]))\n",
    "        \n",
    "\n",
    "user_id = np.array(user_id, dtype='int32')\n",
    "gender = np.array(gender, dtype='int32')\n",
    "age = np.array(age, dtype='int32')\n",
    "occupation = np.array(occupation, dtype='int32')\n",
    "zip = np.array(zip, dtype='int32')\n",
    "hist_movie_id = np.array(hist_movie_id, dtype='int32')\n",
    "hist_len = np.array(hist_len, dtype='int32')\n",
    "pos_movie_id = np.array(pos_movie_id, dtype='int32')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 3. Generate user features for testing and full item features for retrieval\n",
    "\n",
    "test_user_model_input = [user_id, gender, age, occupation, zip, hist_movie_id, hist_len]\n",
    "all_item_model_input = list(range(0, 3706+1))\n",
    "\n",
    "user_embedding_model = Model(inputs=re_model.user_input, outputs=re_model.user_embedding)\n",
    "item_embedding_model = Model(inputs=re_model.item_input, outputs=re_model.item_embedding)\n",
    "\n",
    "user_embs = user_embedding_model.predict(test_user_model_input)\n",
    "item_embs = item_embedding_model.predict(all_item_model_input, batch_size=2 ** 12)\n",
    "\n",
    "print(user_embs.shape)\n",
    "print(item_embs.shape)\n",
    "\n",
    "\n",
    "user_embs = np.reshape(user_embs, (-1, 64))\n",
    "item_embs = np.reshape(item_embs, (-1, 64))\n",
    "\n",
    "print(user_embs[:2])\n",
    "print(item_embs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9e65a3-cfaa-43f3-8fc6-111901bb12e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470171bd-1acc-415f-b7dd-da592797c8dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_tf26_cpu",
   "language": "python",
   "name": "py37_tf26_cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
