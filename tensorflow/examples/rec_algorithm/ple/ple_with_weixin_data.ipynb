{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "607fc475-a204-48d3-b80e-8443d2deefe6",
   "metadata": {},
   "source": [
    "## 多任务学习之PLE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eceb9ed-0023-47a2-a8c9-0755095df3c1",
   "metadata": {},
   "source": [
    "## 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdb5cb82-4b7e-4a92-b683-98d3c61be011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "## 样本处理逻辑文件在：../../data_process/wechat_algo_data_process.ipynb\n",
    "## 原始数据在：../../../data/wechat_algo_data/\n",
    "\n",
    "# 全局参数定义\n",
    "data_dir = \"../../../data/wechat_algo_data/middle_data_and_feature\"\n",
    "train_output_path = os.path.join(data_dir, \"train.txt\")\n",
    "val_output_path = os.path.join(data_dir, \"val.txt\")\n",
    "test_output_path = os.path.join(data_dir, \"test.txt\")\n",
    "encoder_output_path = os.path.join(data_dir, \"encoder.txt\")\n",
    "\n",
    "train = joblib.load(train_output_path)\n",
    "val = joblib.load(val_output_path)\n",
    "test = joblib.load(test_output_path)\n",
    "encoder = joblib.load(encoder_output_path)\n",
    "\n",
    "train_num = len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2302c568-4733-45c2-a4b5-c5096dfce6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import gc\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.python.keras.layers import Layer\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from tensorflow.keras.models import Model,load_model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,ReduceLROnPlateau,EarlyStopping\n",
    "\n",
    "from tensorflow.keras import optimizers,initializers\n",
    "from tensorflow.python.keras.initializers import glorot_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708e9910-0aee-4e86-b21f-210192f09aaf",
   "metadata": {},
   "source": [
    "### 定义PLE模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c2c9ef7-9bc3-47ef-8681-16e0ada39356",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanPoolLayer(Layer):\n",
    "    def __init__(self, axis, **kwargs):\n",
    "        super(MeanPoolLayer, self).__init__(**kwargs)\n",
    "        self.axis = axis\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        mask = tf.expand_dims(tf.cast(mask,tf.float32),axis = -1)\n",
    "        x = x * mask\n",
    "        return K.sum(x, axis=self.axis) / (K.sum(mask, axis=self.axis) + 1e-9)\n",
    "\n",
    "class PleLayer(tf.keras.layers.Layer):\n",
    "    '''\n",
    "    n_experts:list,每个任务使用几个expert。[2,3]第一个任务使用2个expert，第二个任务使用3个expert。\n",
    "    n_expert_share:int,共享的部分设置的expert个数。\n",
    "    expert_dim:int,每个专家网络输出的向量维度。\n",
    "    n_task:int,任务个数。\n",
    "    '''\n",
    "    def __init__(self,n_task,n_experts,expert_dim,n_expert_share,dnn_reg_l2 = 1e-5):\n",
    "        super(PleLayer, self).__init__()\n",
    "        self.n_task = n_task\n",
    "        \n",
    "        # 生成多个任务特定网络和1个共享网络。\n",
    "        self.E_layer = []\n",
    "        for i in range(n_task):\n",
    "            sub_exp = [Dense(expert_dim,activation = 'relu') for j in range(n_experts[i])]\n",
    "            self.E_layer.append(sub_exp)\n",
    "            \n",
    "        self.share_layer = [Dense(expert_dim,activation = 'relu') for j in range(n_expert_share)]\n",
    "        #定义门控网络\n",
    "        self.gate_layers = [Dense(n_expert_share+n_experts[i],kernel_regularizer=regularizers.l2(dnn_reg_l2),\n",
    "                                  activation = 'softmax') for i in range(n_task)]\n",
    "\n",
    "    def call(self,x):\n",
    "        #特定网络和共享网络\n",
    "        E_net = [[expert(x) for expert in sub_expert] for sub_expert in self.E_layer]\n",
    "        share_net = [expert(x) for expert in self.share_layer]\n",
    "        \n",
    "        #门的权重乘上，指定任务和共享任务的输出。\n",
    "        towers = []\n",
    "        for i in range(self.n_task):\n",
    "            g = self.gate_layers[i](x)\n",
    "            g = tf.expand_dims(g,axis = -1) #(bs,n_expert_share+n_experts[i],1)\n",
    "            _e = share_net+E_net[i]  \n",
    "            _e = Concatenate(axis = 1)([expert[:,tf.newaxis,:] for expert in _e]) #(bs,n_expert_share+n_experts[i],expert_dim)\n",
    "            _tower = tf.matmul(_e, g,transpose_a=True)\n",
    "            towers.append(Flatten()(_tower)) #(bs,expert_dim)\n",
    "        return towers\n",
    "\n",
    "def build_ple(sparse_cols,dense_cols,sparse_max_len,embed_dim,expert_dim = 4,\n",
    "              varlens_cols = [],varlens_max_len = [],dnn_hidden_units = (64,64),\n",
    "              n_task = 2,n_experts = [2,2],n_expert_share = 4,dnn_reg_l2 = 1e-6,\n",
    "              drop_rate = 0.0,embedding_reg_l2 = 1e-6,targets = []):\n",
    "\n",
    "   #输入部分，分为sparse,varlens,dense部分。\n",
    "    sparse_inputs = {f:Input([1],name = f) for f in sparse_cols}\n",
    "    dense_inputs = {f:Input([1],name = f) for f in dense_cols}\n",
    "    varlens_inputs = {f:Input([None,1],name = f) for f in varlens_cols}\n",
    "        \n",
    "    input_embed = {}\n",
    "    #离散特征，embedding到k维\n",
    "    for f in sparse_cols:\n",
    "        _input = sparse_inputs[f]\n",
    "        embedding = Embedding(sparse_max_len[f], embed_dim, \n",
    "            embeddings_regularizer=tf.keras.regularizers.l2(embedding_reg_l2)) \n",
    "        input_embed[f] =Flatten()(embedding(_input)) #(bs,k)\n",
    "        \n",
    "    #多标签离散变量\n",
    "    for f in varlens_inputs:\n",
    "        _input = varlens_inputs[f]\n",
    "        mask = Masking(mask_value = 0).compute_mask(_input)\n",
    "        embedding = Embedding(varlens_max_len[f], embed_dim,\n",
    "            embeddings_regularizer=tf.keras.regularizers.l2(1e-6))\n",
    "        _embed =Reshape([-1,embed_dim])(embedding(_input))\n",
    "        out_embed = MeanPoolLayer(axis=1)(_embed,mask)\n",
    "        input_embed[f] = out_embed\n",
    "        \n",
    "    input_embed.update(dense_inputs) #加入连续变量\n",
    "    input_embed = Concatenate(axis = -1)([input_embed[f] for f in input_embed])    \n",
    "                                  \n",
    "    for num in dnn_hidden_units:\n",
    "        input_embed = Dropout(drop_rate)(Dense(num,activation = 'relu',\n",
    "                    kernel_regularizer=regularizers.l2(dnn_reg_l2))(input_embed))\n",
    "    #Ple网络层\n",
    "    towers = PleLayer(n_task,n_experts,expert_dim,n_expert_share)(input_embed)\n",
    "    outputs = [Dense(1,activation = 'sigmoid',kernel_regularizer=regularizers.l2(dnn_reg_l2),\n",
    "                       name = f,use_bias = True)(_t) for f,_t in zip(targets,towers)]\n",
    "    inputs = [sparse_inputs[f] for f in sparse_inputs]+[varlens_inputs[f] for f in varlens_inputs]\\\n",
    "                +[dense_inputs[f] for f in dense_inputs]\n",
    "    model = Model(inputs,outputs) \n",
    "    \n",
    "    return model   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dace6b1a-e0cc-4528-abbc-11a5ed6163a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-01 16:10:02.086039: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib:/usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda-9.0/lib64:/usr/local/cuda-9.0/extras/CUPTI/lib64:/usr/local/cuda-8.0/lib64:/usr/local/cuda-8.0/extras/CUPTI/lib64:/usr/lib:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/lib:/usr/local/lib:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/lib:/usr/local/lib:/usr/local/cuda-11.0/lib64:/usr/local/cuda-11.0/extras/CUPTI/lib64/:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/lib:/usr/local/lib:/usr/local/cuda-11.0/lib64:/usr/local/cuda-11.0/extras/CUPTI/lib64/:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/lib:/usr/local/lib\n",
      "2022-11-01 16:10:02.093866: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-11-01 16:10:02.093905: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (9-223-244-157): /proc/driver/nvidia/version does not exist\n",
      "2022-11-01 16:10:02.098100: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-01 16:10:02.754226: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "6552/6552 [==============================] - 766s 116ms/step - loss: 0.2803 - read_comment_loss: 0.0973 - like_loss: 0.0951 - click_avatar_loss: 0.0390 - forward_loss: 0.0221 - read_comment_auc: 0.9163 - like_auc: 0.8284 - click_avatar_auc: 0.7800 - forward_auc: 0.7668 - val_loss: 0.2734 - val_read_comment_loss: 0.0967 - val_like_loss: 0.0920 - val_click_avatar_loss: 0.0378 - val_forward_loss: 0.0198 - val_read_comment_auc: 0.9109 - val_like_auc: 0.8197 - val_click_avatar_auc: 0.7929 - val_forward_auc: 0.7639\n",
      "Epoch 2/4\n",
      "6552/6552 [==============================] - 759s 116ms/step - loss: 0.2706 - read_comment_loss: 0.0938 - like_loss: 0.0920 - click_avatar_loss: 0.0371 - forward_loss: 0.0210 - read_comment_auc: 0.9253 - like_auc: 0.8457 - click_avatar_auc: 0.8138 - forward_auc: 0.7987 - val_loss: 0.2703 - val_read_comment_loss: 0.0961 - val_like_loss: 0.0910 - val_click_avatar_loss: 0.0377 - val_forward_loss: 0.0195 - val_read_comment_auc: 0.9131 - val_like_auc: 0.8233 - val_click_avatar_auc: 0.7909 - val_forward_auc: 0.7749\n",
      "Epoch 3/4\n",
      "6552/6552 [==============================] - 756s 115ms/step - loss: 0.2686 - read_comment_loss: 0.0936 - like_loss: 0.0915 - click_avatar_loss: 0.0369 - forward_loss: 0.0208 - read_comment_auc: 0.9259 - like_auc: 0.8485 - click_avatar_auc: 0.8161 - forward_auc: 0.8052 - val_loss: 0.2695 - val_read_comment_loss: 0.0964 - val_like_loss: 0.0910 - val_click_avatar_loss: 0.0371 - val_forward_loss: 0.0195 - val_read_comment_auc: 0.9103 - val_like_auc: 0.8220 - val_click_avatar_auc: 0.7988 - val_forward_auc: 0.7677\n",
      "Epoch 4/4\n",
      "6552/6552 [==============================] - 759s 116ms/step - loss: 0.2673 - read_comment_loss: 0.0933 - like_loss: 0.0914 - click_avatar_loss: 0.0368 - forward_loss: 0.0207 - read_comment_auc: 0.9264 - like_auc: 0.8492 - click_avatar_auc: 0.8176 - forward_auc: 0.8097 - val_loss: 0.2671 - val_read_comment_loss: 0.0952 - val_like_loss: 0.0907 - val_click_avatar_loss: 0.0375 - val_forward_loss: 0.0192 - val_read_comment_auc: 0.9142 - val_like_auc: 0.8248 - val_click_avatar_auc: 0.7940 - val_forward_auc: 0.7747\n"
     ]
    }
   ],
   "source": [
    "target = [\"read_comment\", \"like\", \"click_avatar\", \"forward\"]\n",
    "sparse_features = ['userid', 'feedid', 'authorid', 'bgm_song_id', 'bgm_singer_id']\n",
    "varlen_features = ['manual_tag_list','manual_keyword_list']\n",
    "dense_features = ['videoplayseconds']\n",
    "\n",
    "# 生成输入特征设置\n",
    "sparse_max_len = {f:len(encoder[f]) + 1 for f in sparse_features}\n",
    "varlens_max_len = {f:len(encoder[f]) + 1 for f in varlen_features}\n",
    "feature_names = sparse_features+varlen_features+dense_features\n",
    "\n",
    "\n",
    "# 构建输入数据\n",
    "train_model_input = {name: train[name] if name not in varlen_features else np.stack(train[name]) for name in feature_names } #训练模型的输入，字典类型。名称和具体值\n",
    "val_model_input = {name: val[name] if name not in varlen_features else np.stack(val[name]) for name in feature_names }\n",
    "test_model_input = {name: test[name] if name not in varlen_features else np.stack(test[name]) for name in feature_names}\n",
    "\n",
    "train_labels = [train[y].values for y in target]\n",
    "val_labels = [val[y].values for y in target]\n",
    "\n",
    "# 删除多余的数据，释放内存\n",
    "del train,val\n",
    "gc.collect()\n",
    "\n",
    "# 构建模型，训练和评估\n",
    "model = build_ple(sparse_features,dense_features,sparse_max_len,embed_dim = 16,expert_dim = 32,\n",
    "          varlens_cols = varlen_features,varlens_max_len = varlens_max_len,dnn_hidden_units = (64,),\n",
    "          n_task = 4,n_experts = [4,4,4,4],n_expert_share = 8,dnn_reg_l2 = 1e-6,\n",
    "          drop_rate = 0.1,embedding_reg_l2 = 1e-6,targets = target)\n",
    "\n",
    "\n",
    "adam = optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(adam, loss = 'binary_crossentropy' ,metrics = [tf.keras.metrics.AUC()],)\n",
    "\n",
    "history = model.fit(train_model_input, train_labels,validation_data = (val_model_input,val_labels),\n",
    "                    batch_size=1024, epochs=4, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf34df7-7420-4637-8de8-713bd01a5a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0243ac79-2d2d-42fb-9cbc-bb100ce0fa0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b761d61-05b4-465d-beab-fcaae6d15062",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_tf26_cpu",
   "language": "python",
   "name": "py37_tf26_cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
