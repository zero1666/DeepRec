{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdd2bc03-57b0-437d-bf56-9a3228c6c8b9",
   "metadata": {},
   "source": [
    "# GraphSage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc122aad-99da-48a3-a007-714441dba8d5",
   "metadata": {},
   "source": [
    "## Cora 数据集处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08e0b0f7-0846-4206-ac1b-1ab9ff462162",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "435accd4-ff97-4302-9184-87581796884a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = namedtuple('Data', ['x', 'y', 'adjacency_dict', 'train_mask', 'val_mask', 'test_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9de489b1-be97-48b7-833b-0c4000b2bc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoraData():\n",
    "    def __init__(self, data_root='../../../data/cora/'):\n",
    "        self._data_root = data_root\n",
    "        self._data = self.process_data()\n",
    "    \n",
    "    def load_data(self, dataset='cora'):\n",
    "        print('Loading {} dataset ...'.format(dataset))\n",
    "        idx_features_labels = np.genfromtxt(\"{}{}.content\".format(self._data_root, dataset), dtype=np.dtype(str))\n",
    "        edges = np.genfromtxt(\"{}{}.cites\".format(self._data_root, dataset), dtype=np.int32)\n",
    "        return idx_features_labels, edges\n",
    "    \n",
    "    def process_data(self):\n",
    "        \n",
    "        print(\"Process data ...\")\n",
    "\n",
    "        idx_features_labels, edges = self.load_data()\n",
    "\n",
    "        features = idx_features_labels[:, 1:-1].astype(np.float32)\n",
    "        features = self.normalize_feature(features)\n",
    "\n",
    "        y = idx_features_labels[:, -1]\n",
    "        labels = self.encode_onehot(y)\n",
    "\n",
    "        idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "        \n",
    "        # print(edges.shape, edges[:3,:])\n",
    "\n",
    "        for self_idx in idx:\n",
    "            edges = np.vstack((edges, [self_idx, self_idx]))\n",
    "            \n",
    "        idx_map = {j: i for i, j in enumerate(idx)}\n",
    "        edge_indexs = np.array(list(map(idx_map.get, edges.flatten())), dtype=np.int32)\n",
    "        edge_indexs = edge_indexs.reshape(edges.shape)\n",
    "        \n",
    "        adjacency = {}\n",
    "        for edge in edge_indexs:\n",
    "            key = edge[0].astype(np.int32)\n",
    "            value = edge[1].astype(np.int32)\n",
    "\n",
    "            target_value = np.array([])\n",
    "            if key in adjacency.keys():\n",
    "                target_value = adjacency[key]\n",
    "\n",
    "            target_value = np.append(target_value, value)\n",
    "\n",
    "            adjacency.update({key : target_value})\n",
    "\n",
    "\n",
    "        train_index = np.arange(150)\n",
    "        val_index = np.arange(150, 500)\n",
    "        test_index = np.arange(500, 2708)\n",
    "\n",
    "        train_mask = np.zeros(edge_indexs.shape[0], dtype = np.bool)\n",
    "        val_mask = np.zeros(edge_indexs.shape[0], dtype = np.bool)\n",
    "        test_mask = np.zeros(edge_indexs.shape[0], dtype = np.bool)\n",
    "        train_mask[train_index] = True\n",
    "        val_mask[val_index] = True\n",
    "        test_mask[test_index] = True\n",
    "        #print(type(train_mask), train_mask.shape, train_mask)\n",
    "        \n",
    "        print('Dataset has {} nodes, {} edges, {} features'.format(features.shape[0], len(adjacency), features.shape[1]))\n",
    "        return Data(x=features, y = labels, adjacency_dict=adjacency, train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)\n",
    "    \n",
    "    def encode_onehot(self, labels):\n",
    "        classes = set(labels)\n",
    "        classes_dict = {c:np.identity(len(classes))[i,:] for i,c in enumerate(classes)}\n",
    "        labels_onehot = np.array(list(map(classes_dict.get, labels)), dtype=np.int32)\n",
    "        return labels_onehot\n",
    "    \n",
    "    def normalize_adj(self, adjacency):\n",
    "        \"\"\"计算 L=D^-0.5 * (A+I) * D^-0.5\"\"\"\n",
    "        adjacency += sp.eye(adjacency.shape[0])\n",
    "        degreee = np.aray(adjacency.sum(1))\n",
    "        d_hat = sp.diags(np.power(degree, -0.5).flatten())\n",
    "        return d_hat.dot(adjacency).dot(d_hat).tocsr().todense()\n",
    "    \n",
    "    def normalize_feature(self, features):\n",
    "        normal_feature = features / features.sum(1).reshape(-1,1)\n",
    "        #print(normal_feature.shape, features.sum(1))\n",
    "        return normal_feature\n",
    "    \n",
    "    def data(self):\n",
    "        \"\"\"返回Data数据对象，包括features, labes, adjacency, train_mask, val_mask, test_mask\"\"\"\n",
    "        return self._data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fffeb42e-d491-44a1-8d2d-b9654f792486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process data ...\n",
      "Loading cora dataset ...\n",
      "Dataset has 2708 nodes, 2708 edges, 1433 features\n"
     ]
    }
   ],
   "source": [
    "data = CoraData().data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04f5319-c27e-4703-99e4-2fa30414f3c2",
   "metadata": {},
   "source": [
    "## 采样邻居\n",
    "GNN模型中，图的信息聚合过程是沿着Graph Edge进行的，GNN中节点在第(k+1)层的特征只与其在(k)层的邻居有关，这种局部性质使得节点在(k)层的特征只与自己的k阶子图有关。因此对于k层网络，只需要采样Graph的k阶子图，就可以满足训练的需求。\n",
    "\n",
    "同时为了提升运算效率，对每个顶点的邻居节点进行有放回的采样，保证每个节点邻居个数都是相同的。这也是神经网络中处理数据的一种常用的策"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6922e92-e35a-4ff3-b07c-8f4dc383d99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sampling(src_nodes, sample_num, neighbor_table):\n",
    "    \"\"\"根据源节点采样指定数量的邻居节点，注意使用的是有放回的采样；\n",
    "    某个节点的邻居节点数量少于采样数量时，采样结果出现重复的节点\n",
    "    \n",
    "    Arguments:\n",
    "        src_nodes {list, ndarray} -- 源节点列表\n",
    "        sample_num {int} -- 需要采样的节点数\n",
    "        neighbor_table {dict} -- 节点到其邻居节点的映射表\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray -- 采样结果构成的列表\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for node in src_nodes:\n",
    "        result = np.random.choice(neighbor_table[node], size = (sample_num,))\n",
    "        results.append(result)\n",
    "    return np.asarray(results).flatten()\n",
    "\n",
    "def multihop_sampling(src_nodes, sample_nums, neighbor_table):\n",
    "    \"\"\"根据源节点进行多阶采样\n",
    "    \n",
    "    Arguments:\n",
    "        src_nodes {list, np.ndarray} -- 源节点id\n",
    "        sample_nums {list of int} -- 每一阶需要采样的个数\n",
    "        neighbor_table {dict} -- 节点到其邻居节点的映射\n",
    "    \n",
    "    Returns:\n",
    "        [list of ndarray] -- 每一阶采样的结果\n",
    "    \"\"\"\n",
    "    #print('src_nodes', src_nodes.size)\n",
    "    #print('sample_nums', sample_nums)\n",
    "    #print('neighbor_table', len(neighbor_table))\n",
    "    sampling_result = [src_nodes]\n",
    "    for k, hopk_num in enumerate(sample_nums):\n",
    "        hopk_result = sampling(sampling_result[k], hopk_num, neighbor_table)\n",
    "        #print('hopk_result', len(hopk_result), hopk_result)\n",
    "        sampling_result.append(hopk_result)\n",
    "    return sampling_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177d624d-aea5-43c8-9a89-3a4931327a81",
   "metadata": {},
   "source": [
    "## 邻居节点聚合器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d10dc881-c936-48b6-a6f3-1071eb10eea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class NeighborAggregator(tf.keras.Model):\n",
    "    def __init__(self,input_dim, output_dim,\n",
    "                use_bias=False, aggr_method=\"mean\"):\n",
    "        \"\"\"聚合节点邻居\n",
    "        Args:\n",
    "            input_dim: 输入特征的维度\n",
    "            output_dim: 输出特征的维度\n",
    "            use_bias: 是否使用偏置 (default: {False})\n",
    "            aggr_method: 邻居聚合方式 (default: {mean})\n",
    "        \"\"\"\n",
    "        super(NeighborAggregator, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.use_bias = use_bias\n",
    "        self.aggr_method = aggr_method\n",
    "        \n",
    "        self.weight = self.add_weight(shape = (self.input_dim, self.output_dim),\n",
    "                                     initializer = 'glorot_uniform', name = 'kernel')\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(shape =(self.output_dim,),\n",
    "                                       initializer = 'zero',\n",
    "                                       name = 'bias')\n",
    "            \n",
    "    def call(self, neighbor_feature):\n",
    "        if self.aggr_method == \"mean\":\n",
    "            aggr_neighbor = tf.math.reduce_mean(neighbor_feature, axis = 1)\n",
    "        elif self.aggr_method == \"sum\":\n",
    "            aggr_neighbor = tf.math.reduce_sum(neighbor_feature, axis = 1)\n",
    "        elif self.aggr_method == \"max\":\n",
    "            aggr_neighbor = tf.math.reduce_max(neighbor_feature, axis = 1)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown aggr type, expected sum, max, or mean, but got {}\"\n",
    "                             .format(self.aggr_method))\n",
    "            \n",
    "        neighbor_hidden = tf.matmul(aggr_neighbor, self.weight)\n",
    "        if self.use_bias:\n",
    "            #print('neigbor_hidden', neighbor_hidden.shape, 'bias', self.bias.shape)\n",
    "            neighbor_hidden +=self.bias\n",
    "        \n",
    "        return neighbor_hidden\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c9271b9-6a95-4ae8-9df2-50cbd2460bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-20 18:35:41.309495: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib64:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/lib/native/:/jre/lib/amd64/:/usr/local/lib64:/usr/local/lib:/usr/local/tensorflow/lib:/usr/local/mysql/lib:/usr/local/jdk/jre/lib/amd64/server:/usr/local/mkl/lib:/usr/local/hadoop/lib/native:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/lib/native/:/jre/lib/amd64/:/usr/local/lib64:/usr/local/lib:/usr/local/tensorflow/lib:/usr/local/mysql/lib:/usr/local/jdk/jre/lib/amd64/server:/usr/local/mkl/lib:/usr/local/hadoop/lib/native:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/lib/native/:/jre/lib/amd64/:/usr/local/lib64:/usr/local/lib:/usr/local/tensorflow/lib:/usr/local/mysql/lib:/usr/local/jdk/jre/lib/amd64/server:/usr/local/mkl/lib:/usr/local/hadoop/lib/native:/usr/lib:/usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda-9.0/lib64:/usr/local/cuda-9.0/extras/CUPTI/lib64:/usr/local/cuda-8.0/lib64:/usr/local/cuda-8.0/extras/CUPTI/lib64:/usr/lib:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/lib:/usr/local/lib:/usr/local/app/.local/lib/python3.6/site-packages/numerous_pywrap:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/lib:/usr/local/lib:/usr/local/app/.local/lib/python3.6/site-packages/numerous_pywrap:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/lib:/usr/local/lib:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/lib:/usr/local/lib\n",
      "2022-07-20 18:35:41.309533: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-07-20 18:35:41.309577: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (9-223-245-158): /proc/driver/nvidia/version does not exist\n",
      "2022-07-20 18:35:41.309789: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.NeighborAggregator at 0x7f11966e1f50>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " NeighborAggregator(input_dim=1433, output_dim=128,use_bias=True, aggr_method=\"mean\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e599d030-e672-44c1-9f75-01ce5a2e5a14",
   "metadata": {},
   "source": [
    "定义模型结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1aa84b8-cb14-4aaa-9025-fa3acff61915",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SageGCN(tf.keras.Model):\n",
    "    def __init__(self, input_dim, hidden_dim,\n",
    "                activation = tf.keras.activations.relu,\n",
    "                aggr_neighbor_method = \"mean\",\n",
    "                aggr_hidden_method = \"sum\"):\n",
    "        \"\"\"SageGCN层定义\n",
    "        Args:\n",
    "            input_dim: 输入特征的维度\n",
    "            hidden_dim: 隐层特征的维度，\n",
    "                当aggr_hidden_method=sum, 输出维度为hidden_dim\n",
    "                当aggr_hidden_method=concat, 输出维度为hidden_dim*2\n",
    "            activation: 激活函数\n",
    "            aggr_neighbor_method: 邻居特征聚合方法，[\"mean\", \"sum\", \"max\"]\n",
    "            aggr_hidden_method: 节点特征的更新方法，[\"sum\", \"concat\"]\n",
    "        \"\"\"\n",
    "        super(SageGCN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.aggr_neighbor_method = aggr_neighbor_method\n",
    "        self.aggr_hidden_method = aggr_hidden_method\n",
    "        self.activation = activation\n",
    "        \n",
    "        self.aggregator = NeighborAggregator(input_dim, hidden_dim, use_bias=True,aggr_method=aggr_neighbor_method)\n",
    "        \n",
    "        self.weight = self.add_weight(shape=(self.input_dim, self.hidden_dim),\n",
    "                                     initializer = 'glorot_uniform',\n",
    "                                     name = 'kernel')\n",
    "        \n",
    "    def call(self, src_node_features, neighbor_node_features):\n",
    "        #print('src_node_features', (src_node_features).shape)\n",
    "        #print('neighbor_node_features', (neighbor_node_features).shape)\n",
    "        neighbor_hidden = self.aggregator(neighbor_node_features)\n",
    "        self_hidden = tf.matmul(src_node_features, self.weight)\n",
    "        \n",
    "        if self.aggr_hidden_method == \"sum\":\n",
    "            hidden = self_hidden + neighbor_hidden\n",
    "        elif self.aggr_hidden_method == \"concat\":\n",
    "            hidden = tf.concat(1, [self_hidden, neighbor_hidden])\n",
    "        else:\n",
    "            raise ValueError(\"Expected sum or concat, got {}\"\n",
    "                             .format(self.aggr_hidden_method))\n",
    "        #print('hidden', hidden.shape)\n",
    "        if self.activation:\n",
    "            return self.activation(hidden)\n",
    "        else:\n",
    "            return hidden\n",
    "    \n",
    "\n",
    "class GraphSage(tf.keras.Model):\n",
    "    def __init__(self, input_dim, hidden_dim,\n",
    "                 num_neighbors_list):\n",
    "\n",
    "        super(GraphSage, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_neighbors_list = num_neighbors_list\n",
    "        self.num_layers = len(num_neighbors_list)\n",
    "        self.gcn = []\n",
    "        self.gcn.append(SageGCN(input_dim, hidden_dim[0]))\n",
    "        \n",
    "        for index in range(0, len(hidden_dim) - 2):\n",
    "            self.gcn.append(SageGCN(hidden_dim[index], hidden_dim[index + 1]))\n",
    "        \n",
    "        self.gcn.append(SageGCN(hidden_dim[-2], hidden_dim[-1], activation=None))\n",
    "\n",
    "    def call(self, node_features_list):\n",
    "        hidden = node_features_list\n",
    "        #print('gcn len', len(self.gcn), self.num_layers, len(node_features_list))\n",
    "        \n",
    "        for l in range(self.num_layers):\n",
    "            next_hidden = []\n",
    "            gcn = self.gcn[l]\n",
    "            for hop in range(self.num_layers - l):\n",
    "                src_node_features = hidden[hop]\n",
    "                src_node_num = len(src_node_features)\n",
    "                #print('src_node_num',src_node_num, src_node_features.shape)\n",
    "                neighbor_node_features = tf.reshape(hidden[hop + 1], (src_node_num, self.num_neighbors_list[hop], -1))\n",
    "                #print('neighbor_node_features',neighbor_node_features.shape, hidden[hop + 1].shape,self.num_neighbors_list[hop] )\n",
    "                h = gcn(src_node_features, neighbor_node_features)\n",
    "                next_hidden.append(h)\n",
    "            hidden = next_hidden\n",
    "            #print('out_hidden', type(hidden), len(hidden), hidden[0].shape)\n",
    "        #print('final_out_hidden', type(hidden), len(hidden), hidden[0].shape)\n",
    "        return hidden[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754a4e98-4188-4dd6-89b3-aaea5f2deaa5",
   "metadata": {},
   "source": [
    "## 模型训练\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8dba37a9-83b7-4e52-8354-b15ca670aec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "INPUT_DIM = 1433 # 输入特征纬度\n",
    "# Note: 采样的邻居阶数需要与GCN的层数保持一致\n",
    "HIDDEM_DIM = [128, 7] # 隐藏单元节点数目\n",
    "NUM_NEIGHBORS_LIST = [10,10] # 每阶采样，采样的邻居节点数\n",
    "\n",
    "assert len(HIDDEM_DIM) == len(NUM_NEIGHBORS_LIST)\n",
    "BTACH_SIZE = 16 # batchsize 大小\n",
    "EPOCHS= 20\n",
    "NUM_BATCH_PER_EPOCH = 20 # 每个epoch，循环的批次数\n",
    "LEARNING_RATE = 0.01 # 学习率\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b210ec5d-b691-46c4-90b3-0759619638d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model =GraphSage(input_dim = INPUT_DIM,\n",
    "                hidden_dim = HIDDEM_DIM,\n",
    "                num_neighbors_list = NUM_NEIGHBORS_LIST)\n",
    "loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE, decay=5e-4)\n",
    "\n",
    "\n",
    "train_index = np.where(data.train_mask)[0]\n",
    "train_label = data.y[train_index]\n",
    "\n",
    "test_index = np.where(data.test_mask)[0]\n",
    "val_index = np.where(data.val_mask)[0]\n",
    "\n",
    "# 记录过程，方便可视化\n",
    "train_loss_results = []\n",
    "train_accuracy_results = []\n",
    "train_val_results = []\n",
    "train_test_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c362cad-4b1d-4789-b1b5-5b20c8e06e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150,) [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149]\n",
      "(150, 7) [[0 0 1 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 1]\n",
      " ...\n",
      " [0 0 0 ... 1 0 0]\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(train_index.shape, train_index)\n",
    "print(train_label.shape, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "952a331f-ff34-47ed-a6ac-31381ecc4350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    for e in range(EPOCHS):\n",
    "        for batch in range(NUM_BATCH_PER_EPOCH):\n",
    "            batch_src_index = np.random.choice(train_index, size=(BTACH_SIZE,))\n",
    "            batch_src_label = train_label[batch_src_index].astype(float)\n",
    "\n",
    "            batch_sampling_result = multihop_sampling(batch_src_index, NUM_NEIGHBORS_LIST, data.adjacency_dict)\n",
    "            batch_sampling_x = [data.x[np.array(idx.astype(np.int32))] for idx in batch_sampling_result]\n",
    "            #print('batch_sampling_result', type(batch_sampling_result), len(batch_sampling_result), batch_sampling_result[1].size)\n",
    "            #print('batch_sampling_x', type(batch_sampling_x),  len(batch_sampling_x), batch_sampling_x[1].size)\n",
    "\n",
    "\n",
    "            loss = 0.0\n",
    "            with tf.GradientTape() as tape:\n",
    "                batch_train_logits = model(batch_sampling_x)\n",
    "                loss = loss_object(batch_src_label, batch_train_logits)\n",
    "                grads = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "                optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "            # print(\"Epoch {:03d} Batch {:03d} Loss: {:.4f}\".format(e, batch, loss))\n",
    "        \n",
    "        train_accuracy = test(train_index)\n",
    "        val_accuracy = test(val_index)\n",
    "        test_accuracy = test(test_index)\n",
    "\n",
    "        train_loss_results.append(loss)\n",
    "        train_accuracy_results.append(train_accuracy)\n",
    "        train_val_results.append(val_accuracy)\n",
    "        train_test_results.append(test_accuracy)\n",
    "\n",
    "        print(\"Epoch {:03d} train accuracy: {} val accuracy: {} test accuracy:{}\".format(e, train_accuracy, val_accuracy, test_accuracy))\n",
    "        \n",
    "        # ISSUE: https://stackoverflow.com/questions/58947679/no-gradients-provided-for-any-variable-in-tensorflow2-0\n",
    "\n",
    "    # 训练过程可视化\n",
    "    fig, axes = plt.subplots(4, sharex=True, figsize=(12, 8))\n",
    "    fig.suptitle('Training Metrics')\n",
    "\n",
    "    axes[0].set_ylabel(\"Loss\", fontsize=14)\n",
    "    axes[0].plot(train_loss_results)\n",
    "\n",
    "    axes[1].set_ylabel(\"Accuracy\", fontsize=14)\n",
    "    axes[1].plot(train_accuracy_results)\n",
    "\n",
    "    axes[2].set_ylabel(\"Val Acc\", fontsize=14)\n",
    "    axes[2].plot(train_val_results)\n",
    "\n",
    "    axes[3].set_ylabel(\"Test Acc\", fontsize=14)\n",
    "    axes[3].plot(train_test_results)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b6a196b-17bd-4c69-815e-18534aae45e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(index):\n",
    "    test_sampling_result = multihop_sampling(index, NUM_NEIGHBORS_LIST, data.adjacency_dict)\n",
    "    test_x = [data.x[idx.astype(np.int32)] for idx in test_sampling_result]\n",
    "    test_logits = model(test_x)\n",
    "    test_label = data.y[index]\n",
    "\n",
    "    ll = tf.math.equal(tf.math.argmax(test_label, -1), tf.math.argmax(test_logits, -1))\n",
    "    accuarcy = tf.reduce_mean(tf.cast(ll, dtype=tf.float32))\n",
    "\n",
    "    return accuarcy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80af0cbc-f089-4c62-b6d0-ce85a4134f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 train accuracy: 0.8399999737739563 val accuracy: 0.47428572177886963 test accuracy:0.35733696818351746\n",
      "Epoch 001 train accuracy: 0.9666666388511658 val accuracy: 0.7142857313156128 test accuracy:0.6707427501678467\n",
      "Epoch 002 train accuracy: 1.0 val accuracy: 0.6885714530944824 test accuracy:0.6435688138008118\n",
      "Epoch 003 train accuracy: 1.0 val accuracy: 0.6828571557998657 test accuracy:0.6417572498321533\n",
      "Epoch 004 train accuracy: 1.0 val accuracy: 0.6857143044471741 test accuracy:0.645380437374115\n",
      "Epoch 005 train accuracy: 1.0 val accuracy: 0.7114285826683044 test accuracy:0.6698369383811951\n",
      "Epoch 006 train accuracy: 1.0 val accuracy: 0.7114285826683044 test accuracy:0.6648550629615784\n",
      "Epoch 007 train accuracy: 1.0 val accuracy: 0.7285714149475098 test accuracy:0.6603260636329651\n",
      "Epoch 008 train accuracy: 1.0 val accuracy: 0.699999988079071 test accuracy:0.6648550629615784\n",
      "Epoch 009 train accuracy: 1.0 val accuracy: 0.7142857313156128 test accuracy:0.6634963750839233\n",
      "Epoch 010 train accuracy: 1.0 val accuracy: 0.7171428799629211 test accuracy:0.6553441882133484\n",
      "Epoch 011 train accuracy: 1.0 val accuracy: 0.7142857313156128 test accuracy:0.6603260636329651\n",
      "Epoch 012 train accuracy: 1.0 val accuracy: 0.7085714340209961 test accuracy:0.6639492511749268\n",
      "Epoch 013 train accuracy: 1.0 val accuracy: 0.6971428394317627 test accuracy:0.6621376872062683\n",
      "Epoch 014 train accuracy: 1.0 val accuracy: 0.7200000286102295 test accuracy:0.664402186870575\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c25933-5f27-4128-a197-1bec22393156",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294df7d0-0c96-4689-a4c0-e0567bb8bbcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_tf26_cpu",
   "language": "python",
   "name": "py37_tf26_cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
