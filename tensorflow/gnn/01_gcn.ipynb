{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cad12a2-4299-482a-aea2-c984965fa760",
   "metadata": {},
   "source": [
    "## Graph Convolutional Network（GCN）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed9e54f-fabb-4159-8a03-86620a1587c3",
   "metadata": {},
   "source": [
    "### Reference\n",
    "1. [图卷积神经网络(GCN)理解与tensorflow2.0代码实现](https://github.com/zxxwin/tf2_gcn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ded197ce-9adf-4e93-9305-b0e8d74f366e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e3b2c60-9aa3-4a4f-8293-722e68613521",
   "metadata": {},
   "source": [
    "## 1. Cora数据集合\n",
    "Cora数据集由机器学习论文组成，是近年来图深度学习很喜欢使用的数据集。\n",
    "整个数据集有2708篇论文，所有样本点被分为7个类别，\n",
    "类别分别是\n",
    "+ 1）基于案例；\n",
    "+ 2）遗传算法；\n",
    "+ 3）神经网络；\n",
    "+ 4）概率方法；\n",
    "+ 5）强化学习；\n",
    "+ 6）规则学习；\n",
    "+ 7）理论。\n",
    "\n",
    "每篇论文都由一个1433维的词向量表示，所以，每个样本点具有1433个特征。词向量的每个元素都对应一个词，且该元素只有0或1两个取值。取0表示该元素对应的词不在论文中，取1表示在论文中。\n",
    "\n",
    "\n",
    "数据下载链接：https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz\n",
    "\n",
    "Reference: [cora数据集的读取和处理](https://blog.csdn.net/weixin_41650348/article/details/109406230)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21a8efde-bf45-4808-b7e3-2c5b92edac38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10078.38s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-07-08 19:01:39--  https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz\n",
      "Resolving linqs-data.soe.ucsc.edu (linqs-data.soe.ucsc.edu)... 128.114.47.74\n",
      "Connecting to linqs-data.soe.ucsc.edu (linqs-data.soe.ucsc.edu)|128.114.47.74|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 168052 (164K) [application/x-gzip]\n",
      "Saving to: ‘cora.tgz’\n",
      "\n",
      "100%[======================================>] 168,052      141KB/s   in 1.2s   \n",
      "\n",
      "2022-07-08 19:01:41 (141 KB/s) - ‘cora.tgz’ saved [168052/168052]\n",
      "\n",
      "cora/\n",
      "cora/README\n",
      "cora/cora.cites\n",
      "cora/cora.content\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10086.30s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/user_code/davidwwang/workspace/tensorflow/gnn\n"
     ]
    }
   ],
   "source": [
    "!cd ../../data;wget https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz;tar -zxvf cora.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bae7130-5243-4e26-b8b9-cf086392a3a8",
   "metadata": {},
   "source": [
    "###### 数据集查看\n",
    "下载的压缩包中有三个文件，分别是cora.cites，cora.content，README。\n",
    "\n",
    "+ README是对数据集的介绍；\n",
    "+ cora.content是所有论文的独自的信息；\n",
    "cora.content共有2708行，每一行代表一个样本点，即一篇论文。每一行由三部分组成，分别是论文的编号，如31336；论文的词向量，一个有1433位的二进制；论文的类别，如Neural_Networks。\n",
    "+ cora.cites是论文之间的引用记录。\n",
    "cora.cites共5429行， 每一行有两个论文编号，表示第一个编号的论文先写，第二个编号的论文引用第一个编号的论文。如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71f064b6-22e4-4a8f-ab89-aa63281b2ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2708, 1435)\n",
      "      0     1     2     3     4     5     6     7     8     9     ...  1425  \\\n",
      "0    31336     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "1  1061127     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "2  1106406     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "\n",
      "   1426  1427  1428  1429  1430  1431  1432  1433                    1434  \n",
      "0     0     1     0     0     0     0     0     0         Neural_Networks  \n",
      "1     1     0     0     0     0     0     0     0           Rule_Learning  \n",
      "2     0     0     0     0     0     0     0     0  Reinforcement_Learning  \n",
      "\n",
      "[3 rows x 1435 columns]\n",
      "(5429, 2)\n",
      "    0       1\n",
      "0  35    1033\n",
      "1  35  103482\n",
      "2  35  103515\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 读取.content 文件\n",
    "cora_content = pd.read_csv('../../data/cora/cora.content', sep='\\t', header=None)\n",
    "# 查看数据初始格式\n",
    "print(cora_content.shape)\n",
    "print(cora_content.head(3))\n",
    "\n",
    "# 读取 .cites文件\n",
    "cora_cites = pd.read_csv('../../data/cora/cora.cites', sep='\\t', header=None)\n",
    "print(cora_cites.shape)\n",
    "print(cora_cites.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d46715e-b1ec-4e21-9be2-340e03da9533",
   "metadata": {},
   "source": [
    "建立从paper_id到[0,2707]数字间的映射函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ec71dc8-a2b7-43e1-a100-a3a5ca2e05c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_idx=list(cora_content.index) #将索引制作成列表\n",
    "paper_id = list(cora_content.iloc[:,0])#将content第一列取出\n",
    "mp = dict(zip(paper_id, content_idx))#映射成{论文id:索引编号}的字典形式\n",
    "#查看某个论文id对应的索引编号\n",
    "mp[31336]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abaf5e5-b3ef-45f2-9b9d-dc96ce2041d8",
   "metadata": {},
   "source": [
    "提取feature matrix（2708，1433）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bdce040-b8ee-4ab4-9561-da9f40ef9ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2708, 1433)\n",
      "   1     2     3     4     5     6     7     8     9     10    ...  1424  \\\n",
      "0     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "1     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "2     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "\n",
      "   1425  1426  1427  1428  1429  1430  1431  1432  1433  \n",
      "0     0     0     1     0     0     0     0     0     0  \n",
      "1     0     1     0     0     0     0     0     0     0  \n",
      "2     0     0     0     0     0     0     0     0     0  \n",
      "\n",
      "[3 rows x 1433 columns]\n"
     ]
    }
   ],
   "source": [
    "#切片提取从第一列到倒数第二列（左闭右开）\n",
    "feature = cora_content.iloc[:,1:-1]\n",
    "print(feature.shape)\n",
    "print(feature.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd978d9-5bc1-4ad6-acea-cadb06c5b55e",
   "metadata": {},
   "source": [
    "标签进行one-hot编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4cbe47a0-8844-49ef-bf3d-838da6ad078f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case_Based</th>\n",
       "      <th>Genetic_Algorithms</th>\n",
       "      <th>Neural_Networks</th>\n",
       "      <th>Probabilistic_Methods</th>\n",
       "      <th>Reinforcement_Learning</th>\n",
       "      <th>Rule_Learning</th>\n",
       "      <th>Theory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Case_Based  Genetic_Algorithms  Neural_Networks  Probabilistic_Methods  \\\n",
       "0           0                   0                1                      0   \n",
       "1           0                   0                0                      0   \n",
       "2           0                   0                0                      0   \n",
       "\n",
       "   Reinforcement_Learning  Rule_Learning  Theory  \n",
       "0                       0              0       0  \n",
       "1                       0              1       0  \n",
       "2                       1              0       0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = cora_content.iloc[:, -1]\n",
    "label = pd.get_dummies(label) # 读热编码\n",
    "label.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad8aa2d-d00f-46fe-b9aa-762b7f0ba6ed",
   "metadata": {},
   "source": [
    "创建adjacent matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0cdd5b65-c70e-4f59-ba34-99c007688d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2708,)\n",
      "10556.0\n"
     ]
    }
   ],
   "source": [
    "mat_size = cora_content.shape[0] #第一维的大小2708就是邻接矩阵的规模\n",
    "adj_mat = np.zeros((mat_size, mat_size)) #创建0矩阵\n",
    "for i, j in zip(cora_cites[0], cora_cites[1]): #枚举形式（u，v）\n",
    "    x = mp[i]\n",
    "    y = mp[j]\n",
    "    adj_mat[x][y]=adj_mat[y][x]=1\n",
    "\n",
    "print(sum(adj_mat).shape)\n",
    "print(sum(sum(adj_mat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838b37a1-fc72-4c6d-95da-8710b3bba255",
   "metadata": {},
   "source": [
    "如果需要后续转为numpy或者其他形式（之前一直使用pandas的dataframe格式）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5dd608f5-9f8e-4c13-93f5-d9e8f1a610a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#转换为numpy格式的数据\n",
    "feature = np.array(feature)\n",
    "label = np.array(label)\n",
    "adj_mat =np.array(adj_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c726a913-34c2-49f4-8cd7-b8164032fd5b",
   "metadata": {},
   "source": [
    "## 2. GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "604bac91-f317-4297-a45b-0af065ebb83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "import scipy.sparse as sp\n",
    "from time import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97201b5a-cd14-43f5-bf63-e1d0f13e490f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "# 在深度学习中往往利用easydict建立一个全局的变量, 这里记录相关的参数配置\n",
    "from easydict import EasyDict\n",
    "config = {\n",
    "    'dataset':'cora',\n",
    "    'hidden1':16,\n",
    "    'epochs':200,\n",
    "    'early_stopping':20,\n",
    "    'weight_decay':5e-4,\n",
    "    'learning_rate': 0.01,\n",
    "    'dropout':0.,\n",
    "    'verbose':True,\n",
    "    'logging':False,\n",
    "    'gpu_id':None\n",
    "}\n",
    "print(type(config))\n",
    "FLAGS = EasyDict(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a79c9d4-e1eb-4150-8ca6-65ba42873fb3",
   "metadata": {},
   "source": [
    "读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47506030-11a3-40e2-a731-4c1bcb121f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def load_data_planetoid(dataset):\n",
    "    keys ={'x', 'y', 'tx','ty','allx', 'ally', 'graph'} # 文件名后缀\n",
    "    # print(type(keys))\n",
    "    objects = defaultdict() # 带默认值的dict\n",
    "    # print(objects)\n",
    "    for key in keys:\n",
    "        with open('data_split/ind.{}.{}'.format(dataset, key), 'rb') as f:\n",
    "            objects[key]=pickle.load(f, encoding='latin1')\n",
    "            # print(key, \"  \", type(objects[key]))\n",
    "    \n",
    "    check_x = objects['x'].toarray()\n",
    "    # print(\"allx \", objects['allx'].toarray().shape)\n",
    "    # print(\"tx \", objects['tx'].toarray().shape)\n",
    "    # print(\"x \", check_x.shape, '\\n', check_x[0:3,:])\n",
    "    # print(\"ally \", objects['ally'].shape)\n",
    "    # print(\"ty \", objects['ty'].shape)\n",
    "    # print(\"y \", objects['y'].shape, '\\n', objects['y'][0:3,:])\n",
    "    # print(\"graph \",len(objects['graph']), objects['graph'][0])\n",
    "    \n",
    "    test_index = [int(x) for x in open('data_split/ind.{}.test.index'.format(dataset))]\n",
    "    # print('test_index', type(test_index), len(test_index), test_index[:3])\n",
    "    test_index_sort = np.sort(test_index)\n",
    "    # print('test_index_sort', test_index_sort[0:5])\n",
    "    G = nx.from_dict_of_lists(objects['graph'])\n",
    "    \n",
    "    A_mat = nx.adjacency_matrix(G)\n",
    "    # print(type(A_mat), A_mat.toarray().shape, '\\n', A_mat.toarray()[0:3,:])\n",
    "    X_mat = sp.vstack((objects['allx'], objects['tx'])).tolil()\n",
    "    # print(type(X_mat), X_mat.toarray().shape, '\\n', X_mat.toarray()[0:3,:])\n",
    "    # 把特征矩阵还原，和对应的邻接矩阵对应起来，因为之前是打乱的，不对齐的话，特征就和对应的节点搞错了。\n",
    "    X_mat[test_index, :] = X_mat[test_index_sort,:]\n",
    "    z_vec = np.vstack((objects['ally'], objects['ty']))\n",
    "    # print(type(z_vec), z_vec.shape, '\\n', z_vec[0:3,:])\n",
    "\n",
    "    z_vec[test_index, :] = z_vec[test_index_sort, :]\n",
    "    z_vec = z_vec.argmax(1)\n",
    "    # print(type(z_vec),  '\\n', z_vec[0:3])\n",
    "\n",
    "    \n",
    "    train_idx = range(len(objects['y']))\n",
    "    val_idx = range(len(objects['y']), len(objects['y']) + 500)\n",
    "    test_idx = test_index_sort.tolist()\n",
    "    # print('train_idx ', len(train_idx), train_idx)\n",
    "    # print('val_idx ', len(val_idx), val_idx)\n",
    "    # print('test_idx ', len(test_idx), test_idx[:20])\n",
    "\n",
    "    return A_mat, X_mat, z_vec, train_idx, val_idx, test_idx\n",
    "\n",
    "cora_data = load_data_planetoid(FLAGS.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347b3491-5550-4d0b-a210-270db7e07a51",
   "metadata": {},
   "source": [
    "处理稀疏矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5637ba77-84cb-4b11-8c07-87ff75ceb94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 稀疏矩阵的 dropout\n",
    "def sparse_dropout(x, dropout_rate, noise_shape):\n",
    "    random_tensor = 1 - dropout_rate\n",
    "    random_tensor += tf.random.uniform(noise_shape)\n",
    "    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
    "    # 从稀疏矩阵中取出dropout_mask对应的元素\n",
    "    pre_out = tf.sparse.retain(x, dropout_mask)\n",
    "    \n",
    "    \n",
    "    return pre_out * (1. / (1 - dropout_rate))\n",
    "    \n",
    "# 稀疏矩阵转稀疏张量\n",
    "def sp_matrix_to_sp_tensor(M):\n",
    "    if not isinstance(M, sp.csr.csr_matrix):\n",
    "        M = M.tocsr()\n",
    "    # 获取非0元素坐标\n",
    "    row, col = M.nonzero()\n",
    "    # SparseTensor 参数： 二维坐标数组，数据，形状\n",
    "    X = tf.SparseTensor(np.mat([row, col]).T, M.data, M.shape)\n",
    "    X = tf.cast(X, tf.float32)\n",
    "    return X\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28e2294-b50a-4ffd-acec-40f3ea10f499",
   "metadata": {},
   "source": [
    "定义图卷积层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b692051-c9a0-4a68-bc02-46d5de4cb800",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import activations, regularizers, constraints, initializers\n",
    "\n",
    "class GCNConv(tf.keras.layers.Layer):\n",
    "    def __init__( self,\n",
    "                 units,\n",
    "                 activation=lambda x:x,\n",
    "                 use_bias = True,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros',\n",
    "                 **kwargs):\n",
    "        \n",
    "        super(GCNConv, self).__init__()\n",
    "        \n",
    "        self.units = units\n",
    "        self.activation = activations.get(activation)\n",
    "        self.use_bias = use_bias\n",
    "        self.kernel_initializer=initializers.get(kernel_initializer)\n",
    "        self.bias_initializer=initializers.get(bias_initializer)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        \"\"\"GCN has two inputs : [shape(An), shape(X)]\n",
    "        \"\"\"\n",
    "        fdim = input_shape[1][1] #feature dim\n",
    "        # 初始化权重矩阵\n",
    "        self.weight = self.add_weight(name='weight',\n",
    "                                     shape=(fdim, self.units),\n",
    "                                     initializer= self.kernel_initializer,\n",
    "                                     trainable=True)\n",
    "        if self.use_bias:\n",
    "            # 初始化偏置项目\n",
    "            self.bias = self.add_weight(name='bias',\n",
    "                                       shape=(self.units, ),\n",
    "                                       initializer = self.bias_initializer,\n",
    "                                       trainable=True)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        \"\"\" GCN has two inputs : [An, X]\n",
    "        \"\"\"\n",
    "        self.An = inputs[0]\n",
    "        self.X = inputs[1]\n",
    "        # 计算XW\n",
    "        if isinstance(self.X, tf.SparseTensor):\n",
    "            h = tf.sparse.sparse_dense_matmul(self.X, self.weight)\n",
    "        else:\n",
    "            h = tf.matmul(self.X, self.weight)\n",
    "        # 计算AxW\n",
    "        output = tf.sparse.sparse_dense_matmul(self.An, h)\n",
    "        \n",
    "        if self.use_bias:\n",
    "            output = tf.nn.bias_add(output, self.bias)\n",
    "        \n",
    "        if self.activation:\n",
    "            output = self.activation(output)\n",
    "            \n",
    "        return output\n",
    "        \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea6130d-b3bb-4e51-b3f8-eda47137c9c7",
   "metadata": {},
   "source": [
    "定义GCN模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5cf35dd-bee3-405b-bc3e-bb5bf880a601",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "class GCN():\n",
    "    def __init__(self, An, X, sizes, **kwargs):\n",
    "        self.with_relu = True\n",
    "        self.with_bias = True\n",
    "        \n",
    "        self.lr = FLAGS.learning_rate\n",
    "        self.dropout = FLAGS.dropout\n",
    "        self.verbose = FLAGS.verbose\n",
    "        \n",
    "        self.An = An\n",
    "        self.X = X\n",
    "        self.layer_sizes = sizes\n",
    "        self.shape = An.shape\n",
    "        \n",
    "        self.An_tf = sp_matrix_to_sp_tensor(self.An)\n",
    "        self.X_tf = sp_matrix_to_sp_tensor(self.X)\n",
    "        \n",
    "        self.layer1 = GCNConv(self.layer_sizes[0], activation='relu')\n",
    "        self.layer2 = GCNConv(self.layer_sizes[1])\n",
    "        self.opt = tf.optimizers.Adam(learning_rate = self.lr)\n",
    "        \n",
    "    def train(self, idx_train, labels_train, idx_val, label_val):\n",
    "        K = labels_train.max() + 1\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        # use adam to optimize\n",
    "        for it in range(FLAGS.epochs):\n",
    "            tic = time()\n",
    "            with tf.GradientTape() as tape:\n",
    "                _loss = self.loss_fn(idx_train, np.eye(K)[labels_train])\n",
    "            # optimize over weights\n",
    "            grad_list = tape.gradient(_loss, self.var_list)\n",
    "            grads_and_vars = zip(grad_list, self.var_list)\n",
    "            self.opt.apply_gradients(grads_and_vars)\n",
    "            \n",
    "            # evaluate on the training\n",
    "            train_loss, train_acc = self.evaluate(idx_train, labels_train, training=True)\n",
    "            train_losses.append(train_loss)\n",
    "            val_loss, val_acc = self.evaluate(idx_val, label_val, training=False)\n",
    "            val_losses.append(val_loss)\n",
    "            toc =time()\n",
    "            if self.verbose:\n",
    "                print(\"iter:{:03d}\".format(it),\n",
    "                      \"train_loss:{:.4f}\".format(train_loss),\n",
    "                      \"train_acc:{:.4f}\".format(train_acc),\n",
    "                      \"val_loss:{:.4f}\".format(val_loss),\n",
    "                      \"val_acc:{:.4f}\".format(val_acc),\n",
    "                      \"time:{:.4f}\".format(toc - tic))\n",
    "            \n",
    "        return train_losses\n",
    "    \n",
    "    def loss_fn(self, idx, labels, training=True):\n",
    "        if training:\n",
    "            # .nnz 是获得X中元素的个数\n",
    "            _X = sparse_dropout(self.X_tf, self.dropout, [self.X.nnz])\n",
    "        else:\n",
    "            _X = self.X_tf\n",
    "            \n",
    "        self.h1 = self.layer1([self.An_tf,_X])\n",
    "        if training:\n",
    "            _h1 = tf.nn.dropout(self.h1, self.dropout)\n",
    "        else:\n",
    "            _h1 = self.h1\n",
    "        \n",
    "        self.h2 = self.layer2([self.An_tf, _h1])\n",
    "        self.var_list = self.layer1.weights + self.layer2.weights\n",
    "        # calculate the loss base on idx and labels\n",
    "        _logits = tf.gather(self.h2, idx)\n",
    "        _loss_per_node = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=_logits)\n",
    "        \n",
    "        _loss = tf.reduce_mean(_loss_per_node)\n",
    "        \n",
    "        # 加上L2正则项\n",
    "        _loss +=FLAGS.weight_decay * sum(map(tf.nn.l2_loss, self.layer1.weights))\n",
    "        return _loss\n",
    "    \n",
    "    def evaluate(self, idx, true_labels, training):\n",
    "        K = true_labels.max() +1\n",
    "        _loss = self.loss_fn(idx, np.eye(K)[true_labels], training=training).numpy()\n",
    "        _pred_logits = tf.gather(self.h2, idx)\n",
    "        _pred_labels = tf.argmax(_pred_logits, axis=1).numpy()\n",
    "        _acc = accuracy_score(_pred_labels, true_labels)\n",
    "        \n",
    "        return _loss, _acc\n",
    "    \n",
    "            \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a4626e-ef6d-4e77-b8a8-bb32b3560ab0",
   "metadata": {},
   "source": [
    "计算标准化的邻接矩阵：根号D * A * 根号D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9f807b2-04e5-4afa-b55b-7919addcd4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算标准化的邻接矩阵：根号D * A * 根号D\n",
    "def preprocess_graph(adj):\n",
    "    # _A = A+I\n",
    "    _adj = adj + sp.eye(adj.shape[0])\n",
    "    # _dseq: 各个节点的度构成的列表\n",
    "    _dseq = _adj.sum(1).A1\n",
    "    # 构造开根号的度矩阵\n",
    "    _D_half = sp.diags(np.power(_dseq, -0.5))\n",
    "    # 计算标准化的邻接矩阵, @ 表示矩阵乘法\n",
    "    adj_normalized = _D_half @ _adj @ _D_half\n",
    "    return adj_normalized.tocsr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d201f86-541e-447e-bb29-578e9c8fccdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-14 09:52:21.024131: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib64:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/lib/native/:/jre/lib/amd64/:/usr/local/lib64:/usr/local/lib:/usr/local/tensorflow/lib:/usr/local/mysql/lib:/usr/local/jdk/jre/lib/amd64/server:/usr/local/mkl/lib:/usr/local/hadoop/lib/native:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/lib/native/:/jre/lib/amd64/:/usr/local/lib64:/usr/local/lib:/usr/local/tensorflow/lib:/usr/local/mysql/lib:/usr/local/jdk/jre/lib/amd64/server:/usr/local/mkl/lib:/usr/local/hadoop/lib/native:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/lib/native/:/jre/lib/amd64/:/usr/local/lib64:/usr/local/lib:/usr/local/tensorflow/lib:/usr/local/mysql/lib:/usr/local/jdk/jre/lib/amd64/server:/usr/local/mkl/lib:/usr/local/hadoop/lib/native:/usr/lib:/usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda-9.0/lib64:/usr/local/cuda-9.0/extras/CUPTI/lib64:/usr/local/cuda-8.0/lib64:/usr/local/cuda-8.0/extras/CUPTI/lib64:/usr/lib:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/lib:/usr/local/lib:/usr/local/app/.local/lib/python3.6/site-packages/numerous_pywrap:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/lib:/usr/local/lib:/usr/local/app/.local/lib/python3.6/site-packages/numerous_pywrap:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/lib:/usr/local/lib:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/lib:/usr/local/lib\n",
      "2022-07-14 09:52:21.024179: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-07-14 09:52:21.024208: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (9-223-245-158): /proc/driver/nvidia/version does not exist\n",
      "2022-07-14 09:52:21.024509: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:000 train_loss:1.8390 train_acc:0.7714 val_loss:1.8974 val_acc:0.4660 time:0.1475\n",
      "iter:001 train_loss:1.7056 train_acc:0.8429 val_loss:1.8314 val_acc:0.5240 time:0.0265\n",
      "iter:002 train_loss:1.5541 train_acc:0.8786 val_loss:1.7559 val_acc:0.6060 time:0.0264\n",
      "iter:003 train_loss:1.3979 train_acc:0.9071 val_loss:1.6733 val_acc:0.6600 time:0.0262\n",
      "iter:004 train_loss:1.2496 train_acc:0.9357 val_loss:1.5893 val_acc:0.6660 time:0.0261\n",
      "iter:005 train_loss:1.1115 train_acc:0.9357 val_loss:1.5041 val_acc:0.6900 time:0.0263\n",
      "iter:006 train_loss:0.9838 train_acc:0.9714 val_loss:1.4187 val_acc:0.7220 time:0.0268\n",
      "iter:007 train_loss:0.8649 train_acc:0.9714 val_loss:1.3338 val_acc:0.7480 time:0.0264\n",
      "iter:008 train_loss:0.7556 train_acc:0.9714 val_loss:1.2537 val_acc:0.7560 time:0.0266\n",
      "iter:009 train_loss:0.6580 train_acc:0.9714 val_loss:1.1832 val_acc:0.7700 time:0.0265\n",
      "iter:010 train_loss:0.5724 train_acc:0.9786 val_loss:1.1236 val_acc:0.7620 time:0.0262\n",
      "iter:011 train_loss:0.4983 train_acc:0.9786 val_loss:1.0742 val_acc:0.7740 time:0.0264\n",
      "iter:012 train_loss:0.4348 train_acc:0.9786 val_loss:1.0336 val_acc:0.7760 time:0.0264\n",
      "iter:013 train_loss:0.3812 train_acc:0.9857 val_loss:1.0003 val_acc:0.7640 time:0.0264\n",
      "iter:014 train_loss:0.3363 train_acc:0.9857 val_loss:0.9729 val_acc:0.7620 time:0.0260\n",
      "iter:015 train_loss:0.2991 train_acc:0.9857 val_loss:0.9501 val_acc:0.7580 time:0.0265\n",
      "iter:016 train_loss:0.2684 train_acc:0.9857 val_loss:0.9312 val_acc:0.7540 time:0.0272\n",
      "iter:017 train_loss:0.2434 train_acc:0.9929 val_loss:0.9156 val_acc:0.7560 time:0.0266\n",
      "iter:018 train_loss:0.2232 train_acc:1.0000 val_loss:0.9030 val_acc:0.7520 time:0.0264\n",
      "iter:019 train_loss:0.2070 train_acc:1.0000 val_loss:0.8934 val_acc:0.7540 time:0.0265\n",
      "iter:020 train_loss:0.1941 train_acc:1.0000 val_loss:0.8864 val_acc:0.7560 time:0.0267\n",
      "iter:021 train_loss:0.1837 train_acc:1.0000 val_loss:0.8818 val_acc:0.7600 time:0.0265\n",
      "iter:022 train_loss:0.1752 train_acc:1.0000 val_loss:0.8791 val_acc:0.7600 time:0.0264\n",
      "iter:023 train_loss:0.1683 train_acc:1.0000 val_loss:0.8781 val_acc:0.7580 time:0.0265\n",
      "iter:024 train_loss:0.1626 train_acc:1.0000 val_loss:0.8785 val_acc:0.7580 time:0.0274\n",
      "iter:025 train_loss:0.1579 train_acc:1.0000 val_loss:0.8800 val_acc:0.7580 time:0.0264\n",
      "iter:026 train_loss:0.1540 train_acc:1.0000 val_loss:0.8823 val_acc:0.7600 time:0.0263\n",
      "iter:027 train_loss:0.1507 train_acc:1.0000 val_loss:0.8850 val_acc:0.7520 time:0.0270\n",
      "iter:028 train_loss:0.1478 train_acc:1.0000 val_loss:0.8878 val_acc:0.7520 time:0.0261\n",
      "iter:029 train_loss:0.1453 train_acc:1.0000 val_loss:0.8904 val_acc:0.7500 time:0.0270\n",
      "iter:030 train_loss:0.1430 train_acc:1.0000 val_loss:0.8924 val_acc:0.7540 time:0.0263\n",
      "iter:031 train_loss:0.1407 train_acc:1.0000 val_loss:0.8939 val_acc:0.7540 time:0.0264\n",
      "iter:032 train_loss:0.1386 train_acc:1.0000 val_loss:0.8948 val_acc:0.7560 time:0.0270\n",
      "iter:033 train_loss:0.1365 train_acc:1.0000 val_loss:0.8952 val_acc:0.7560 time:0.0261\n",
      "iter:034 train_loss:0.1345 train_acc:1.0000 val_loss:0.8951 val_acc:0.7540 time:0.0264\n",
      "iter:035 train_loss:0.1325 train_acc:1.0000 val_loss:0.8947 val_acc:0.7540 time:0.0264\n",
      "iter:036 train_loss:0.1305 train_acc:1.0000 val_loss:0.8941 val_acc:0.7540 time:0.0267\n",
      "iter:037 train_loss:0.1285 train_acc:1.0000 val_loss:0.8934 val_acc:0.7540 time:0.0262\n",
      "iter:038 train_loss:0.1264 train_acc:1.0000 val_loss:0.8925 val_acc:0.7520 time:0.0262\n",
      "iter:039 train_loss:0.1244 train_acc:1.0000 val_loss:0.8916 val_acc:0.7560 time:0.0265\n",
      "iter:040 train_loss:0.1224 train_acc:1.0000 val_loss:0.8905 val_acc:0.7560 time:0.0271\n",
      "iter:041 train_loss:0.1203 train_acc:1.0000 val_loss:0.8894 val_acc:0.7560 time:0.0260\n",
      "iter:042 train_loss:0.1183 train_acc:1.0000 val_loss:0.8881 val_acc:0.7540 time:0.0261\n",
      "iter:043 train_loss:0.1162 train_acc:1.0000 val_loss:0.8867 val_acc:0.7560 time:0.0264\n",
      "iter:044 train_loss:0.1142 train_acc:1.0000 val_loss:0.8851 val_acc:0.7580 time:0.0272\n",
      "iter:045 train_loss:0.1122 train_acc:1.0000 val_loss:0.8833 val_acc:0.7580 time:0.0267\n",
      "iter:046 train_loss:0.1103 train_acc:1.0000 val_loss:0.8812 val_acc:0.7580 time:0.0268\n",
      "iter:047 train_loss:0.1083 train_acc:1.0000 val_loss:0.8789 val_acc:0.7580 time:0.0289\n",
      "iter:048 train_loss:0.1065 train_acc:1.0000 val_loss:0.8762 val_acc:0.7580 time:0.0286\n",
      "iter:049 train_loss:0.1046 train_acc:1.0000 val_loss:0.8734 val_acc:0.7580 time:0.0285\n",
      "iter:050 train_loss:0.1028 train_acc:1.0000 val_loss:0.8702 val_acc:0.7580 time:0.0282\n",
      "iter:051 train_loss:0.1011 train_acc:1.0000 val_loss:0.8668 val_acc:0.7580 time:0.0281\n",
      "iter:052 train_loss:0.0994 train_acc:1.0000 val_loss:0.8632 val_acc:0.7580 time:0.0276\n",
      "iter:053 train_loss:0.0977 train_acc:1.0000 val_loss:0.8594 val_acc:0.7580 time:0.0287\n",
      "iter:054 train_loss:0.0961 train_acc:1.0000 val_loss:0.8556 val_acc:0.7580 time:0.0286\n",
      "iter:055 train_loss:0.0945 train_acc:1.0000 val_loss:0.8518 val_acc:0.7600 time:0.0285\n",
      "iter:056 train_loss:0.0930 train_acc:1.0000 val_loss:0.8480 val_acc:0.7600 time:0.0283\n",
      "iter:057 train_loss:0.0916 train_acc:1.0000 val_loss:0.8443 val_acc:0.7640 time:0.0282\n",
      "iter:058 train_loss:0.0902 train_acc:1.0000 val_loss:0.8407 val_acc:0.7640 time:0.0275\n",
      "iter:059 train_loss:0.0888 train_acc:1.0000 val_loss:0.8372 val_acc:0.7640 time:0.0262\n",
      "iter:060 train_loss:0.0875 train_acc:1.0000 val_loss:0.8339 val_acc:0.7640 time:0.0264\n",
      "iter:061 train_loss:0.0863 train_acc:1.0000 val_loss:0.8308 val_acc:0.7680 time:0.0259\n",
      "iter:062 train_loss:0.0851 train_acc:1.0000 val_loss:0.8278 val_acc:0.7680 time:0.0270\n",
      "iter:063 train_loss:0.0839 train_acc:1.0000 val_loss:0.8249 val_acc:0.7680 time:0.0271\n",
      "iter:064 train_loss:0.0828 train_acc:1.0000 val_loss:0.8222 val_acc:0.7680 time:0.0278\n",
      "iter:065 train_loss:0.0817 train_acc:1.0000 val_loss:0.8196 val_acc:0.7680 time:0.0273\n",
      "iter:066 train_loss:0.0806 train_acc:1.0000 val_loss:0.8171 val_acc:0.7680 time:0.0275\n",
      "iter:067 train_loss:0.0796 train_acc:1.0000 val_loss:0.8147 val_acc:0.7680 time:0.0270\n",
      "iter:068 train_loss:0.0786 train_acc:1.0000 val_loss:0.8125 val_acc:0.7680 time:0.0270\n",
      "iter:069 train_loss:0.0777 train_acc:1.0000 val_loss:0.8103 val_acc:0.7680 time:0.0267\n",
      "iter:070 train_loss:0.0768 train_acc:1.0000 val_loss:0.8083 val_acc:0.7700 time:0.0275\n",
      "iter:071 train_loss:0.0759 train_acc:1.0000 val_loss:0.8065 val_acc:0.7700 time:0.0269\n",
      "iter:072 train_loss:0.0751 train_acc:1.0000 val_loss:0.8047 val_acc:0.7700 time:0.0275\n",
      "iter:073 train_loss:0.0742 train_acc:1.0000 val_loss:0.8031 val_acc:0.7720 time:0.0276\n",
      "iter:074 train_loss:0.0734 train_acc:1.0000 val_loss:0.8016 val_acc:0.7720 time:0.0278\n",
      "iter:075 train_loss:0.0727 train_acc:1.0000 val_loss:0.8001 val_acc:0.7720 time:0.0279\n",
      "iter:076 train_loss:0.0719 train_acc:1.0000 val_loss:0.7988 val_acc:0.7720 time:0.0273\n",
      "iter:077 train_loss:0.0712 train_acc:1.0000 val_loss:0.7975 val_acc:0.7740 time:0.0278\n",
      "iter:078 train_loss:0.0705 train_acc:1.0000 val_loss:0.7965 val_acc:0.7740 time:0.0274\n",
      "iter:079 train_loss:0.0698 train_acc:1.0000 val_loss:0.7955 val_acc:0.7740 time:0.0275\n",
      "iter:080 train_loss:0.0691 train_acc:1.0000 val_loss:0.7948 val_acc:0.7740 time:0.0277\n",
      "iter:081 train_loss:0.0684 train_acc:1.0000 val_loss:0.7942 val_acc:0.7740 time:0.0268\n",
      "iter:082 train_loss:0.0678 train_acc:1.0000 val_loss:0.7937 val_acc:0.7740 time:0.0265\n",
      "iter:083 train_loss:0.0672 train_acc:1.0000 val_loss:0.7932 val_acc:0.7740 time:0.0264\n",
      "iter:084 train_loss:0.0666 train_acc:1.0000 val_loss:0.7927 val_acc:0.7740 time:0.0261\n",
      "iter:085 train_loss:0.0660 train_acc:1.0000 val_loss:0.7923 val_acc:0.7740 time:0.0272\n",
      "iter:086 train_loss:0.0654 train_acc:1.0000 val_loss:0.7919 val_acc:0.7740 time:0.0278\n",
      "iter:087 train_loss:0.0648 train_acc:1.0000 val_loss:0.7914 val_acc:0.7740 time:0.0277\n",
      "iter:088 train_loss:0.0643 train_acc:1.0000 val_loss:0.7909 val_acc:0.7760 time:0.0279\n",
      "iter:089 train_loss:0.0637 train_acc:1.0000 val_loss:0.7904 val_acc:0.7760 time:0.0273\n",
      "iter:090 train_loss:0.0632 train_acc:1.0000 val_loss:0.7898 val_acc:0.7740 time:0.0270\n",
      "iter:091 train_loss:0.0627 train_acc:1.0000 val_loss:0.7892 val_acc:0.7740 time:0.0267\n",
      "iter:092 train_loss:0.0622 train_acc:1.0000 val_loss:0.7886 val_acc:0.7740 time:0.0273\n",
      "iter:093 train_loss:0.0617 train_acc:1.0000 val_loss:0.7880 val_acc:0.7740 time:0.0277\n",
      "iter:094 train_loss:0.0612 train_acc:1.0000 val_loss:0.7875 val_acc:0.7740 time:0.0274\n",
      "iter:095 train_loss:0.0607 train_acc:1.0000 val_loss:0.7870 val_acc:0.7740 time:0.0277\n",
      "iter:096 train_loss:0.0603 train_acc:1.0000 val_loss:0.7866 val_acc:0.7740 time:0.0282\n",
      "iter:097 train_loss:0.0598 train_acc:1.0000 val_loss:0.7863 val_acc:0.7740 time:0.0271\n",
      "iter:098 train_loss:0.0594 train_acc:1.0000 val_loss:0.7861 val_acc:0.7740 time:0.0253\n",
      "iter:099 train_loss:0.0589 train_acc:1.0000 val_loss:0.7858 val_acc:0.7740 time:0.0259\n",
      "iter:100 train_loss:0.0585 train_acc:1.0000 val_loss:0.7856 val_acc:0.7740 time:0.0255\n",
      "iter:101 train_loss:0.0581 train_acc:1.0000 val_loss:0.7853 val_acc:0.7740 time:0.0254\n",
      "iter:102 train_loss:0.0576 train_acc:1.0000 val_loss:0.7850 val_acc:0.7740 time:0.0252\n",
      "iter:103 train_loss:0.0572 train_acc:1.0000 val_loss:0.7848 val_acc:0.7740 time:0.0252\n",
      "iter:104 train_loss:0.0568 train_acc:1.0000 val_loss:0.7845 val_acc:0.7740 time:0.0263\n",
      "iter:105 train_loss:0.0564 train_acc:1.0000 val_loss:0.7843 val_acc:0.7740 time:0.0251\n",
      "iter:106 train_loss:0.0561 train_acc:1.0000 val_loss:0.7841 val_acc:0.7740 time:0.0249\n",
      "iter:107 train_loss:0.0557 train_acc:1.0000 val_loss:0.7839 val_acc:0.7740 time:0.0283\n",
      "iter:108 train_loss:0.0553 train_acc:1.0000 val_loss:0.7837 val_acc:0.7740 time:0.0262\n",
      "iter:109 train_loss:0.0549 train_acc:1.0000 val_loss:0.7835 val_acc:0.7740 time:0.0238\n",
      "iter:110 train_loss:0.0546 train_acc:1.0000 val_loss:0.7832 val_acc:0.7740 time:0.0259\n",
      "iter:111 train_loss:0.0542 train_acc:1.0000 val_loss:0.7830 val_acc:0.7740 time:0.0275\n",
      "iter:112 train_loss:0.0539 train_acc:1.0000 val_loss:0.7828 val_acc:0.7740 time:0.0279\n",
      "iter:113 train_loss:0.0535 train_acc:1.0000 val_loss:0.7826 val_acc:0.7740 time:0.0272\n",
      "iter:114 train_loss:0.0532 train_acc:1.0000 val_loss:0.7825 val_acc:0.7740 time:0.0257\n",
      "iter:115 train_loss:0.0529 train_acc:1.0000 val_loss:0.7824 val_acc:0.7740 time:0.0241\n",
      "iter:116 train_loss:0.0525 train_acc:1.0000 val_loss:0.7823 val_acc:0.7760 time:0.0244\n",
      "iter:117 train_loss:0.0522 train_acc:1.0000 val_loss:0.7821 val_acc:0.7760 time:0.0246\n",
      "iter:118 train_loss:0.0519 train_acc:1.0000 val_loss:0.7819 val_acc:0.7760 time:0.0245\n",
      "iter:119 train_loss:0.0516 train_acc:1.0000 val_loss:0.7818 val_acc:0.7760 time:0.0247\n",
      "iter:120 train_loss:0.0513 train_acc:1.0000 val_loss:0.7817 val_acc:0.7760 time:0.0250\n",
      "iter:121 train_loss:0.0510 train_acc:1.0000 val_loss:0.7816 val_acc:0.7760 time:0.0262\n",
      "iter:122 train_loss:0.0507 train_acc:1.0000 val_loss:0.7816 val_acc:0.7760 time:0.0241\n",
      "iter:123 train_loss:0.0504 train_acc:1.0000 val_loss:0.7815 val_acc:0.7760 time:0.0276\n",
      "iter:124 train_loss:0.0501 train_acc:1.0000 val_loss:0.7814 val_acc:0.7760 time:0.0265\n",
      "iter:125 train_loss:0.0498 train_acc:1.0000 val_loss:0.7814 val_acc:0.7760 time:0.0284\n",
      "iter:126 train_loss:0.0495 train_acc:1.0000 val_loss:0.7813 val_acc:0.7760 time:0.0264\n",
      "iter:127 train_loss:0.0492 train_acc:1.0000 val_loss:0.7812 val_acc:0.7780 time:0.0263\n",
      "iter:128 train_loss:0.0489 train_acc:1.0000 val_loss:0.7811 val_acc:0.7780 time:0.0266\n",
      "iter:129 train_loss:0.0487 train_acc:1.0000 val_loss:0.7811 val_acc:0.7780 time:0.0269\n",
      "iter:130 train_loss:0.0484 train_acc:1.0000 val_loss:0.7810 val_acc:0.7780 time:0.0269\n",
      "iter:131 train_loss:0.0481 train_acc:1.0000 val_loss:0.7808 val_acc:0.7780 time:0.0266\n",
      "iter:132 train_loss:0.0479 train_acc:1.0000 val_loss:0.7807 val_acc:0.7780 time:0.0257\n",
      "iter:133 train_loss:0.0476 train_acc:1.0000 val_loss:0.7806 val_acc:0.7780 time:0.0240\n",
      "iter:134 train_loss:0.0474 train_acc:1.0000 val_loss:0.7805 val_acc:0.7780 time:0.0240\n",
      "iter:135 train_loss:0.0471 train_acc:1.0000 val_loss:0.7804 val_acc:0.7780 time:0.0246\n",
      "iter:136 train_loss:0.0469 train_acc:1.0000 val_loss:0.7803 val_acc:0.7780 time:0.0245\n",
      "iter:137 train_loss:0.0466 train_acc:1.0000 val_loss:0.7802 val_acc:0.7780 time:0.0259\n",
      "iter:138 train_loss:0.0464 train_acc:1.0000 val_loss:0.7802 val_acc:0.7780 time:0.0258\n",
      "iter:139 train_loss:0.0462 train_acc:1.0000 val_loss:0.7802 val_acc:0.7780 time:0.0254\n",
      "iter:140 train_loss:0.0459 train_acc:1.0000 val_loss:0.7802 val_acc:0.7780 time:0.0239\n",
      "iter:141 train_loss:0.0457 train_acc:1.0000 val_loss:0.7801 val_acc:0.7780 time:0.0238\n",
      "iter:142 train_loss:0.0455 train_acc:1.0000 val_loss:0.7800 val_acc:0.7780 time:0.0244\n",
      "iter:143 train_loss:0.0452 train_acc:1.0000 val_loss:0.7801 val_acc:0.7780 time:0.0240\n",
      "iter:144 train_loss:0.0450 train_acc:1.0000 val_loss:0.7803 val_acc:0.7780 time:0.0252\n",
      "iter:145 train_loss:0.0448 train_acc:1.0000 val_loss:0.7804 val_acc:0.7780 time:0.0258\n",
      "iter:146 train_loss:0.0446 train_acc:1.0000 val_loss:0.7804 val_acc:0.7780 time:0.0261\n",
      "iter:147 train_loss:0.0444 train_acc:1.0000 val_loss:0.7803 val_acc:0.7780 time:0.0268\n",
      "iter:148 train_loss:0.0441 train_acc:1.0000 val_loss:0.7801 val_acc:0.7780 time:0.0282\n",
      "iter:149 train_loss:0.0439 train_acc:1.0000 val_loss:0.7800 val_acc:0.7800 time:0.0292\n",
      "iter:150 train_loss:0.0437 train_acc:1.0000 val_loss:0.7799 val_acc:0.7800 time:0.0277\n",
      "iter:151 train_loss:0.0435 train_acc:1.0000 val_loss:0.7799 val_acc:0.7780 time:0.0278\n",
      "iter:152 train_loss:0.0433 train_acc:1.0000 val_loss:0.7799 val_acc:0.7780 time:0.0278\n",
      "iter:153 train_loss:0.0431 train_acc:1.0000 val_loss:0.7799 val_acc:0.7780 time:0.0276\n",
      "iter:154 train_loss:0.0429 train_acc:1.0000 val_loss:0.7798 val_acc:0.7780 time:0.0288\n",
      "iter:155 train_loss:0.0427 train_acc:1.0000 val_loss:0.7797 val_acc:0.7780 time:0.0280\n",
      "iter:156 train_loss:0.0425 train_acc:1.0000 val_loss:0.7798 val_acc:0.7780 time:0.0276\n",
      "iter:157 train_loss:0.0423 train_acc:1.0000 val_loss:0.7799 val_acc:0.7780 time:0.0277\n",
      "iter:158 train_loss:0.0421 train_acc:1.0000 val_loss:0.7801 val_acc:0.7780 time:0.0281\n",
      "iter:159 train_loss:0.0420 train_acc:1.0000 val_loss:0.7801 val_acc:0.7780 time:0.0278\n",
      "iter:160 train_loss:0.0418 train_acc:1.0000 val_loss:0.7800 val_acc:0.7780 time:0.0280\n",
      "iter:161 train_loss:0.0416 train_acc:1.0000 val_loss:0.7798 val_acc:0.7760 time:0.0286\n",
      "iter:162 train_loss:0.0414 train_acc:1.0000 val_loss:0.7797 val_acc:0.7760 time:0.0292\n",
      "iter:163 train_loss:0.0412 train_acc:1.0000 val_loss:0.7797 val_acc:0.7760 time:0.0286\n",
      "iter:164 train_loss:0.0411 train_acc:1.0000 val_loss:0.7798 val_acc:0.7760 time:0.0284\n",
      "iter:165 train_loss:0.0409 train_acc:1.0000 val_loss:0.7799 val_acc:0.7760 time:0.0285\n",
      "iter:166 train_loss:0.0407 train_acc:1.0000 val_loss:0.7799 val_acc:0.7760 time:0.0279\n",
      "iter:167 train_loss:0.0405 train_acc:1.0000 val_loss:0.7799 val_acc:0.7760 time:0.0276\n",
      "iter:168 train_loss:0.0404 train_acc:1.0000 val_loss:0.7798 val_acc:0.7760 time:0.0283\n",
      "iter:169 train_loss:0.0402 train_acc:1.0000 val_loss:0.7799 val_acc:0.7760 time:0.0282\n",
      "iter:170 train_loss:0.0400 train_acc:1.0000 val_loss:0.7799 val_acc:0.7760 time:0.0277\n",
      "iter:171 train_loss:0.0399 train_acc:1.0000 val_loss:0.7800 val_acc:0.7760 time:0.0276\n",
      "iter:172 train_loss:0.0397 train_acc:1.0000 val_loss:0.7800 val_acc:0.7760 time:0.0271\n",
      "iter:173 train_loss:0.0395 train_acc:1.0000 val_loss:0.7800 val_acc:0.7760 time:0.0283\n",
      "iter:174 train_loss:0.0394 train_acc:1.0000 val_loss:0.7798 val_acc:0.7760 time:0.0280\n",
      "iter:175 train_loss:0.0392 train_acc:1.0000 val_loss:0.7798 val_acc:0.7760 time:0.0281\n",
      "iter:176 train_loss:0.0391 train_acc:1.0000 val_loss:0.7799 val_acc:0.7760 time:0.0273\n",
      "iter:177 train_loss:0.0389 train_acc:1.0000 val_loss:0.7800 val_acc:0.7740 time:0.0275\n",
      "iter:178 train_loss:0.0387 train_acc:1.0000 val_loss:0.7802 val_acc:0.7740 time:0.0289\n",
      "iter:179 train_loss:0.0386 train_acc:1.0000 val_loss:0.7804 val_acc:0.7740 time:0.0264\n",
      "iter:180 train_loss:0.0384 train_acc:1.0000 val_loss:0.7805 val_acc:0.7740 time:0.0265\n",
      "iter:181 train_loss:0.0383 train_acc:1.0000 val_loss:0.7806 val_acc:0.7740 time:0.0264\n",
      "iter:182 train_loss:0.0381 train_acc:1.0000 val_loss:0.7806 val_acc:0.7740 time:0.0282\n",
      "iter:183 train_loss:0.0380 train_acc:1.0000 val_loss:0.7807 val_acc:0.7740 time:0.0276\n",
      "iter:184 train_loss:0.0379 train_acc:1.0000 val_loss:0.7807 val_acc:0.7740 time:0.0266\n",
      "iter:185 train_loss:0.0377 train_acc:1.0000 val_loss:0.7807 val_acc:0.7740 time:0.0265\n",
      "iter:186 train_loss:0.0376 train_acc:1.0000 val_loss:0.7806 val_acc:0.7740 time:0.0271\n",
      "iter:187 train_loss:0.0374 train_acc:1.0000 val_loss:0.7806 val_acc:0.7740 time:0.0267\n",
      "iter:188 train_loss:0.0373 train_acc:1.0000 val_loss:0.7806 val_acc:0.7740 time:0.0265\n",
      "iter:189 train_loss:0.0372 train_acc:1.0000 val_loss:0.7807 val_acc:0.7740 time:0.0268\n",
      "iter:190 train_loss:0.0370 train_acc:1.0000 val_loss:0.7808 val_acc:0.7740 time:0.0265\n",
      "iter:191 train_loss:0.0369 train_acc:1.0000 val_loss:0.7810 val_acc:0.7740 time:0.0263\n",
      "iter:192 train_loss:0.0367 train_acc:1.0000 val_loss:0.7810 val_acc:0.7740 time:0.0257\n",
      "iter:193 train_loss:0.0366 train_acc:1.0000 val_loss:0.7811 val_acc:0.7740 time:0.0264\n",
      "iter:194 train_loss:0.0365 train_acc:1.0000 val_loss:0.7811 val_acc:0.7740 time:0.0267\n",
      "iter:195 train_loss:0.0363 train_acc:1.0000 val_loss:0.7811 val_acc:0.7740 time:0.0263\n",
      "iter:196 train_loss:0.0362 train_acc:1.0000 val_loss:0.7812 val_acc:0.7740 time:0.0262\n",
      "iter:197 train_loss:0.0361 train_acc:1.0000 val_loss:0.7813 val_acc:0.7740 time:0.0272\n",
      "iter:198 train_loss:0.0360 train_acc:1.0000 val_loss:0.7814 val_acc:0.7740 time:0.0267\n",
      "iter:199 train_loss:0.0358 train_acc:1.0000 val_loss:0.7815 val_acc:0.7740 time:0.0261\n",
      "Dataset cora Test loss 0.6501 test acc 0.8090\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 读取数据\n",
    "    # A_mat：邻接矩阵\n",
    "    # X_mat：特征矩阵\n",
    "    # z_vec：label\n",
    "    # train_idx,val_idx,test_idx: 要使用的节点序号\n",
    "    A_mat, X_mat, z_vec, train_idx, val_idx, test_idx = load_data_planetoid(FLAGS.dataset)\n",
    "    # 邻居矩阵标准化\n",
    "    An_mat = preprocess_graph(A_mat)\n",
    "    # 节点的类别个数\n",
    "    K = z_vec.max() + 1\n",
    "    # 构造GCN模型\n",
    "    gcn = GCN(An_mat, X_mat, [FLAGS.hidden1, K])\n",
    "    # 训练\n",
    "    gcn.train(train_idx, z_vec[train_idx], val_idx, z_vec[val_idx])\n",
    "    # 测试\n",
    "    test_res = gcn.evaluate(test_idx, z_vec[test_idx], training=False)\n",
    "    print(\"Dataset {}\".format(FLAGS.dataset),\n",
    "          \"Test loss {:.4f}\".format(test_res[0]),\n",
    "          \"test acc {:.4f}\".format(test_res[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90aff27-01d8-4e24-a543-5870edfe1f3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b3aa35-3791-4a4c-9898-9b1651d190ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e06b15c-ab2d-471b-b494-ba3109436607",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_tf26_cpu",
   "language": "python",
   "name": "py37_tf26_cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
