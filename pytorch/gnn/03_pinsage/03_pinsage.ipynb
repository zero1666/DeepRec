{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eba130d-ede1-493b-925e-23dcc1573005",
   "metadata": {},
   "source": [
    "# Pinsage Pytorch 实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1061b994-1582-4318-9af0-624f8da2577e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#全局配置\n",
    "input_dir = \"../../../data/ml-1m/\"\n",
    "output_dir = \"../../../data/ml-1m/tmp/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.getcwd()\n",
    "\n",
    "# 模型运行参数\n",
    "num_epochs = 3\n",
    "num_workers = 2\n",
    "\n",
    "hidden_dims = 64\n",
    "batch_size = 32\n",
    "num_random_walks=10\n",
    "random_walk_length=2\n",
    "random_walk_restart_prob=0.5\n",
    "num_neighbors=3\n",
    "num_layers=2\n",
    "lr=3e-5\n",
    "k=10\n",
    "batches_per_epoch=100\n",
    "device='cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656f8811-6b9f-48cc-852b-cef41c90aeee",
   "metadata": {},
   "source": [
    "## 生成图样本文件\n",
    "### Utils\n",
    "##### 图处理工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e1d492-f22f-4ad6-9217-a5f3e81e41e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"build.py Graph builder from pandas dataframes\"\"\"\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "import dgl\n",
    "from pandas.api.types import is_categorical, is_categorical_dtype, is_numeric_dtype\n",
    "\n",
    "\n",
    "def _series_to_tensor(serises):\n",
    "    if is_categorical(series):\n",
    "        return torch.LongTensor(series.cat.codes.values.astype(\"int64\"))\n",
    "    else:\n",
    "        return torch.FloatTensor(series.values)\n",
    "\n",
    "\n",
    "class PandasGraphBUilder(object):\n",
    "    \"\"\"Creates a heterogeneous graph from multiple pandas dataframes.\n",
    "    Examples\n",
    "    --------\n",
    "    Let's say we have the following three pandas dataframes:\n",
    "    User table ``users``:\n",
    "    ===========  ===========  =======\n",
    "    ``user_id``  ``country``  ``age``\n",
    "    ===========  ===========  =======\n",
    "    XYZZY        U.S.         25\n",
    "    FOO          China        24\n",
    "    BAR          China        23\n",
    "    ===========  ===========  =======\n",
    "    Game table ``games``:\n",
    "    ===========  =========  ==============  ==================\n",
    "    ``game_id``  ``title``  ``is_sandbox``  ``is_multiplayer``\n",
    "    ===========  =========  ==============  ==================\n",
    "    1            Minecraft  True            True\n",
    "    2            Tetris 99  False           True\n",
    "    ===========  =========  ==============  ==================\n",
    "    Play relationship table ``plays``:\n",
    "    ===========  ===========  =========\n",
    "    ``user_id``  ``game_id``  ``hours``\n",
    "    ===========  ===========  =========\n",
    "    XYZZY        1            24\n",
    "    FOO          1            20\n",
    "    FOO          2            16\n",
    "    BAR          2            28\n",
    "    ===========  ===========  =========\n",
    "    One could then create a bidirectional bipartite graph as follows:\n",
    "    >>> builder = PandasGraphBuilder()\n",
    "    >>> builder.add_entities(users, 'user_id', 'user')\n",
    "    >>> builder.add_entities(games, 'game_id', 'game')\n",
    "    >>> builder.add_binary_relations(plays, 'user_id', 'game_id', 'plays')\n",
    "    >>> builder.add_binary_relations(plays, 'game_id', 'user_id', 'played-by')\n",
    "    >>> g = builder.build()\n",
    "    >>> g.num_nodes('user')\n",
    "    3\n",
    "    >>> g.num_edges('plays')\n",
    "    4\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.entity_tables = {}\n",
    "        self.relation_tables = {}\n",
    "\n",
    "        self.entity_pk_to_name = {}  # mapping from primary key name to entity name\n",
    "        self.entity_pk = {}  # mapping from entity name to primary key\n",
    "        self.entity_key_map = {}  # mapping from entity names to primary key values\n",
    "        self.num_nodes_per_type = {}\n",
    "        self.edges_per_relation = {}\n",
    "        self.relation_src_key = {}\n",
    "        self.relation_dst_key = {}\n",
    "        self.relation_name_to_etype = {}\n",
    "\n",
    "    def add_entities(self, entity_table, primary_key, name):\n",
    "        entities = entity_table[primary_key].astype(\"category\")\n",
    "        if not (entities.value_counts() == 1).all():\n",
    "            raise ValueError(\"Different entity with the same primary key detected.\")\n",
    "        # preserve the category order in the original entity table\n",
    "        entities = entities.cat.reorder_categories(entity_table[primary_key].values)\n",
    "\n",
    "        self.entity_pk_to_name[primary_key] = name\n",
    "        self.entity_pk[name] = primary_key\n",
    "        self.num_nodes_per_type[name] = entity_table.shape[0]\n",
    "        self.entity_key_map[name] = entities\n",
    "        self.entity_tables[name] = entity_table\n",
    "\n",
    "    def add_binary_relations(self, relation_table, source_key, destination_key, name):\n",
    "        src = relation_table[source_key].astype(\"category\")\n",
    "        src = src.cat.set_categories(\n",
    "            self.entity_key_map[self.entity_pk_to_name[source_key]].cat.categories\n",
    "        )\n",
    "        dst = relation_table[destination_key].astype(\"category\")\n",
    "        dst = dst.cat.set_categories(\n",
    "            self.entity_key_map[self.entity_pk_to_name[destination_key]].cat.categories\n",
    "        )\n",
    "\n",
    "        if src.isnull().any():\n",
    "            raise ValueError(\n",
    "                \"Some source entities in relation %s do not exist in entity %s.\"\n",
    "                % (name, source_key)\n",
    "            )\n",
    "        if dst.isnull().any():\n",
    "            raise ValueError(\n",
    "                \"Some destination entities in relation %s do not exist in entity %s.\"\n",
    "                % (name, destination_key)\n",
    "            )\n",
    "\n",
    "        srctype = self.entity_pk_to_name[source_key]\n",
    "        dsttype = self.entity_pk_to_name[destination_key]\n",
    "        etype = (srctype, name, dsttype)\n",
    "        self.relation_name_to_etype[name] = etype\n",
    "        self.edges_per_relation[etype] = (\n",
    "            src.cat.codes.values.astype(\"int64\"),\n",
    "            dst.cat.codes.values.astype(\"int64\"),\n",
    "        )\n",
    "        self.relation_tables[name] = relation_table\n",
    "        self.relation_src_key[name] = source_key\n",
    "        self.relation_dst_key[name] = destination_key\n",
    "\n",
    "    def build(self):\n",
    "        # Create heterograph\n",
    "        graph = dgl.heterograph(self.edges_per_relation, self.num_nodes_per_type)\n",
    "        return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fd3228-5aa1-4a70-a05c-644c96521446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import dgl\n",
    "import numpy as np\n",
    "import scipy.sparse as ssp\n",
    "import torch\n",
    "import tqdm\n",
    "\n",
    "\n",
    "# This is the train-test split method most of the recommender system papers running on MovieLens\n",
    "# takes.  It essentially follows the intuition of \"training on the past and predict the future\".\n",
    "# One can also change the threshold to make validation and test set take larger proportions.\n",
    "def train_test_split_by_time(df, timestamp, user):\n",
    "    df[\"train_mask\"] = np.ones((len(df),), dtype=np.bool)\n",
    "    df[\"val_mask\"] = np.zeros((len(df),), dtype=np.bool)\n",
    "    df[\"test_mask\"] = np.zeros((len(df),), dtype=np.bool)\n",
    "    df = dd.from_pandas(df, npartitions=10)\n",
    "\n",
    "    def train_test_split(df):\n",
    "        df = df.sort_values([timestamp])\n",
    "        if df.shape[0] > 1:\n",
    "            df.iloc[-1, -3] = False\n",
    "            df.iloc[-1, -1] = True\n",
    "        if df.shape[0] > 2:\n",
    "            df.iloc[-2, -3] = False\n",
    "            df.iloc[-2, -2] = True\n",
    "        return df\n",
    "    print(type(df), df.shape,df.shape[0])\n",
    "    df = (\n",
    "        df.groupby(user, group_keys=False)\n",
    "        .apply(train_test_split)\n",
    "        .compute(scheduler=\"processes\")\n",
    "        .sort_index()\n",
    "    )\n",
    "    print(df[df[user] == df[user].unique()[0]].sort_values(timestamp))\n",
    "    return (\n",
    "        df[\"train_mask\"].to_numpy().nonzero()[0],\n",
    "        df[\"val_mask\"].to_numpy().nonzero()[0],\n",
    "        df[\"test_mask\"].to_numpy().nonzero()[0],\n",
    "    )\n",
    "\n",
    "\n",
    "def build_train_graph(g, train_indices, utype, itype, etype, etype_rev):\n",
    "    train_g = g.edge_subgraph(\n",
    "        {etype: train_indices, etype_rev: train_indices}, relabel_nodes=False\n",
    "    )\n",
    "\n",
    "    # copy features\n",
    "    for ntype in g.ntypes:\n",
    "        for col, data in g.nodes[ntype].data.items():\n",
    "            train_g.nodes[ntype].data[col] = data\n",
    "    for etype in g.etypes:\n",
    "        for col, data in g.edges[etype].data.items():\n",
    "            train_g.edges[etype].data[col] = data[train_g.edges[etype].data[dgl.EID]]\n",
    "\n",
    "    return train_g\n",
    "\n",
    "\n",
    "def build_val_test_matrix(g, val_indices, test_indices, utype, itype, etype):\n",
    "    n_users = g.num_nodes(utype)\n",
    "    n_items = g.num_nodes(itype)\n",
    "    val_src, val_dst = g.find_edges(val_indices, etype=etype)\n",
    "    test_src, test_dst = g.find_edges(test_indices, etype=etype)\n",
    "    val_src = val_src.numpy()\n",
    "    val_dst = val_dst.numpy()\n",
    "    test_src = test_src.numpy()\n",
    "    test_dst = test_dst.numpy()\n",
    "    val_matrix = ssp.coo_matrix(\n",
    "        (np.ones_like(val_src), (val_src, val_dst)), (n_users, n_items)\n",
    "    )\n",
    "    test_matrix = ssp.coo_matrix(\n",
    "        (np.ones_like(test_src), (test_src, test_dst)), (n_users, n_items)\n",
    "    )\n",
    "\n",
    "    return val_matrix, test_matrix\n",
    "\n",
    "\n",
    "def linear_normalize(values):\n",
    "    return (values - values.min(0, keepdims=True)) / (\n",
    "        values.max(0, keepdims=True) - values.min(0, keepdims=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec85324-a1c2-4597-8512-8422d3dcef7b",
   "metadata": {},
   "source": [
    "### Prepare datasets（MovieLens 1M）\n",
    "+ [dgl pinsage](https://github.com/dmlc/dgl/tree/master/examples/pytorch/pinsage)\n",
    "\n",
    "MovieLens 1M数据集含有来自6000名用户对4000部电影的100万条评分数据。分为三个表：评分，用户信息，电影信息。这些数据都是dat文件格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff40477-a69b-4446-919d-ae86eea3fd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "import dgl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as ssp\n",
    "import torch\n",
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17aac03b-c120-4795-8e16-275d3ccdb778",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build heterogenerous graph\n",
    "# Load data\n",
    "users = []\n",
    "with open(os.path.join(input_dir, \"users.dat\"), encoding=\"latin1\") as f:\n",
    "    for l in f:\n",
    "        id_, gender, age, occupation, zip_ = l.strip().split(\"::\")\n",
    "        users.append(\n",
    "            {\n",
    "                \"user_id\": int(id_),\n",
    "                \"gender\": gender,\n",
    "                \"age\": age,\n",
    "                \"occupation\": occupation,\n",
    "                \"zip\": zip_,\n",
    "            }\n",
    "        )\n",
    "users = pd.DataFrame(users).astype(\"category\")\n",
    "\n",
    "movies = []\n",
    "with open(os.path.join(input_dir, \"movies.dat\"), encoding=\"latin1\") as f:\n",
    "    for l in f:\n",
    "        id_, title, genres = l.strip().split(\"::\")\n",
    "        genres_set = set(genres.split(\"|\"))\n",
    "\n",
    "        # extract year\n",
    "        assert re.match(r\".*\\([0-9]{4}\\)$\", title)\n",
    "        year = title[-5:-1]\n",
    "        title = title[:-6].strip()\n",
    "\n",
    "        data = {\"movie_id\": int(id_), \"title\": title, \"year\": year}\n",
    "        for g in genres_set:\n",
    "            data[g] = True\n",
    "        movies.append(data)\n",
    "movies = pd.DataFrame(movies).astype({\"year\": \"category\"})\n",
    "\n",
    "ratings = []\n",
    "with open(os.path.join(input_dir, \"ratings.dat\"), encoding=\"latin1\") as f:\n",
    "    for l in f:\n",
    "        user_id, movie_id, rating, timestamp = [int(_) for _ in l.split(\"::\")]\n",
    "        ratings.append(\n",
    "            {\n",
    "                \"user_id\": user_id,\n",
    "                \"movie_id\": movie_id,\n",
    "                \"rating\": rating,\n",
    "                \"timestamp\": timestamp,\n",
    "            }\n",
    "        )\n",
    "ratings = pd.DataFrame(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc9b6dd-f406-4214-b4b3-4bb280c6f167",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"users:\", users.shape)\n",
    "print(users.head())\n",
    "print(\"movies\", movies.shape)\n",
    "print(movies.head())\n",
    "print(\"rating\", ratings.shape)\n",
    "print(ratings.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b416b3-c03d-49af-80d1-c00caa4474cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the users and items that never appear in the rating table.\n",
    "distinct_users_in_rating = ratings[\"user_id\"].unique()\n",
    "distinct_movies_in_rating = ratings[\"movie_id\"].unique()\n",
    "users = users[users[\"user_id\"].isin(distinct_users_in_rating)]\n",
    "movies = movies[movies[\"movie_id\"].isin(distinct_movies_in_rating)]\n",
    "\n",
    "print(\"users:\", users.shape)\n",
    "print(\"movies\", movies.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28990d53-c932-4b88-af9f-18faa7b04193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the movie features into genres (a vector), year (a category), title (a string)\n",
    "genera_columns = movies.columns.drop([\"movie_id\", \"title\", \"year\"])\n",
    "movies[genera_columns] = movies[genera_columns].fillna(False).astype(\"bool\")\n",
    "movies_categorical = movies.drop(\"title\", axis=1)\n",
    "\n",
    "print(movies_categorical.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598717b4-d08a-4916-ac81-85119b37a814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build graph\n",
    "graph_builder = PandasGraphBUilder()\n",
    "graph_builder.add_entities(users, \"user_id\", \"user\")\n",
    "graph_builder.add_entities(movies_categorical, \"movie_id\", \"movie\")\n",
    "graph_builder.add_binary_relations(ratings, \"user_id\", \"movie_id\", \"watched\")\n",
    "graph_builder.add_binary_relations(ratings, \"movie_id\", \"user_id\", \"watched-by\")\n",
    "g = graph_builder.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a753a61e-7d4c-4299-80e8-7c80e9b6db73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign features.\n",
    "# Note that variable-sized features such as texts or images are handled elsewhere.\n",
    "g.nodes[\"user\"].data[\"gender\"] = torch.LongTensor(users[\"gender\"].cat.codes.values)\n",
    "g.nodes[\"user\"].data[\"age\"] = torch.LongTensor(users[\"age\"].cat.codes.values)\n",
    "g.nodes[\"user\"].data[\"occupation\"] = torch.LongTensor(\n",
    "    users[\"occupation\"].cat.codes.values\n",
    ")\n",
    "g.nodes[\"user\"].data[\"zip\"] = torch.LongTensor(users[\"zip\"].cat.codes.values)\n",
    "\n",
    "g.nodes[\"movie\"].data[\"year\"] = torch.LongTensor(movies[\"year\"].cat.codes.values)\n",
    "g.nodes[\"movie\"].data[\"genre\"] = torch.FloatTensor(movies[genera_columns].values)\n",
    "\n",
    "g.edges[\"watched\"].data[\"rating\"] = torch.LongTensor(ratings[\"rating\"].values)\n",
    "g.edges[\"watched\"].data[\"timestamp\"] = torch.LongTensor(ratings[\"timestamp\"].values)\n",
    "g.edges[\"watched-by\"].data[\"rating\"] = torch.LongTensor(ratings[\"rating\"].values)\n",
    "g.edges[\"watched-by\"].data[\"timestamp\"] = torch.LongTensor(ratings[\"timestamp\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8448707-e5a4-46df-96bc-b386e654c5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((g.nodes[\"user\"].data[\"gender\"].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79edc36-a897-4b39-9543-281a35ee0edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-validation-test split\n",
    "# This is a little bit tricky as we want to select the last interaction for test, and the\n",
    "# second-to-last interaction for validation.\n",
    "train_indices, val_indices, test_indices = train_test_split_by_time(\n",
    "    ratings, \"timestamp\", \"user_id\"\n",
    ")\n",
    "# Build the graph with training interactions only.\n",
    "train_g = build_train_graph(g, train_indices, \"user\", \"movie\", \"watched\", \"watched-by\")\n",
    "assert train_g.out_degrees(etype=\"watched\").min() > 0\n",
    "\n",
    "val_matrix, test_matrix = build_val_test_matrix(\n",
    "    g, val_indices, test_indices, \"user\", \"movie\", \"watched\"\n",
    ")\n",
    "## Build title set\n",
    "movie_textual_dataset = {\"title\": movies[\"title\"].values}\n",
    "# The model should build their own vocabulary and process the texts.  Here is one example\n",
    "# of using torchtext to pad and numericalize a batch of strings.\n",
    "#     field = torchtext.data.Field(include_lengths=True, lower=True, batch_first=True)\n",
    "#     examples = [torchtext.data.Example.fromlist([t], [('title', title_field)]) for t in texts]\n",
    "#     titleset = torchtext.data.Dataset(examples, [('title', title_field)])\n",
    "#     field.build_vocab(titleset.title, vectors='fasttext.simple.300d')\n",
    "#     token_ids, lengths = field.process([examples[0].title, examples[1].title])\n",
    "## Dump the graph and the datasets\n",
    "dgl.save_graphs(os.path.join(output_dir, \"train_g.bin\"), train_g)\n",
    "dataset = {\n",
    "    \"val-matrix\": val_matrix,\n",
    "    \"test-matrix\": test_matrix,\n",
    "    \"item-texts\": movie_textual_dataset,\n",
    "    \"item-images\": None,\n",
    "    \"user-type\": \"user\",\n",
    "    \"item-type\": \"movie\",\n",
    "    \"user-to-item-type\": \"watched\",\n",
    "    \"item-to-user-type\": \"watched-by\",\n",
    "    \"timestamp-edge-column\": \"timestamp\",\n",
    "}\n",
    "with open(os.path.join(output_dir, \"data.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(dataset, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5d39a6-203f-483c-840f-628698ad82aa",
   "metadata": {},
   "source": [
    "## 准备模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c3d8c95-6fba-40cb-9072-1120fcfdc688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchtext\n",
    "import dgl\n",
    "import os\n",
    "import tqdm\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16dac97a-ff86-40eb-85fc-08e55bedebee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info_path = os.path.join(output_dir, 'data.pkl')\n",
    "with open(data_info_path, 'rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "train_g_path = os.path.join(output_dir, 'train_g.bin')\n",
    "g_list, _ = dgl.load_graphs(train_g_path)\n",
    "dataset['train-graph'] = g_list[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b266d17e-96ae-46f2-bbd2-7f8bbf4b50fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'> dict_keys(['val-matrix', 'test-matrix', 'item-texts', 'item-images', 'user-type', 'item-type', 'user-to-item-type', 'item-to-user-type', 'timestamp-edge-column', 'train-graph'])\n",
      "<class 'list'> 1 \n",
      " Graph(num_nodes={'movie': 3706, 'user': 6040},\n",
      "      num_edges={('movie', 'watched-by', 'user'): 988129, ('user', 'watched', 'movie'): 988129},\n",
      "      metagraph=[('movie', 'user', 'watched-by'), ('user', 'movie', 'watched')])\n"
     ]
    }
   ],
   "source": [
    "print(type(dataset), dataset.keys())\n",
    "print(type(g_list),len(g_list),'\\n', dataset['train-graph'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1943f54e-d72b-4ff0-ad4b-29eb6349d5a5",
   "metadata": {},
   "source": [
    "### Sampler\n",
    "数据是如何提供给模型的。 PinSAGE在实现模型时，，而其主要改进都是发生在\"供应训练数据\"这一环节，包括：生成mini-batch、负样本采样、为mini-batch生成卷积各层需要的计算子图、将计算子图中的相应边删除等工作。\n",
    "###### heads, tails, neg_tails\n",
    "+ heads：每次从原图所有item节点中采样出batch_size个item节点\n",
    "+ tails：由这heads个item节点出发，经过两跳的随机游走(item→user→item)再落脚的item节点。因为这些节点与heads节点被共同的user消费过，认为有内在相似性，作为heads的正样本\n",
    "+ neg_tails：每次从原图所有item节点中，再采出batch_size个item节点。这部分随机采样的节点作为heads的负样本。\n",
    "+ 属于某一个batch，但是由哪一层卷积无关。\n",
    "\n",
    "###### pos_graph和neg_graph\n",
    "+ 只由heads, tails, neg_tails这些节点构成一个局部图。因为是部分节点，因此不遵循原图中item节点的编号空间，需要重新编号\n",
    "+ 这个局部图中由heads→tails的边构成了pos_graph。这些边上的分数是参与pairwise loss中的positive score。\n",
    "+ 这个局部图中由heads→neg_tails的边构成了neg_graph。这些边上的分数是参与pairwise loss中的negative score。\n",
    "+ 注意，pos_graph与neg_graph中的节点是相同的，所以seeds = pos_graph.ndata[dgl.NID]能够代表这个局部图中的所有节点，即seeds = heads + tails + neg_tails。\n",
    "+ 注意，pos_graph与neg_graph中边的数目是相同的，一条正边只与一条负边对应。这是一个缺陷，因为实践中，往往需要一条正边与多条负边比较。\n",
    "+ 属于某一个batch，但是由哪一层卷积无关。\n",
    "\n",
    "##### frontier\n",
    "+ 属于某个batch中的某一层卷积\n",
    "+ 在原图g上（不是在pos_graph或neg_graph上），以seeds item node为起点，通过random walk进行重要性采样，得到seeds最重要的邻居。包含了所有item节点(因此item编号与g原图中相同)，但是只在seed item节点和其最重要的邻居之间，才有边所构成的图，称为frontier。\n",
    "+ 由于neighbor sampling是从顶部倒着向底部进行，所以第N层的seeds就是第N+1层的input nodes，最顶部一层的seeds就是heads+tails+neg_tails构成的所有item节点\n",
    "+ 如果只是预测，以上过程就已经足够了。但是在训练中，为了防止信息泄漏，还需要将frontier中有可能存在的“正边”(heads⇒tails)和“负边”(heads⇒neg_tails)统统删除。\n",
    "\n",
    "###### block\n",
    "+ 属于某个batch中的某一层卷积\n",
    "+ 为了信息传递之用的一种特殊二部图结构\n",
    "+ frontier中还是包含了原图中所有item节点，而由frontier生成的block只包含了seeds节点、指向seeds节点的邻居、它们中间的边。\n",
    "+ 因为block去除了无关节点，因此信息传递起来更高效，但是需要给节点重要编号\n",
    "+ 相比于传统图中的src/dst节点，block中更常用的概念是input/output nodes，而且所有output nodes都排列在input nodes中的头部。\n",
    "+ blocks[0].srcnodes，代表为计算出目标节点（这里的heads + tails + neg_tails），必须参与计算的全部输入节点。\n",
    "+ blocks[-1].dstnodes，代表我们感兴趣的目标节点（这里的heads + tails + neg_tails）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b61a2f0a-f919-4dfc-b57a-57eed35c8010",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dgl\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "from torchtext.data.functional import numericalize_tokens_from_iterator\n",
    "\n",
    "def padding(array, yy, val):\n",
    "    \"\"\"\n",
    "    :param array: torch tensor array\n",
    "    :param yy: desired width\n",
    "    :param val: padded value\n",
    "    :return: padded array\n",
    "    \"\"\"\n",
    "    w = array.shape[0]\n",
    "    b = 0\n",
    "    bb = yy - b - w\n",
    "\n",
    "    return torch.nn.functional.pad(array, pad=(b, bb), mode='constant', value=val)\n",
    "\n",
    "def compact_and_copy(frontier, seeds):\n",
    "    block = dgl.to_block(frontier, seeds)\n",
    "    for col, data in frontier.edata.items():\n",
    "        if col == dgl.EID:\n",
    "            continue\n",
    "        block.edata[col] = data[block.edata[dgl.EID]]\n",
    "    return block\n",
    "\n",
    "class ItemToItemBatchSampler(IterableDataset):\n",
    "    def __init__(self, g, user_type, item_type, batch_size):\n",
    "        self.g = g\n",
    "        self.user_type = user_type\n",
    "        self.item_type = item_type\n",
    "        self.user_to_item_etype = list(g.metagraph()[user_type][item_type])[0]\n",
    "        self.item_to_user_etype = list(g.metagraph()[item_type][user_type])[0]\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            # 随机采样做heads\n",
    "            heads = torch.randint(0, self.g.num_nodes(self.item_type), (self.batch_size,))\n",
    "            # 二跳游走，得到与heads被同一个用户消费过的其他item，做正样本\n",
    "            # 还是有很多不足，\n",
    "            # 1. 这种游走肯定会使正样本集中于少数热门item\n",
    "            # 2. 如果item只被一个用户消费过，二跳游走岂不是又回到起始item，这种corner case还是要处理的 \n",
    "            tails = dgl.sampling.random_walk(\n",
    "                self.g,\n",
    "                heads,\n",
    "                metapath=[self.item_to_user_etype, self.user_to_item_etype])[0][:, 2]\n",
    "            # 随机采样做负样本\n",
    "            # 没有hard negative也是可以接受的\n",
    "            neg_tails = torch.randint(0, self.g.num_nodes(self.item_type), (self.batch_size,))\n",
    "\n",
    "            mask = (tails != -1)\n",
    "            yield heads[mask], tails[mask], neg_tails[mask]\n",
    "\n",
    "class NeighborSampler(object):\n",
    "    def __init__(self, g, user_type, item_type, random_walk_length, random_walk_restart_prob,\n",
    "                 num_random_walks, num_neighbors, num_layers):\n",
    "        self.g = g\n",
    "        self.user_type = user_type\n",
    "        self.item_type = item_type\n",
    "        self.user_to_item_etype = list(g.metagraph()[user_type][item_type])[0]\n",
    "        self.item_to_user_etype = list(g.metagraph()[item_type][user_type])[0]\n",
    "        # 每层都有一个采样器，根据随机游走来决定某节点邻居的重要性\n",
    "        # 可以认为经过多次游走，落脚于某邻居节点的次数越多，则这个邻居越重要，就更应该优先作为邻居\n",
    "        self.samplers = [\n",
    "            dgl.sampling.PinSAGESampler(g, item_type, user_type, random_walk_length,\n",
    "                random_walk_restart_prob, num_random_walks, num_neighbors)\n",
    "            for _ in range(num_layers)]\n",
    "\n",
    "    def sample_blocks(self, seeds, heads=None, tails=None, neg_tails=None):\n",
    "        \"\"\"\n",
    "        由seeds（实际上就是batch中的heads+tails+neg_tails）回溯生成各层卷积需要的block\n",
    "        \"\"\"\n",
    "        blocks = []\n",
    "        for sampler in self.samplers:\n",
    "            frontier = sampler(seeds) # 通过随机游走进行重要性采样，生成中间状态frontier\n",
    "            if heads is not None:\n",
    "                # 如果是在训练，需要将heads->tails和head->neg_tails这些待预测的边都去掉，防止信息泄漏\n",
    "                eids = frontier.edge_ids(torch.cat([heads, heads]), torch.cat([tails, neg_tails]), return_uv=True)[2]\n",
    "                if len(eids) > 0:\n",
    "                    old_frontier = frontier\n",
    "                    frontier = dgl.remove_edges(old_frontier, eids)\n",
    "                    \n",
    "            # 只保留seeds这些节点，将frontier压缩成block\n",
    "            # 并设置block的input/output nodes       \n",
    "            block = compact_and_copy(frontier, seeds)\n",
    "            # 本层的输入节点就是下一层的seeds\n",
    "            seeds = block.srcdata[dgl.NID]\n",
    "            blocks.insert(0, block)\n",
    "        return blocks\n",
    "\n",
    "    def sample_from_item_pairs(self, heads, tails, neg_tails):\n",
    "        # Create a graph with positive connections only and another graph with negative\n",
    "        # connections only.\n",
    "        pos_graph = dgl.graph(\n",
    "            (heads, tails),\n",
    "            num_nodes=self.g.num_nodes(self.item_type))\n",
    "        neg_graph = dgl.graph(\n",
    "            (heads, neg_tails),\n",
    "            num_nodes=self.g.num_nodes(self.item_type))\n",
    "        # 去除heads, tails, neg_tails以外的节点\n",
    "        # 将大图压缩成小图，避免不必要的信息传递，提升计算效率\n",
    "        pos_graph, neg_graph = dgl.compact_graphs([pos_graph, neg_graph])\n",
    "        seeds = pos_graph.ndata[dgl.NID]\n",
    "\n",
    "        blocks = self.sample_blocks(seeds, heads, tails, neg_tails)\n",
    "        return pos_graph, neg_graph, blocks\n",
    "\n",
    "def assign_simple_node_features(ndata, g, ntype, assign_id=False):\n",
    "    \"\"\"\n",
    "    Copies data to the given block from the corresponding nodes in the original graph.\n",
    "    \"\"\"\n",
    "    for col in g.nodes[ntype].data.keys():\n",
    "        if not assign_id and col == dgl.NID:\n",
    "            continue\n",
    "        induced_nodes = ndata[dgl.NID]\n",
    "        ndata[col] = g.nodes[ntype].data[col][induced_nodes]\n",
    "\n",
    "def assign_textual_node_features(ndata, textset, ntype):\n",
    "    \"\"\"\n",
    "    Assigns numericalized tokens from a torchtext dataset to given block.\n",
    "    The numericalized tokens would be stored in the block as node features\n",
    "    with the same name as ``field_name``.\n",
    "    The length would be stored as another node feature with name\n",
    "    ``field_name + '__len'``.\n",
    "    block : DGLHeteroGraph\n",
    "        First element of the compacted blocks, with \"dgl.NID\" as the\n",
    "        corresponding node ID in the original graph, hence the index to the\n",
    "        text dataset.\n",
    "        The numericalized tokens (and lengths if available) would be stored\n",
    "        onto the blocks as new node features.\n",
    "    textset : torchtext.data.Dataset\n",
    "        A torchtext dataset whose number of examples is the same as that\n",
    "        of nodes in the original graph.\n",
    "    \"\"\"\n",
    "    node_ids = ndata[dgl.NID].numpy()\n",
    "\n",
    "    for field_name, field in textset.items():\n",
    "        textlist, vocab, pad_var, batch_first = field\n",
    "        \n",
    "        examples = [textlist[i] for i in node_ids]\n",
    "        ids_iter = numericalize_tokens_from_iterator(vocab, examples)\n",
    "        \n",
    "        maxsize = max([len(textlist[i]) for i in node_ids])\n",
    "        ids = next(ids_iter)\n",
    "        x = torch.asarray([num for num in ids])\n",
    "        lengths = torch.tensor([len(x)])\n",
    "        tokens = padding(x, maxsize, pad_var)\n",
    "\n",
    "        for ids in ids_iter:\n",
    "            x = torch.asarray([num for num in ids])\n",
    "            l = torch.tensor([len(x)])\n",
    "            y = padding(x, maxsize, pad_var)\n",
    "            tokens = torch.vstack((tokens,y))\n",
    "            lengths = torch.cat((lengths, l))\n",
    "       \n",
    "        if not batch_first:\n",
    "            tokens = tokens.t()\n",
    "\n",
    "        ndata[field_name] = tokens\n",
    "        ndata[field_name + '__len'] = lengths\n",
    "\n",
    "def assign_features_to_blocks(blocks, g, textset, ntype):\n",
    "    # For the first block (which is closest to the input), copy the features from\n",
    "    # the original graph as well as the texts.\n",
    "    assign_simple_node_features(blocks[0].srcdata, g, ntype)\n",
    "    assign_textual_node_features(blocks[0].srcdata, textset, ntype)\n",
    "    assign_simple_node_features(blocks[-1].dstdata, g, ntype)\n",
    "    assign_textual_node_features(blocks[-1].dstdata, textset, ntype)\n",
    "\n",
    "class PinSAGECollator(object):\n",
    "    def __init__(self, sampler, g, ntype, textset):\n",
    "        self.sampler = sampler\n",
    "        self.ntype = ntype\n",
    "        self.g = g\n",
    "        self.textset = textset\n",
    "\n",
    "    def collate_train(self, batches):\n",
    "        heads, tails, neg_tails = batches[0]\n",
    "        # Construct multilayer neighborhood via PinSAGE...\n",
    "        pos_graph, neg_graph, blocks = self.sampler.sample_from_item_pairs(heads, tails, neg_tails)\n",
    "        assign_features_to_blocks(blocks, self.g, self.textset, self.ntype)\n",
    "\n",
    "        return pos_graph, neg_graph, blocks\n",
    "\n",
    "    def collate_test(self, samples):\n",
    "        batch = torch.LongTensor(samples)\n",
    "        blocks = self.sampler.sample_blocks(batch)\n",
    "        assign_features_to_blocks(blocks, self.g, self.textset, self.ntype)\n",
    "        return blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8fab9a-4790-4126-b64a-abab2f92f3a5",
   "metadata": {},
   "source": [
    "### 定义模型结构"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12febd1-fa6d-47a1-a76e-f9733ed3fa85",
   "metadata": {},
   "source": [
    "##### Layer定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcfc7ad1-5bcd-4bc2-a73f-2e5d9a288f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl\n",
    "import dgl.nn.pytorch as dglnn\n",
    "import dgl.function as fn\n",
    "\n",
    "def disable_grad(module):\n",
    "    for param in module.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "def _init_input_modules(g, ntype, textset, hidden_dims):\n",
    "    # We initialize the linear projections of each input feature ``x`` as\n",
    "    # follows:\n",
    "    # * If ``x`` is a scalar integral feature, we assume that ``x`` is a categorical\n",
    "    #   feature, and assume the range of ``x`` is 0..max(x).\n",
    "    # * If ``x`` is a float one-dimensional feature, we assume that ``x`` is a\n",
    "    #   numeric vector.\n",
    "    # * If ``x`` is a field of a textset, we process it as bag of words.\n",
    "    module_dict = nn.ModuleDict()\n",
    "\n",
    "    for column, data in g.nodes[ntype].data.items():\n",
    "        if column == dgl.NID:\n",
    "            continue\n",
    "        if data.dtype == torch.float32:\n",
    "            assert data.ndim == 2\n",
    "            m = nn.Linear(data.shape[1], hidden_dims)\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            module_dict[column] = m\n",
    "        elif data.dtype == torch.int64:\n",
    "            assert data.ndim == 1\n",
    "            m = nn.Embedding(\n",
    "                data.max() + 2, hidden_dims, padding_idx=-1)\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            module_dict[column] = m\n",
    "\n",
    "    if textset is not None:\n",
    "        for column, field in textset.items():\n",
    "            textlist, vocab, pad_var, batch_first = field            \n",
    "            module_dict[column] = BagOfWords(vocab, hidden_dims)\n",
    "\n",
    "    return module_dict\n",
    "\n",
    "class BagOfWords(nn.Module):\n",
    "    def __init__(self, vocab, hidden_dims):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb = nn.Embedding(\n",
    "            len(vocab.get_itos()), hidden_dims,\n",
    "            padding_idx=vocab.get_stoi()['<pad>'])\n",
    "        nn.init.xavier_uniform_(self.emb.weight)\n",
    "\n",
    "    def forward(self, x, length):\n",
    "        return self.emb(x).sum(1) / length.unsqueeze(1).float()\n",
    "\n",
    "class LinearProjector(nn.Module):\n",
    "    \"\"\"\n",
    "    Projects each input feature of the graph linearly and sums them up\n",
    "    \"\"\"\n",
    "    def __init__(self, full_graph, ntype, textset, hidden_dims):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ntype = ntype\n",
    "        # 遍历graph中node的每种特征，根据特征类型，定义相应的特征转化器\n",
    "        # 比如，如果特征类型是float矩阵，就定义一个nn.Linear线性变化为指定维度\n",
    "        # 比如，如果特征类型是int，就定义Embedding矩阵，将id型特征转化为向量\n",
    "        self.inputs = _init_input_modules(full_graph, ntype, textset, hidden_dims)\n",
    "\n",
    "    def forward(self, ndata):\n",
    "        projections = []\n",
    "        for feature, data in ndata.items():\n",
    "            # NID是计算子图中节点、边在原图中的编号，没必要用做特征\n",
    "            if feature == dgl.NID or feature.endswith('__len'):\n",
    "                # This is an additional feature indicating the length of the ``feature``\n",
    "                # column; we shouldn't process this.\n",
    "                continue\n",
    "\n",
    "            module = self.inputs[feature]  # 根据特征名取出相应的特征转化器\n",
    "            if isinstance(module, BagOfWords):\n",
    "                # Textual feature; find the length and pass it to the textual module.\n",
    "                length = ndata[feature + '__len']\n",
    "                result = module(data, length)\n",
    "            else:\n",
    "                result = module(data) # 将原始特征值转化为hidden_dims长的向量\n",
    "            projections.append(result)\n",
    "        # 将每个特征都映射后的hidden_dims长的向量，element-wise相加\n",
    "        # 返回一个[#nodes, hidden_dims]的Tensor\n",
    "        return torch.stack(projections, 1).sum(1)\n",
    "\n",
    "class WeightedSAGEConv(nn.Module):\n",
    "    \"\"\"\n",
    "    单层卷积模块，聚合时考虑了边上的权重\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dims, hidden_dims, output_dims, act=F.relu):\n",
    "        super().__init__()\n",
    "\n",
    "        self.act = act\n",
    "        self.Q = nn.Linear(input_dims, hidden_dims)\n",
    "        self.W = nn.Linear(input_dims + hidden_dims, output_dims)\n",
    "        self.reset_parameters()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        gain = nn.init.calculate_gain('relu')\n",
    "        nn.init.xavier_uniform_(self.Q.weight, gain=gain)\n",
    "        nn.init.xavier_uniform_(self.W.weight, gain=gain)\n",
    "        nn.init.constant_(self.Q.bias, 0)\n",
    "        nn.init.constant_(self.W.bias, 0)\n",
    "\n",
    "    def forward(self, g, h, weights):\n",
    "        \"\"\"\n",
    "        g : graph\n",
    "        h : node features\n",
    "        weights : scalar edge weights\n",
    "        \"\"\"\n",
    "        h_src, h_dst = h\n",
    "        with g.local_scope():\n",
    "            # 将src节点上的原始特征映射成hidden_dims长，存储于'n'字段\n",
    "            g.srcdata['n'] = self.act(self.Q(self.dropout(h_src)))\n",
    "            g.edata['w'] = weights.float()\n",
    "            \n",
    "            # src节点上的特征'n'乘以边上的权重，构成消息'm'\n",
    "            # dst节点将所有接收到的消息'm'，相加起来，存入dst节点的'n'字段\n",
    "            g.update_all(fn.u_mul_e('n', 'w', 'm'), fn.sum('m', 'n'))\n",
    "            # 将边上的权重w拷贝成消息'm'\n",
    "            # dst节点将所有接收到的消息'm'，相加起来，存入dst节点的'ws'字段\n",
    "            g.update_all(fn.copy_e('w', 'm'), fn.sum('m', 'ws'))\n",
    "            n = g.dstdata['n'] # 邻居节点的embedding的加权和\n",
    "            ws = g.dstdata['ws'].unsqueeze(1).clamp(min=1)# 边上权重之和\n",
    "            \n",
    "            # 先将邻居节点的embedding，做加权平均\n",
    "            # 再拼接上一轮卷积后，dst节点自身的embedding\n",
    "            # 再经过线性变化与非线性激活，得到这一轮卷积后各dst节点的embedding\n",
    "            z = self.act(self.W(self.dropout(torch.cat([n / ws, h_dst], 1))))\n",
    "            \n",
    "            # 本轮卷积后，各dst节点的embedding除以模长，进行归一化\n",
    "            z_norm = z.norm(2, 1, keepdim=True)\n",
    "            z_norm = torch.where(z_norm == 0, torch.tensor(1.).to(z_norm), z_norm)\n",
    "            z = z / z_norm\n",
    "            return z\n",
    "\n",
    "class SAGENet(nn.Module):\n",
    "    \"\"\"\n",
    "    多层卷积模块\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dims, n_layers):\n",
    "        \"\"\"\n",
    "        g : DGLHeteroGraph\n",
    "            The user-item interaction graph.\n",
    "            This is only for finding the range of categorical variables.\n",
    "        item_textsets : torchtext.data.Dataset\n",
    "            The textual features of each item node.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        for _ in range(n_layers):\n",
    "            self.convs.append(WeightedSAGEConv(hidden_dims, hidden_dims, hidden_dims))\n",
    "\n",
    "    def forward(self, blocks, h):\n",
    "        for layer, block in zip(self.convs, blocks):\n",
    "            # 从h中分离出h_dst，应该是一种比较老的写法\n",
    "            # 直接写成h_dst = h[:block.number_of_dst_nodes()]即可\n",
    "            # 看了一下源码，这里用名称前缀来判断是否是dst的写法，应该就是block.number_of_dst_nodes()的内部实现\n",
    "            h_dst = h[:block.num_nodes('DST/' + block.ntypes[0])]\n",
    "            h = layer(block, (h, h_dst), block.edata['weights'])\n",
    "        return h\n",
    "\n",
    "class ItemToItemScorer(nn.Module):\n",
    "    \"\"\"\n",
    "    给边打分.SAGENet已经得到了由batch所构建的图上所有节点(heads + tails + neg_tails)的embedding，\n",
    "    这个模块给pos_graph和neg_graph中的每条边打分。打分逻辑就是，某边两端节点的点积，再加上两端节点的bias。\n",
    "    \"\"\"\n",
    "    def __init__(self, full_graph, ntype):\n",
    "        super().__init__()\n",
    "\n",
    "        n_nodes = full_graph.num_nodes(ntype)\n",
    "        self.bias = nn.Parameter(torch.zeros(n_nodes, 1))\n",
    "\n",
    "    def _add_bias(self, edges):\n",
    "        bias_src = self.bias[edges.src[dgl.NID]]\n",
    "        bias_dst = self.bias[edges.dst[dgl.NID]]\n",
    "        return {'s': edges.data['s'] + bias_src + bias_dst}\n",
    "\n",
    "    def forward(self, item_item_graph, h):\n",
    "        \"\"\"\n",
    "        item_item_graph : graph consists of edges connecting the pairs\n",
    "        h : hidden state of every node\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        输入节点组成的图(item_item_graph)和节点上的最终embedding(h)，计算item_item_graph中每条边上的得分\n",
    "\n",
    "        调用该函数时，item_item_graph会被传入pos_graph，或neg_graph，\n",
    "        这两幅图，都是由batch中的heads + tails + neg_tails组成的，只不过中间连接的边不同\n",
    "\n",
    "        无论传入的是pos_graph还是neg_graph，h都是相同的，\n",
    "        都是batch中heads + tails + neg_tails这些节点上的最终embedding\n",
    "        \"\"\"\n",
    "        with item_item_graph.local_scope():\n",
    "            item_item_graph.ndata['h'] = h\n",
    "            # 边两端节点的embedding做点积\n",
    "            item_item_graph.apply_edges(fn.u_dot_v('h', 'h', 's'))\n",
    "            # 再加上首尾两节点的bias\n",
    "            item_item_graph.apply_edges(self._add_bias)\n",
    "            pair_score = item_item_graph.edata['s']\n",
    "        return pair_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1721ccff-dc26-4da0-a869-e2a0cf23f9d2",
   "metadata": {},
   "source": [
    "##### 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c37e3d7-0d67-45a8-85c5-0fca7a98262d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PinSAGEModel(nn.Module):\n",
    "    def __init__(self, full_graph, ntype, textsets, hidden_dims, n_layers):\n",
    "        super().__init__()\n",
    "        # 负责将节点上的各种特征都映射成向量，并聚合在一起，形成这个节点的原始特征向量\n",
    "        self.proj = LinearProjector(full_graph, ntype, textsets, hidden_dims)\n",
    "        # 负责多层图卷积，得到各节点最终的embedding\n",
    "        self.sage = SAGENet(hidden_dims, n_layers)\n",
    "        # 负责根据首尾两端的节点的embedding，计算边上的得分\n",
    "        self.scorer = ItemToItemScorer(full_graph, ntype)\n",
    "\n",
    "    def forward(self, pos_graph, neg_graph, blocks):\n",
    "        # 得到batch中heads+tails+neg_tails这些节点的最终embedding\n",
    "        h_item = self.get_repr(blocks)\n",
    "        # 得到heads->tails这些边上的得分\n",
    "        pos_score = self.scorer(pos_graph, h_item)\n",
    "        # 得到heads->neg_tails这些边上的得分\n",
    "        neg_score = self.scorer(neg_graph, h_item)\n",
    "        # pos_graph与neg_graph边数相等，因此neg_score与pos_score相减\n",
    "        # 返回margin hinge loss，这里的margin是1 \n",
    "        return (neg_score - pos_score + 1).clamp(min=0)\n",
    "\n",
    "    def get_repr(self, blocks):\n",
    "        h_item = self.proj(blocks[0].srcdata) # 将输入节点上的原始特征映射成hidden_dims长的向量\n",
    "        h_item_dst = self.proj(blocks[-1].dstdata) # 将输出节点上的原始特征映射成hidden_dims长的向量\n",
    "        # 通过self.sage，经过多层卷积，得到输出节点上的卷积结果\n",
    "        # 再加上这些输出节点上原始特征的映射结果\n",
    "        # 得到输出节点上最终的向量表示\n",
    "        return h_item_dst + self.sage(blocks, h_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7d4979-4b75-4f27-a810-fb59dc06bdfc",
   "metadata": {},
   "source": [
    "##### Evaluation定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4af9907-94aa-40af-abae-5958627e9108",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import dgl\n",
    "import argparse\n",
    "\n",
    "def prec(recommendations, ground_truth):\n",
    "    n_users, n_items = ground_truth.shape\n",
    "    K = recommendations.shape[1]\n",
    "    user_idx = np.repeat(np.arange(n_users), K)\n",
    "    item_idx = recommendations.flatten()\n",
    "    relevance = ground_truth[user_idx, item_idx].reshape((n_users, K))\n",
    "    hit = relevance.any(axis=1).mean()\n",
    "    return hit\n",
    "\n",
    "class LatestNNRecommender(object):\n",
    "    def __init__(self, user_ntype, item_ntype, user_to_item_etype, timestamp, batch_size):\n",
    "        self.user_ntype = user_ntype\n",
    "        self.item_ntype = item_ntype\n",
    "        self.user_to_item_etype = user_to_item_etype\n",
    "        self.batch_size = batch_size\n",
    "        self.timestamp = timestamp\n",
    "\n",
    "    def recommend(self, full_graph, K, h_user, h_item):\n",
    "        \"\"\"\n",
    "        Return a (n_user, K) matrix of recommended items for each user\n",
    "        \"\"\"\n",
    "        graph_slice = full_graph.edge_type_subgraph([self.user_to_item_etype])\n",
    "        n_users = full_graph.num_nodes(self.user_ntype)\n",
    "        latest_interactions = dgl.sampling.select_topk(graph_slice, 1, self.timestamp, edge_dir='out')\n",
    "        user, latest_items = latest_interactions.all_edges(form='uv', order='srcdst')\n",
    "        # each user should have at least one \"latest\" interaction\n",
    "        assert torch.equal(user, torch.arange(n_users))\n",
    "\n",
    "        recommended_batches = []\n",
    "        user_batches = torch.arange(n_users).split(self.batch_size)\n",
    "        for user_batch in user_batches:\n",
    "            latest_item_batch = latest_items[user_batch].to(device=h_item.device)\n",
    "            dist = h_item[latest_item_batch] @ h_item.t()\n",
    "            # exclude items that are already interacted\n",
    "            for i, u in enumerate(user_batch.tolist()):\n",
    "                interacted_items = full_graph.successors(u, etype=self.user_to_item_etype)\n",
    "                dist[i, interacted_items] = -np.inf\n",
    "            recommended_batches.append(dist.topk(K, 1)[1])\n",
    "\n",
    "        recommendations = torch.cat(recommended_batches, 0)\n",
    "        return recommendations\n",
    "\n",
    "\n",
    "def evaluate_nn(dataset, h_item, k, batch_size):\n",
    "    g = dataset['train-graph']\n",
    "    val_matrix = dataset['val-matrix'].tocsr()\n",
    "    test_matrix = dataset['test-matrix'].tocsr()\n",
    "    item_texts = dataset['item-texts']\n",
    "    user_ntype = dataset['user-type']\n",
    "    item_ntype = dataset['item-type']\n",
    "    user_to_item_etype = dataset['user-to-item-type']\n",
    "    timestamp = dataset['timestamp-edge-column']\n",
    "\n",
    "    rec_engine = LatestNNRecommender(\n",
    "        user_ntype, item_ntype, user_to_item_etype, timestamp, batch_size)\n",
    "\n",
    "    recommendations = rec_engine.recommend(g, k, None, h_item).cpu().numpy()\n",
    "    return prec(recommendations, val_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9d75442-8fa1-43ec-bfaf-6493686ab2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "g= dataset['train-graph']\n",
    "val_matrix = dataset['val-matrix'].tocsr()\n",
    "test_matrix = dataset['test-matrix'].tocsr()\n",
    "item_texts = dataset['item-texts']\n",
    "user_ntype = dataset['user-type']\n",
    "item_ntype = dataset['item-type']\n",
    "user_to_item_etype = dataset['user-to-item-type']\n",
    "timestamp = dataset['timestamp-edge-column']\n",
    "device = torch.device(device)\n",
    "# Assign user and movie IDs and use them as features (to learn an individual trainable\n",
    "# embedding for each entity)\n",
    "g.nodes[user_ntype].data['id'] = torch.arange(g.num_nodes(user_ntype))\n",
    "g.nodes[item_ntype].data['id'] = torch.arange(g.num_nodes(item_ntype))\n",
    "# Prepare torchtext dataset and Vocabulary\n",
    "textset = {}\n",
    "tokenizer = get_tokenizer(None)\n",
    "textlist = []\n",
    "batch_first = True\n",
    "for i in range(g.num_nodes(item_ntype)):\n",
    "    for key in item_texts.keys():\n",
    "        l = tokenizer(item_texts[key][i].lower())\n",
    "        textlist.append(l)\n",
    "for key, field in item_texts.items():\n",
    "    vocab2 = build_vocab_from_iterator(textlist, specials=[\"<unk>\",\"<pad>\"])\n",
    "    textset[key] = (textlist, vocab2, vocab2.get_stoi()['<pad>'], batch_first)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b47b031-0857-4aac-b47c-8b9e400e3a27",
   "metadata": {},
   "source": [
    "\n",
    "#### Sampler过程\n",
    "Neighbor Sampling和Negative Sampling都发生在原图上，但是只发生在原图的部分节点上。比如DGL样例是为了实现item2item召回功能，因此两种采样都只发生在原图的item type节点上\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2c2809c-2245-4b0a-beac-8f5cbbc72d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\"\"\"\n",
    "# *************** 准备数据流\n",
    "# 负责采样出三个batch_size大小的节点列表: heads, tails,  neg_tails\n",
    "batch_sampler = ItemToItemBatchSampler(\n",
    "    g, user_ntype, item_ntype, batch_size)\n",
    "# 由一个batch中的heads,tails,neg_tails构建训练这个batch所需要的 pos_graph,neg_graph和blocks\n",
    "# 负责真正neighor sampling的逻辑\n",
    "# 根据batch_sampler提供的一个batch中的heads, tails, neg_tails\n",
    "# 由heads-->tails构成positive graph\n",
    "# 由heads-->neg_tails构成negative graph\n",
    "# 再由heads+tails+neg_tails反向搜索，构建每层卷积所需要的block\n",
    "neighbor_sampler = NeighborSampler(\n",
    "    g, user_ntype, item_ntype, random_walk_length,\n",
    "    random_walk_restart_prob, num_random_walks, num_neighbors,\n",
    "    num_layers)\n",
    "# 只是neighbor_sampler的一层封装，给定一个batch，\n",
    "# 1. 调用neighbor_sampler为生成这个batch中的heads,tails,neg_tails\n",
    "# 2. 根据heads,tails,neg_tails, 生成pos_graph,neg_graph和blocks，\n",
    "# 3. 然后将原图中节点的特征拷贝进blocks中的各个节点\n",
    "collator = PinSAGECollator(neighbor_sampler, g, item_ntype, textset)\n",
    "\n",
    "# 每次next()返回:pos_graph,neg_graph和blocks，做训练之用\n",
    "dataloader = DataLoader(\n",
    "    batch_sampler, # 每次调用生成一个batch，包含heads, tails, 和neg_tails\n",
    "    collate_fn=collator.collate_train, # 由heads+tails+和neg_tails生成pos_graph, neg_graph和blocks\n",
    "    num_workers=num_workers)\n",
    "\n",
    "# 每次next()返回blocks，做训练中测试之用(不能用于serving，因为低效，也因为获取block的过程中也有随机的成分)\n",
    "dataloader_test = DataLoader(\n",
    "    torch.arange(g.num_nodes(item_ntype)), # 原图中所有item node\n",
    "    batch_size=batch_size,\n",
    "    # 只生成blocks。注意这个函数只能用于训练时的测试，并不能用于生成上线用的向量\n",
    "    # 因为其中生成block也用到了邻居采样\n",
    "    # 而真正上线用的向量，必须拿一个节点的所有邻居进行卷积得到\n",
    "    collate_fn=collator.collate_test,\n",
    "    num_workers=num_workers)\n",
    "dataloader_it = iter(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e7645d-e065-4c93-9d00-e9e155f73ec0",
   "metadata": {},
   "source": [
    "\n",
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fb0a8be-7697-4c75-b38c-5de60ff7699b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]/data/miniconda3/envs/py39_torch_cpu/lib/python3.9/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "100%|██████████| 100/100 [00:17<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03543046357615894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:16<00:00,  6.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03559602649006623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:16<00:00,  5.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03642384105960265\n"
     ]
    }
   ],
   "source": [
    "# *************** 准备模型\n",
    "model = PinSAGEModel(g, item_ntype, textset,hidden_dims, num_layers).to(device)\n",
    "# Optimizer\n",
    "opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# *************** 训练\n",
    "# For each batch of head-tail-negative triplets...\n",
    "for epoch_id in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_id in tqdm.trange(batches_per_epoch):\n",
    "        pos_graph, neg_graph, blocks = next(dataloader_it)\n",
    "        # Copy to GPU\n",
    "        for i in range(len(blocks)):\n",
    "            blocks[i] = blocks[i].to(device)\n",
    "        pos_graph = pos_graph.to(device)\n",
    "        neg_graph = neg_graph.to(device)\n",
    "        loss = model(pos_graph, neg_graph, blocks).mean()\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    # Evaluate\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        item_batches = torch.arange(g.num_nodes(item_ntype)).split(batch_size)\n",
    "        h_item_batches = []\n",
    "        for blocks in dataloader_test:\n",
    "            for i in range(len(blocks)):\n",
    "                blocks[i] = blocks[i].to(device)\n",
    "            h_item_batches.append(model.get_repr(blocks))\n",
    "        h_item = torch.cat(h_item_batches, 0)\n",
    "        print(evaluate_nn(dataset, h_item, k, batch_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0967dbf9-3bf0-44e2-8304-bb5dc2729dd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaa18fb-200a-4700-8684-b9ba46dd2822",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d177855e-6eaa-4947-9d41-4e29698019bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39_torch_cpu",
   "language": "python",
   "name": "py39_torch_cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
