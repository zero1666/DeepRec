{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eba130d-ede1-493b-925e-23dcc1573005",
   "metadata": {},
   "source": [
    "# Pinsage Pytorch 实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1061b994-1582-4318-9af0-624f8da2577e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#全局配置\n",
    "input_dir = \"../../../data/ml-1m/\"\n",
    "output_dir = \"../../../data/ml-1m/tmp/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.getcwd()\n",
    "\n",
    "# 模型运行参数\n",
    "num_epochs = 3\n",
    "num_workers = 2\n",
    "\n",
    "hidden_dims = 64\n",
    "batch_size = 32\n",
    "num_random_walks=10\n",
    "random_walk_length=2\n",
    "random_walk_restart_prob=0.5\n",
    "num_neighbors=3\n",
    "num_layers=2\n",
    "lr=3e-5\n",
    "k=10\n",
    "batches_per_epoch=100\n",
    "device='cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656f8811-6b9f-48cc-852b-cef41c90aeee",
   "metadata": {},
   "source": [
    "## 生成图样本文件\n",
    "### Utils\n",
    "##### 图处理工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79e1d492-f22f-4ad6-9217-a5f3e81e41e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"build.py Graph builder from pandas dataframes\"\"\"\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "import dgl\n",
    "from pandas.api.types import is_categorical, is_categorical_dtype, is_numeric_dtype\n",
    "\n",
    "\n",
    "def _series_to_tensor(serises):\n",
    "    if is_categorical(series):\n",
    "        return torch.LongTensor(series.cat.codes.values.astype(\"int64\"))\n",
    "    else:\n",
    "        return torch.FloatTensor(series.values)\n",
    "\n",
    "\n",
    "class PandasGraphBUilder(object):\n",
    "    \"\"\"Creates a heterogeneous graph from multiple pandas dataframes.\n",
    "    Examples\n",
    "    --------\n",
    "    Let's say we have the following three pandas dataframes:\n",
    "    User table ``users``:\n",
    "    ===========  ===========  =======\n",
    "    ``user_id``  ``country``  ``age``\n",
    "    ===========  ===========  =======\n",
    "    XYZZY        U.S.         25\n",
    "    FOO          China        24\n",
    "    BAR          China        23\n",
    "    ===========  ===========  =======\n",
    "    Game table ``games``:\n",
    "    ===========  =========  ==============  ==================\n",
    "    ``game_id``  ``title``  ``is_sandbox``  ``is_multiplayer``\n",
    "    ===========  =========  ==============  ==================\n",
    "    1            Minecraft  True            True\n",
    "    2            Tetris 99  False           True\n",
    "    ===========  =========  ==============  ==================\n",
    "    Play relationship table ``plays``:\n",
    "    ===========  ===========  =========\n",
    "    ``user_id``  ``game_id``  ``hours``\n",
    "    ===========  ===========  =========\n",
    "    XYZZY        1            24\n",
    "    FOO          1            20\n",
    "    FOO          2            16\n",
    "    BAR          2            28\n",
    "    ===========  ===========  =========\n",
    "    One could then create a bidirectional bipartite graph as follows:\n",
    "    >>> builder = PandasGraphBuilder()\n",
    "    >>> builder.add_entities(users, 'user_id', 'user')\n",
    "    >>> builder.add_entities(games, 'game_id', 'game')\n",
    "    >>> builder.add_binary_relations(plays, 'user_id', 'game_id', 'plays')\n",
    "    >>> builder.add_binary_relations(plays, 'game_id', 'user_id', 'played-by')\n",
    "    >>> g = builder.build()\n",
    "    >>> g.num_nodes('user')\n",
    "    3\n",
    "    >>> g.num_edges('plays')\n",
    "    4\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.entity_tables = {}\n",
    "        self.relation_tables = {}\n",
    "\n",
    "        self.entity_pk_to_name = {}  # mapping from primary key name to entity name\n",
    "        self.entity_pk = {}  # mapping from entity name to primary key\n",
    "        self.entity_key_map = {}  # mapping from entity names to primary key values\n",
    "        self.num_nodes_per_type = {}\n",
    "        self.edges_per_relation = {}\n",
    "        self.relation_src_key = {}\n",
    "        self.relation_dst_key = {}\n",
    "        self.relation_name_to_etype = {}\n",
    "\n",
    "    def add_entities(self, entity_table, primary_key, name):\n",
    "        entities = entity_table[primary_key].astype(\"category\")\n",
    "        if not (entities.value_counts() == 1).all():\n",
    "            raise ValueError(\"Different entity with the same primary key detected.\")\n",
    "        # preserve the category order in the original entity table\n",
    "        entities = entities.cat.reorder_categories(entity_table[primary_key].values)\n",
    "\n",
    "        self.entity_pk_to_name[primary_key] = name\n",
    "        self.entity_pk[name] = primary_key\n",
    "        self.num_nodes_per_type[name] = entity_table.shape[0]\n",
    "        self.entity_key_map[name] = entities\n",
    "        self.entity_tables[name] = entity_table\n",
    "\n",
    "    def add_binary_relations(self, relation_table, source_key, destination_key, name):\n",
    "        src = relation_table[source_key].astype(\"category\")\n",
    "        src = src.cat.set_categories(\n",
    "            self.entity_key_map[self.entity_pk_to_name[source_key]].cat.categories\n",
    "        )\n",
    "        dst = relation_table[destination_key].astype(\"category\")\n",
    "        dst = dst.cat.set_categories(\n",
    "            self.entity_key_map[self.entity_pk_to_name[destination_key]].cat.categories\n",
    "        )\n",
    "\n",
    "        if src.isnull().any():\n",
    "            raise ValueError(\n",
    "                \"Some source entities in relation %s do not exist in entity %s.\"\n",
    "                % (name, source_key)\n",
    "            )\n",
    "        if dst.isnull().any():\n",
    "            raise ValueError(\n",
    "                \"Some destination entities in relation %s do not exist in entity %s.\"\n",
    "                % (name, destination_key)\n",
    "            )\n",
    "\n",
    "        srctype = self.entity_pk_to_name[source_key]\n",
    "        dsttype = self.entity_pk_to_name[destination_key]\n",
    "        etype = (srctype, name, dsttype)\n",
    "        self.relation_name_to_etype[name] = etype\n",
    "        self.edges_per_relation[etype] = (\n",
    "            src.cat.codes.values.astype(\"int64\"),\n",
    "            dst.cat.codes.values.astype(\"int64\"),\n",
    "        )\n",
    "        self.relation_tables[name] = relation_table\n",
    "        self.relation_src_key[name] = source_key\n",
    "        self.relation_dst_key[name] = destination_key\n",
    "\n",
    "    def build(self):\n",
    "        # Create heterograph\n",
    "        graph = dgl.heterograph(self.edges_per_relation, self.num_nodes_per_type)\n",
    "        return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63fd3228-5aa1-4a70-a05c-644c96521446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import dgl\n",
    "import numpy as np\n",
    "import scipy.sparse as ssp\n",
    "import torch\n",
    "import tqdm\n",
    "\n",
    "\n",
    "# This is the train-test split method most of the recommender system papers running on MovieLens\n",
    "# takes.  It essentially follows the intuition of \"training on the past and predict the future\".\n",
    "# One can also change the threshold to make validation and test set take larger proportions.\n",
    "def train_test_split_by_time(df, timestamp, user):\n",
    "    df[\"train_mask\"] = np.ones((len(df),), dtype=np.bool)\n",
    "    df[\"val_mask\"] = np.zeros((len(df),), dtype=np.bool)\n",
    "    df[\"test_mask\"] = np.zeros((len(df),), dtype=np.bool)\n",
    "    df = dd.from_pandas(df, npartitions=10)\n",
    "\n",
    "    def train_test_split(df):\n",
    "        df = df.sort_values([timestamp])\n",
    "        if df.shape[0] > 1:\n",
    "            df.iloc[-1, -3] = False\n",
    "            df.iloc[-1, -1] = True\n",
    "        if df.shape[0] > 2:\n",
    "            df.iloc[-2, -3] = False\n",
    "            df.iloc[-2, -2] = True\n",
    "        return df\n",
    "    print(type(df), df.shape,df.shape[0])\n",
    "    df = (\n",
    "        df.groupby(user, group_keys=False)\n",
    "        .apply(train_test_split)\n",
    "        .compute(scheduler=\"processes\")\n",
    "        .sort_index()\n",
    "    )\n",
    "    print(df[df[user] == df[user].unique()[0]].sort_values(timestamp))\n",
    "    return (\n",
    "        df[\"train_mask\"].to_numpy().nonzero()[0],\n",
    "        df[\"val_mask\"].to_numpy().nonzero()[0],\n",
    "        df[\"test_mask\"].to_numpy().nonzero()[0],\n",
    "    )\n",
    "\n",
    "\n",
    "def build_train_graph(g, train_indices, utype, itype, etype, etype_rev):\n",
    "    train_g = g.edge_subgraph(\n",
    "        {etype: train_indices, etype_rev: train_indices}, relabel_nodes=False\n",
    "    )\n",
    "\n",
    "    # copy features\n",
    "    for ntype in g.ntypes:\n",
    "        for col, data in g.nodes[ntype].data.items():\n",
    "            train_g.nodes[ntype].data[col] = data\n",
    "    for etype in g.etypes:\n",
    "        for col, data in g.edges[etype].data.items():\n",
    "            train_g.edges[etype].data[col] = data[train_g.edges[etype].data[dgl.EID]]\n",
    "\n",
    "    return train_g\n",
    "\n",
    "\n",
    "def build_val_test_matrix(g, val_indices, test_indices, utype, itype, etype):\n",
    "    n_users = g.num_nodes(utype)\n",
    "    n_items = g.num_nodes(itype)\n",
    "    val_src, val_dst = g.find_edges(val_indices, etype=etype)\n",
    "    test_src, test_dst = g.find_edges(test_indices, etype=etype)\n",
    "    val_src = val_src.numpy()\n",
    "    val_dst = val_dst.numpy()\n",
    "    test_src = test_src.numpy()\n",
    "    test_dst = test_dst.numpy()\n",
    "    val_matrix = ssp.coo_matrix(\n",
    "        (np.ones_like(val_src), (val_src, val_dst)), (n_users, n_items)\n",
    "    )\n",
    "    test_matrix = ssp.coo_matrix(\n",
    "        (np.ones_like(test_src), (test_src, test_dst)), (n_users, n_items)\n",
    "    )\n",
    "\n",
    "    return val_matrix, test_matrix\n",
    "\n",
    "\n",
    "def linear_normalize(values):\n",
    "    return (values - values.min(0, keepdims=True)) / (\n",
    "        values.max(0, keepdims=True) - values.min(0, keepdims=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec85324-a1c2-4597-8512-8422d3dcef7b",
   "metadata": {},
   "source": [
    "### Prepare datasets（MovieLens 1M）\n",
    "+ [dgl pinsage](https://github.com/dmlc/dgl/tree/master/examples/pytorch/pinsage)\n",
    "\n",
    "MovieLens 1M数据集含有来自6000名用户对4000部电影的100万条评分数据。分为三个表：评分，用户信息，电影信息。这些数据都是dat文件格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ff40477-a69b-4446-919d-ae86eea3fd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "import dgl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as ssp\n",
    "import torch\n",
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17aac03b-c120-4795-8e16-275d3ccdb778",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build heterogenerous graph\n",
    "# Load data\n",
    "users = []\n",
    "with open(os.path.join(input_dir, \"users.dat\"), encoding=\"latin1\") as f:\n",
    "    for l in f:\n",
    "        id_, gender, age, occupation, zip_ = l.strip().split(\"::\")\n",
    "        users.append(\n",
    "            {\n",
    "                \"user_id\": int(id_),\n",
    "                \"gender\": gender,\n",
    "                \"age\": age,\n",
    "                \"occupation\": occupation,\n",
    "                \"zip\": zip_,\n",
    "            }\n",
    "        )\n",
    "users = pd.DataFrame(users).astype(\"category\")\n",
    "\n",
    "movies = []\n",
    "with open(os.path.join(input_dir, \"movies.dat\"), encoding=\"latin1\") as f:\n",
    "    for l in f:\n",
    "        id_, title, genres = l.strip().split(\"::\")\n",
    "        genres_set = set(genres.split(\"|\"))\n",
    "\n",
    "        # extract year\n",
    "        assert re.match(r\".*\\([0-9]{4}\\)$\", title)\n",
    "        year = title[-5:-1]\n",
    "        title = title[:-6].strip()\n",
    "\n",
    "        data = {\"movie_id\": int(id_), \"title\": title, \"year\": year}\n",
    "        for g in genres_set:\n",
    "            data[g] = True\n",
    "        movies.append(data)\n",
    "movies = pd.DataFrame(movies).astype({\"year\": \"category\"})\n",
    "\n",
    "ratings = []\n",
    "with open(os.path.join(input_dir, \"ratings.dat\"), encoding=\"latin1\") as f:\n",
    "    for l in f:\n",
    "        user_id, movie_id, rating, timestamp = [int(_) for _ in l.split(\"::\")]\n",
    "        ratings.append(\n",
    "            {\n",
    "                \"user_id\": user_id,\n",
    "                \"movie_id\": movie_id,\n",
    "                \"rating\": rating,\n",
    "                \"timestamp\": timestamp,\n",
    "            }\n",
    "        )\n",
    "ratings = pd.DataFrame(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fc9b6dd-f406-4214-b4b3-4bb280c6f167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users: (6040, 5)\n",
      "  user_id gender age occupation    zip\n",
      "0       1      F   1         10  48067\n",
      "1       2      M  56         16  70072\n",
      "2       3      M  25         15  55117\n",
      "3       4      M  45          7  02460\n",
      "4       5      M  25         20  55455\n",
      "movies (3883, 21)\n",
      "   movie_id                        title  year Animation Comedy Children's  \\\n",
      "0         1                    Toy Story  1995      True   True       True   \n",
      "1         2                      Jumanji  1995       NaN    NaN       True   \n",
      "2         3             Grumpier Old Men  1995       NaN   True        NaN   \n",
      "3         4            Waiting to Exhale  1995       NaN   True        NaN   \n",
      "4         5  Father of the Bride Part II  1995       NaN   True        NaN   \n",
      "\n",
      "  Adventure Fantasy Romance Drama  ... Crime Thriller Horror Sci-Fi  \\\n",
      "0       NaN     NaN     NaN   NaN  ...   NaN      NaN    NaN    NaN   \n",
      "1      True    True     NaN   NaN  ...   NaN      NaN    NaN    NaN   \n",
      "2       NaN     NaN    True   NaN  ...   NaN      NaN    NaN    NaN   \n",
      "3       NaN     NaN     NaN  True  ...   NaN      NaN    NaN    NaN   \n",
      "4       NaN     NaN     NaN   NaN  ...   NaN      NaN    NaN    NaN   \n",
      "\n",
      "  Documentary  War Musical Mystery Film-Noir Western  \n",
      "0         NaN  NaN     NaN     NaN       NaN     NaN  \n",
      "1         NaN  NaN     NaN     NaN       NaN     NaN  \n",
      "2         NaN  NaN     NaN     NaN       NaN     NaN  \n",
      "3         NaN  NaN     NaN     NaN       NaN     NaN  \n",
      "4         NaN  NaN     NaN     NaN       NaN     NaN  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "rating (1000209, 4)\n",
      "   user_id  movie_id  rating  timestamp\n",
      "0        1      1193       5  978300760\n",
      "1        1       661       3  978302109\n",
      "2        1       914       3  978301968\n",
      "3        1      3408       4  978300275\n",
      "4        1      2355       5  978824291\n"
     ]
    }
   ],
   "source": [
    "print(\"users:\", users.shape)\n",
    "print(users.head())\n",
    "print(\"movies\", movies.shape)\n",
    "print(movies.head())\n",
    "print(\"rating\", ratings.shape)\n",
    "print(ratings.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37b416b3-c03d-49af-80d1-c00caa4474cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users: (6040, 5)\n",
      "movies (3706, 21)\n"
     ]
    }
   ],
   "source": [
    "# Filter the users and items that never appear in the rating table.\n",
    "distinct_users_in_rating = ratings[\"user_id\"].unique()\n",
    "distinct_movies_in_rating = ratings[\"movie_id\"].unique()\n",
    "users = users[users[\"user_id\"].isin(distinct_users_in_rating)]\n",
    "movies = movies[movies[\"movie_id\"].isin(distinct_movies_in_rating)]\n",
    "\n",
    "print(\"users:\", users.shape)\n",
    "print(\"movies\", movies.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28990d53-c932-4b88-af9f-18faa7b04193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   movie_id  year  Animation  Comedy  Children's  Adventure  Fantasy  Romance  \\\n",
      "0         1  1995       True    True        True      False    False    False   \n",
      "1         2  1995      False   False        True       True     True    False   \n",
      "2         3  1995      False    True       False      False    False     True   \n",
      "3         4  1995      False    True       False      False    False    False   \n",
      "4         5  1995      False    True       False      False    False    False   \n",
      "\n",
      "   Drama  Action  Crime  Thriller  Horror  Sci-Fi  Documentary    War  \\\n",
      "0  False   False  False     False   False   False        False  False   \n",
      "1  False   False  False     False   False   False        False  False   \n",
      "2  False   False  False     False   False   False        False  False   \n",
      "3   True   False  False     False   False   False        False  False   \n",
      "4  False   False  False     False   False   False        False  False   \n",
      "\n",
      "   Musical  Mystery  Film-Noir  Western  \n",
      "0    False    False      False    False  \n",
      "1    False    False      False    False  \n",
      "2    False    False      False    False  \n",
      "3    False    False      False    False  \n",
      "4    False    False      False    False  \n"
     ]
    }
   ],
   "source": [
    "# Group the movie features into genres (a vector), year (a category), title (a string)\n",
    "genera_columns = movies.columns.drop([\"movie_id\", \"title\", \"year\"])\n",
    "movies[genera_columns] = movies[genera_columns].fillna(False).astype(\"bool\")\n",
    "movies_categorical = movies.drop(\"title\", axis=1)\n",
    "\n",
    "print(movies_categorical.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "598717b4-d08a-4916-ac81-85119b37a814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build graph\n",
    "graph_builder = PandasGraphBUilder()\n",
    "graph_builder.add_entities(users, \"user_id\", \"user\")\n",
    "graph_builder.add_entities(movies_categorical, \"movie_id\", \"movie\")\n",
    "graph_builder.add_binary_relations(ratings, \"user_id\", \"movie_id\", \"watched\")\n",
    "graph_builder.add_binary_relations(ratings, \"movie_id\", \"user_id\", \"watched-by\")\n",
    "g = graph_builder.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a753a61e-7d4c-4299-80e8-7c80e9b6db73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_62828/887568951.py:3: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:178.)\n",
      "  g.nodes[\"user\"].data[\"gender\"] = torch.LongTensor(users[\"gender\"].cat.codes.values)\n"
     ]
    }
   ],
   "source": [
    "# Assign features.\n",
    "# Note that variable-sized features such as texts or images are handled elsewhere.\n",
    "g.nodes[\"user\"].data[\"gender\"] = torch.LongTensor(users[\"gender\"].cat.codes.values)\n",
    "g.nodes[\"user\"].data[\"age\"] = torch.LongTensor(users[\"age\"].cat.codes.values)\n",
    "g.nodes[\"user\"].data[\"occupation\"] = torch.LongTensor(\n",
    "    users[\"occupation\"].cat.codes.values\n",
    ")\n",
    "g.nodes[\"user\"].data[\"zip\"] = torch.LongTensor(users[\"zip\"].cat.codes.values)\n",
    "\n",
    "g.nodes[\"movie\"].data[\"year\"] = torch.LongTensor(movies[\"year\"].cat.codes.values)\n",
    "g.nodes[\"movie\"].data[\"genre\"] = torch.FloatTensor(movies[genera_columns].values)\n",
    "\n",
    "g.edges[\"watched\"].data[\"rating\"] = torch.LongTensor(ratings[\"rating\"].values)\n",
    "g.edges[\"watched\"].data[\"timestamp\"] = torch.LongTensor(ratings[\"timestamp\"].values)\n",
    "g.edges[\"watched-by\"].data[\"rating\"] = torch.LongTensor(ratings[\"rating\"].values)\n",
    "g.edges[\"watched-by\"].data[\"timestamp\"] = torch.LongTensor(ratings[\"timestamp\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8448707-e5a4-46df-96bc-b386e654c5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6040])\n"
     ]
    }
   ],
   "source": [
    "print((g.nodes[\"user\"].data[\"gender\"].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b79edc36-a897-4b39-9543-281a35ee0edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask.dataframe.core.DataFrame'> (Delayed('int-c845e192-8e59-4b84-8f62-db71a84079b1'), 7) Delayed('int-32a66189-d86e-468c-a19e-e64c74b4f8c1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_62828/680607132.py:13: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  df[\"train_mask\"] = np.ones((len(df),), dtype=np.bool)\n",
      "/tmp/ipykernel_62828/680607132.py:14: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  df[\"val_mask\"] = np.zeros((len(df),), dtype=np.bool)\n",
      "/tmp/ipykernel_62828/680607132.py:15: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  df[\"test_mask\"] = np.zeros((len(df),), dtype=np.bool)\n",
      "/tmp/ipykernel_62828/680607132.py:29: UserWarning: `meta` is not specified, inferred from partial data. Please provide `meta` if the result is unexpected.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta={'x': 'f8', 'y': 'f8'}) for dataframe result\n",
      "  or:     .apply(func, meta=('x', 'f8'))            for series result\n",
      "  df.groupby(user, group_keys=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    user_id  movie_id  rating  timestamp  train_mask  val_mask  test_mask\n",
      "31        1      3186       4  978300019        True     False      False\n",
      "27        1      1721       4  978300055        True     False      False\n",
      "37        1      1022       5  978300055        True     False      False\n",
      "22        1      1270       5  978300055        True     False      False\n",
      "24        1      2340       3  978300103        True     False      False\n",
      "36        1      1836       5  978300172        True     False      False\n",
      "3         1      3408       4  978300275        True     False      False\n",
      "47        1      1207       4  978300719        True     False      False\n",
      "7         1      2804       5  978300719        True     False      False\n",
      "21        1       720       3  978300760        True     False      False\n",
      "0         1      1193       5  978300760        True     False      False\n",
      "44        1       260       4  978300760        True     False      False\n",
      "9         1       919       4  978301368        True     False      False\n",
      "51        1       608       4  978301398        True     False      False\n",
      "43        1      2692       4  978301570        True     False      False\n",
      "41        1      1961       5  978301590        True     False      False\n",
      "48        1      2028       5  978301619        True     False      False\n",
      "18        1      3105       5  978301713        True     False      False\n",
      "11        1       938       4  978301752        True     False      False\n",
      "42        1      1962       4  978301753        True     False      False\n",
      "14        1      1035       5  978301753        True     False      False\n",
      "39        1       150       5  978301777        True     False      False\n",
      "17        1      2018       4  978301777        True     False      False\n",
      "45        1      1028       5  978301777        True     False      False\n",
      "26        1      1097       4  978301953        True     False      False\n",
      "2         1       914       3  978301968        True     False      False\n",
      "19        1      2797       4  978302039        True     False      False\n",
      "6         1      1287       5  978302039        True     False      False\n",
      "38        1      2762       4  978302091        True     False      False\n",
      "52        1      1246       4  978302091        True     False      False\n",
      "1         1       661       3  978302109        True     False      False\n",
      "13        1      2918       4  978302124        True     False      False\n",
      "49        1       531       4  978302149        True     False      False\n",
      "50        1      3114       4  978302174        True     False      False\n",
      "15        1      2791       4  978302188        True     False      False\n",
      "46        1      1029       5  978302205        True     False      False\n",
      "20        1      2321       3  978302205        True     False      False\n",
      "5         1      1197       3  978302268        True     False      False\n",
      "8         1       594       4  978302268        True     False      False\n",
      "12        1      2398       4  978302281        True     False      False\n",
      "28        1      1545       4  978824139        True     False      False\n",
      "23        1       527       5  978824195        True     False      False\n",
      "40        1         1       5  978824268        True     False      False\n",
      "33        1       588       4  978824268        True     False      False\n",
      "16        1      2687       3  978824268        True     False      False\n",
      "29        1       745       3  978824268        True     False      False\n",
      "10        1       595       5  978824268        True     False      False\n",
      "30        1      2294       4  978824291        True     False      False\n",
      "35        1       783       4  978824291        True     False      False\n",
      "4         1      2355       5  978824291        True     False      False\n",
      "34        1      1907       4  978824330        True     False      False\n",
      "32        1      1566       4  978824330       False      True      False\n",
      "25        1        48       5  978824351       False     False       True\n"
     ]
    }
   ],
   "source": [
    "# Train-validation-test split\n",
    "# This is a little bit tricky as we want to select the last interaction for test, and the\n",
    "# second-to-last interaction for validation.\n",
    "train_indices, val_indices, test_indices = train_test_split_by_time(\n",
    "    ratings, \"timestamp\", \"user_id\"\n",
    ")\n",
    "# Build the graph with training interactions only.\n",
    "train_g = build_train_graph(g, train_indices, \"user\", \"movie\", \"watched\", \"watched-by\")\n",
    "assert train_g.out_degrees(etype=\"watched\").min() > 0\n",
    "\n",
    "val_matrix, test_matrix = build_val_test_matrix(\n",
    "    g, val_indices, test_indices, \"user\", \"movie\", \"watched\"\n",
    ")\n",
    "## Build title set\n",
    "movie_textual_dataset = {\"title\": movies[\"title\"].values}\n",
    "# The model should build their own vocabulary and process the texts.  Here is one example\n",
    "# of using torchtext to pad and numericalize a batch of strings.\n",
    "#     field = torchtext.data.Field(include_lengths=True, lower=True, batch_first=True)\n",
    "#     examples = [torchtext.data.Example.fromlist([t], [('title', title_field)]) for t in texts]\n",
    "#     titleset = torchtext.data.Dataset(examples, [('title', title_field)])\n",
    "#     field.build_vocab(titleset.title, vectors='fasttext.simple.300d')\n",
    "#     token_ids, lengths = field.process([examples[0].title, examples[1].title])\n",
    "## Dump the graph and the datasets\n",
    "dgl.save_graphs(os.path.join(output_dir, \"train_g.bin\"), train_g)\n",
    "dataset = {\n",
    "    \"val-matrix\": val_matrix,\n",
    "    \"test-matrix\": test_matrix,\n",
    "    \"item-texts\": movie_textual_dataset,\n",
    "    \"item-images\": None,\n",
    "    \"user-type\": \"user\",\n",
    "    \"item-type\": \"movie\",\n",
    "    \"user-to-item-type\": \"watched\",\n",
    "    \"item-to-user-type\": \"watched-by\",\n",
    "    \"timestamp-edge-column\": \"timestamp\",\n",
    "}\n",
    "with open(os.path.join(output_dir, \"data.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(dataset, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5d39a6-203f-483c-840f-628698ad82aa",
   "metadata": {},
   "source": [
    "## 准备模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c3d8c95-6fba-40cb-9072-1120fcfdc688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchtext\n",
    "import dgl\n",
    "import os\n",
    "import tqdm\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16dac97a-ff86-40eb-85fc-08e55bedebee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_info_path = os.path.join(output_dir, 'data.pkl')\n",
    "with open(data_info_path, 'rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "train_g_path = os.path.join(output_dir, 'train_g.bin')\n",
    "g_list, _ = dgl.load_graphs(train_g_path)\n",
    "dataset['train-graph'] = g_list[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b266d17e-96ae-46f2-bbd2-7f8bbf4b50fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'> dict_keys(['val-matrix', 'test-matrix', 'item-texts', 'item-images', 'user-type', 'item-type', 'user-to-item-type', 'item-to-user-type', 'timestamp-edge-column', 'train-graph'])\n",
      "<class 'list'> Graph(num_nodes={'movie': 3706, 'user': 6040},\n",
      "      num_edges={('movie', 'watched-by', 'user'): 988129, ('user', 'watched', 'movie'): 988129},\n",
      "      metagraph=[('movie', 'user', 'watched-by'), ('user', 'movie', 'watched')])\n"
     ]
    }
   ],
   "source": [
    "print(type(dataset), dataset.keys())\n",
    "print(type(g_list), dataset['train-graph'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1943f54e-d72b-4ff0-ad4b-29eb6349d5a5",
   "metadata": {},
   "source": [
    "### Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b61a2f0a-f919-4dfc-b57a-57eed35c8010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dgl\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "from torchtext.data.functional import numericalize_tokens_from_iterator\n",
    "\n",
    "def padding(array, yy, val):\n",
    "    \"\"\"\n",
    "    :param array: torch tensor array\n",
    "    :param yy: desired width\n",
    "    :param val: padded value\n",
    "    :return: padded array\n",
    "    \"\"\"\n",
    "    w = array.shape[0]\n",
    "    b = 0\n",
    "    bb = yy - b - w\n",
    "\n",
    "    return torch.nn.functional.pad(array, pad=(b, bb), mode='constant', value=val)\n",
    "\n",
    "def compact_and_copy(frontier, seeds):\n",
    "    block = dgl.to_block(frontier, seeds)\n",
    "    for col, data in frontier.edata.items():\n",
    "        if col == dgl.EID:\n",
    "            continue\n",
    "        block.edata[col] = data[block.edata[dgl.EID]]\n",
    "    return block\n",
    "\n",
    "class ItemToItemBatchSampler(IterableDataset):\n",
    "    def __init__(self, g, user_type, item_type, batch_size):\n",
    "        self.g = g\n",
    "        self.user_type = user_type\n",
    "        self.item_type = item_type\n",
    "        self.user_to_item_etype = list(g.metagraph()[user_type][item_type])[0]\n",
    "        self.item_to_user_etype = list(g.metagraph()[item_type][user_type])[0]\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            heads = torch.randint(0, self.g.num_nodes(self.item_type), (self.batch_size,))\n",
    "            tails = dgl.sampling.random_walk(\n",
    "                self.g,\n",
    "                heads,\n",
    "                metapath=[self.item_to_user_etype, self.user_to_item_etype])[0][:, 2]\n",
    "            neg_tails = torch.randint(0, self.g.num_nodes(self.item_type), (self.batch_size,))\n",
    "\n",
    "            mask = (tails != -1)\n",
    "            yield heads[mask], tails[mask], neg_tails[mask]\n",
    "\n",
    "class NeighborSampler(object):\n",
    "    def __init__(self, g, user_type, item_type, random_walk_length, random_walk_restart_prob,\n",
    "                 num_random_walks, num_neighbors, num_layers):\n",
    "        self.g = g\n",
    "        self.user_type = user_type\n",
    "        self.item_type = item_type\n",
    "        self.user_to_item_etype = list(g.metagraph()[user_type][item_type])[0]\n",
    "        self.item_to_user_etype = list(g.metagraph()[item_type][user_type])[0]\n",
    "        self.samplers = [\n",
    "            dgl.sampling.PinSAGESampler(g, item_type, user_type, random_walk_length,\n",
    "                random_walk_restart_prob, num_random_walks, num_neighbors)\n",
    "            for _ in range(num_layers)]\n",
    "\n",
    "    def sample_blocks(self, seeds, heads=None, tails=None, neg_tails=None):\n",
    "        blocks = []\n",
    "        for sampler in self.samplers:\n",
    "            frontier = sampler(seeds)\n",
    "            if heads is not None:\n",
    "                eids = frontier.edge_ids(torch.cat([heads, heads]), torch.cat([tails, neg_tails]), return_uv=True)[2]\n",
    "                if len(eids) > 0:\n",
    "                    old_frontier = frontier\n",
    "                    frontier = dgl.remove_edges(old_frontier, eids)\n",
    "                    #print(old_frontier)\n",
    "                    #print(frontier)\n",
    "                    #print(frontier.edata['weights'])\n",
    "                    #frontier.edata['weights'] = old_frontier.edata['weights'][frontier.edata[dgl.EID]]\n",
    "            block = compact_and_copy(frontier, seeds)\n",
    "            seeds = block.srcdata[dgl.NID]\n",
    "            blocks.insert(0, block)\n",
    "        return blocks\n",
    "\n",
    "    def sample_from_item_pairs(self, heads, tails, neg_tails):\n",
    "        # Create a graph with positive connections only and another graph with negative\n",
    "        # connections only.\n",
    "        pos_graph = dgl.graph(\n",
    "            (heads, tails),\n",
    "            num_nodes=self.g.num_nodes(self.item_type))\n",
    "        neg_graph = dgl.graph(\n",
    "            (heads, neg_tails),\n",
    "            num_nodes=self.g.num_nodes(self.item_type))\n",
    "        pos_graph, neg_graph = dgl.compact_graphs([pos_graph, neg_graph])\n",
    "        seeds = pos_graph.ndata[dgl.NID]\n",
    "\n",
    "        blocks = self.sample_blocks(seeds, heads, tails, neg_tails)\n",
    "        return pos_graph, neg_graph, blocks\n",
    "\n",
    "def assign_simple_node_features(ndata, g, ntype, assign_id=False):\n",
    "    \"\"\"\n",
    "    Copies data to the given block from the corresponding nodes in the original graph.\n",
    "    \"\"\"\n",
    "    for col in g.nodes[ntype].data.keys():\n",
    "        if not assign_id and col == dgl.NID:\n",
    "            continue\n",
    "        induced_nodes = ndata[dgl.NID]\n",
    "        ndata[col] = g.nodes[ntype].data[col][induced_nodes]\n",
    "\n",
    "def assign_textual_node_features(ndata, textset, ntype):\n",
    "    \"\"\"\n",
    "    Assigns numericalized tokens from a torchtext dataset to given block.\n",
    "    The numericalized tokens would be stored in the block as node features\n",
    "    with the same name as ``field_name``.\n",
    "    The length would be stored as another node feature with name\n",
    "    ``field_name + '__len'``.\n",
    "    block : DGLHeteroGraph\n",
    "        First element of the compacted blocks, with \"dgl.NID\" as the\n",
    "        corresponding node ID in the original graph, hence the index to the\n",
    "        text dataset.\n",
    "        The numericalized tokens (and lengths if available) would be stored\n",
    "        onto the blocks as new node features.\n",
    "    textset : torchtext.data.Dataset\n",
    "        A torchtext dataset whose number of examples is the same as that\n",
    "        of nodes in the original graph.\n",
    "    \"\"\"\n",
    "    node_ids = ndata[dgl.NID].numpy()\n",
    "\n",
    "    for field_name, field in textset.items():\n",
    "        textlist, vocab, pad_var, batch_first = field\n",
    "        \n",
    "        examples = [textlist[i] for i in node_ids]\n",
    "        ids_iter = numericalize_tokens_from_iterator(vocab, examples)\n",
    "        \n",
    "        maxsize = max([len(textlist[i]) for i in node_ids])\n",
    "        ids = next(ids_iter)\n",
    "        x = torch.asarray([num for num in ids])\n",
    "        lengths = torch.tensor([len(x)])\n",
    "        tokens = padding(x, maxsize, pad_var)\n",
    "\n",
    "        for ids in ids_iter:\n",
    "            x = torch.asarray([num for num in ids])\n",
    "            l = torch.tensor([len(x)])\n",
    "            y = padding(x, maxsize, pad_var)\n",
    "            tokens = torch.vstack((tokens,y))\n",
    "            lengths = torch.cat((lengths, l))\n",
    "       \n",
    "        if not batch_first:\n",
    "            tokens = tokens.t()\n",
    "\n",
    "        ndata[field_name] = tokens\n",
    "        ndata[field_name + '__len'] = lengths\n",
    "\n",
    "def assign_features_to_blocks(blocks, g, textset, ntype):\n",
    "    # For the first block (which is closest to the input), copy the features from\n",
    "    # the original graph as well as the texts.\n",
    "    assign_simple_node_features(blocks[0].srcdata, g, ntype)\n",
    "    assign_textual_node_features(blocks[0].srcdata, textset, ntype)\n",
    "    assign_simple_node_features(blocks[-1].dstdata, g, ntype)\n",
    "    assign_textual_node_features(blocks[-1].dstdata, textset, ntype)\n",
    "\n",
    "class PinSAGECollator(object):\n",
    "    def __init__(self, sampler, g, ntype, textset):\n",
    "        self.sampler = sampler\n",
    "        self.ntype = ntype\n",
    "        self.g = g\n",
    "        self.textset = textset\n",
    "\n",
    "    def collate_train(self, batches):\n",
    "        heads, tails, neg_tails = batches[0]\n",
    "        # Construct multilayer neighborhood via PinSAGE...\n",
    "        pos_graph, neg_graph, blocks = self.sampler.sample_from_item_pairs(heads, tails, neg_tails)\n",
    "        assign_features_to_blocks(blocks, self.g, self.textset, self.ntype)\n",
    "\n",
    "        return pos_graph, neg_graph, blocks\n",
    "\n",
    "    def collate_test(self, samples):\n",
    "        batch = torch.LongTensor(samples)\n",
    "        blocks = self.sampler.sample_blocks(batch)\n",
    "        assign_features_to_blocks(blocks, self.g, self.textset, self.ntype)\n",
    "        return blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8fab9a-4790-4126-b64a-abab2f92f3a5",
   "metadata": {},
   "source": [
    "### 定义模型结构"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12febd1-fa6d-47a1-a76e-f9733ed3fa85",
   "metadata": {},
   "source": [
    "##### Layer定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcfc7ad1-5bcd-4bc2-a73f-2e5d9a288f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl\n",
    "import dgl.nn.pytorch as dglnn\n",
    "import dgl.function as fn\n",
    "\n",
    "def disable_grad(module):\n",
    "    for param in module.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "def _init_input_modules(g, ntype, textset, hidden_dims):\n",
    "    # We initialize the linear projections of each input feature ``x`` as\n",
    "    # follows:\n",
    "    # * If ``x`` is a scalar integral feature, we assume that ``x`` is a categorical\n",
    "    #   feature, and assume the range of ``x`` is 0..max(x).\n",
    "    # * If ``x`` is a float one-dimensional feature, we assume that ``x`` is a\n",
    "    #   numeric vector.\n",
    "    # * If ``x`` is a field of a textset, we process it as bag of words.\n",
    "    module_dict = nn.ModuleDict()\n",
    "\n",
    "    for column, data in g.nodes[ntype].data.items():\n",
    "        if column == dgl.NID:\n",
    "            continue\n",
    "        if data.dtype == torch.float32:\n",
    "            assert data.ndim == 2\n",
    "            m = nn.Linear(data.shape[1], hidden_dims)\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            module_dict[column] = m\n",
    "        elif data.dtype == torch.int64:\n",
    "            assert data.ndim == 1\n",
    "            m = nn.Embedding(\n",
    "                data.max() + 2, hidden_dims, padding_idx=-1)\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            module_dict[column] = m\n",
    "\n",
    "    if textset is not None:\n",
    "        for column, field in textset.items():\n",
    "            textlist, vocab, pad_var, batch_first = field            \n",
    "            module_dict[column] = BagOfWords(vocab, hidden_dims)\n",
    "\n",
    "    return module_dict\n",
    "\n",
    "class BagOfWords(nn.Module):\n",
    "    def __init__(self, vocab, hidden_dims):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb = nn.Embedding(\n",
    "            len(vocab.get_itos()), hidden_dims,\n",
    "            padding_idx=vocab.get_stoi()['<pad>'])\n",
    "        nn.init.xavier_uniform_(self.emb.weight)\n",
    "\n",
    "    def forward(self, x, length):\n",
    "        return self.emb(x).sum(1) / length.unsqueeze(1).float()\n",
    "\n",
    "class LinearProjector(nn.Module):\n",
    "    \"\"\"\n",
    "    Projects each input feature of the graph linearly and sums them up\n",
    "    \"\"\"\n",
    "    def __init__(self, full_graph, ntype, textset, hidden_dims):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ntype = ntype\n",
    "        self.inputs = _init_input_modules(full_graph, ntype, textset, hidden_dims)\n",
    "\n",
    "    def forward(self, ndata):\n",
    "        projections = []\n",
    "        for feature, data in ndata.items():\n",
    "            if feature == dgl.NID or feature.endswith('__len'):\n",
    "                # This is an additional feature indicating the length of the ``feature``\n",
    "                # column; we shouldn't process this.\n",
    "                continue\n",
    "\n",
    "            module = self.inputs[feature]\n",
    "            if isinstance(module, BagOfWords):\n",
    "                # Textual feature; find the length and pass it to the textual module.\n",
    "                length = ndata[feature + '__len']\n",
    "                result = module(data, length)\n",
    "            else:\n",
    "                result = module(data)\n",
    "            projections.append(result)\n",
    "\n",
    "        return torch.stack(projections, 1).sum(1)\n",
    "\n",
    "class WeightedSAGEConv(nn.Module):\n",
    "    def __init__(self, input_dims, hidden_dims, output_dims, act=F.relu):\n",
    "        super().__init__()\n",
    "\n",
    "        self.act = act\n",
    "        self.Q = nn.Linear(input_dims, hidden_dims)\n",
    "        self.W = nn.Linear(input_dims + hidden_dims, output_dims)\n",
    "        self.reset_parameters()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        gain = nn.init.calculate_gain('relu')\n",
    "        nn.init.xavier_uniform_(self.Q.weight, gain=gain)\n",
    "        nn.init.xavier_uniform_(self.W.weight, gain=gain)\n",
    "        nn.init.constant_(self.Q.bias, 0)\n",
    "        nn.init.constant_(self.W.bias, 0)\n",
    "\n",
    "    def forward(self, g, h, weights):\n",
    "        \"\"\"\n",
    "        g : graph\n",
    "        h : node features\n",
    "        weights : scalar edge weights\n",
    "        \"\"\"\n",
    "        h_src, h_dst = h\n",
    "        with g.local_scope():\n",
    "            g.srcdata['n'] = self.act(self.Q(self.dropout(h_src)))\n",
    "            g.edata['w'] = weights.float()\n",
    "            g.update_all(fn.u_mul_e('n', 'w', 'm'), fn.sum('m', 'n'))\n",
    "            g.update_all(fn.copy_e('w', 'm'), fn.sum('m', 'ws'))\n",
    "            n = g.dstdata['n']\n",
    "            ws = g.dstdata['ws'].unsqueeze(1).clamp(min=1)\n",
    "            z = self.act(self.W(self.dropout(torch.cat([n / ws, h_dst], 1))))\n",
    "            z_norm = z.norm(2, 1, keepdim=True)\n",
    "            z_norm = torch.where(z_norm == 0, torch.tensor(1.).to(z_norm), z_norm)\n",
    "            z = z / z_norm\n",
    "            return z\n",
    "\n",
    "class SAGENet(nn.Module):\n",
    "    def __init__(self, hidden_dims, n_layers):\n",
    "        \"\"\"\n",
    "        g : DGLHeteroGraph\n",
    "            The user-item interaction graph.\n",
    "            This is only for finding the range of categorical variables.\n",
    "        item_textsets : torchtext.data.Dataset\n",
    "            The textual features of each item node.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        for _ in range(n_layers):\n",
    "            self.convs.append(WeightedSAGEConv(hidden_dims, hidden_dims, hidden_dims))\n",
    "\n",
    "    def forward(self, blocks, h):\n",
    "        for layer, block in zip(self.convs, blocks):\n",
    "            h_dst = h[:block.num_nodes('DST/' + block.ntypes[0])]\n",
    "            h = layer(block, (h, h_dst), block.edata['weights'])\n",
    "        return h\n",
    "\n",
    "class ItemToItemScorer(nn.Module):\n",
    "    def __init__(self, full_graph, ntype):\n",
    "        super().__init__()\n",
    "\n",
    "        n_nodes = full_graph.num_nodes(ntype)\n",
    "        self.bias = nn.Parameter(torch.zeros(n_nodes, 1))\n",
    "\n",
    "    def _add_bias(self, edges):\n",
    "        bias_src = self.bias[edges.src[dgl.NID]]\n",
    "        bias_dst = self.bias[edges.dst[dgl.NID]]\n",
    "        return {'s': edges.data['s'] + bias_src + bias_dst}\n",
    "\n",
    "    def forward(self, item_item_graph, h):\n",
    "        \"\"\"\n",
    "        item_item_graph : graph consists of edges connecting the pairs\n",
    "        h : hidden state of every node\n",
    "        \"\"\"\n",
    "        with item_item_graph.local_scope():\n",
    "            item_item_graph.ndata['h'] = h\n",
    "            item_item_graph.apply_edges(fn.u_dot_v('h', 'h', 's'))\n",
    "            item_item_graph.apply_edges(self._add_bias)\n",
    "            pair_score = item_item_graph.edata['s']\n",
    "        return pair_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1721ccff-dc26-4da0-a869-e2a0cf23f9d2",
   "metadata": {},
   "source": [
    "##### 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c37e3d7-0d67-45a8-85c5-0fca7a98262d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PinSAGEModel(nn.Module):\n",
    "    def __init__(self, full_graph, ntype, textsets, hidden_dims, n_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.proj = LinearProjector(full_graph, ntype, textsets, hidden_dims)\n",
    "        self.sage = SAGENet(hidden_dims, n_layers)\n",
    "        self.scorer = ItemToItemScorer(full_graph, ntype)\n",
    "\n",
    "    def forward(self, pos_graph, neg_graph, blocks):\n",
    "        h_item = self.get_repr(blocks)\n",
    "        pos_score = self.scorer(pos_graph, h_item)\n",
    "        neg_score = self.scorer(neg_graph, h_item)\n",
    "        return (neg_score - pos_score + 1).clamp(min=0)\n",
    "\n",
    "    def get_repr(self, blocks):\n",
    "        h_item = self.proj(blocks[0].srcdata)\n",
    "        h_item_dst = self.proj(blocks[-1].dstdata)\n",
    "        return h_item_dst + self.sage(blocks, h_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7d4979-4b75-4f27-a810-fb59dc06bdfc",
   "metadata": {},
   "source": [
    "##### Evaluation定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4af9907-94aa-40af-abae-5958627e9108",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import dgl\n",
    "import argparse\n",
    "\n",
    "def prec(recommendations, ground_truth):\n",
    "    n_users, n_items = ground_truth.shape\n",
    "    K = recommendations.shape[1]\n",
    "    user_idx = np.repeat(np.arange(n_users), K)\n",
    "    item_idx = recommendations.flatten()\n",
    "    relevance = ground_truth[user_idx, item_idx].reshape((n_users, K))\n",
    "    hit = relevance.any(axis=1).mean()\n",
    "    return hit\n",
    "\n",
    "class LatestNNRecommender(object):\n",
    "    def __init__(self, user_ntype, item_ntype, user_to_item_etype, timestamp, batch_size):\n",
    "        self.user_ntype = user_ntype\n",
    "        self.item_ntype = item_ntype\n",
    "        self.user_to_item_etype = user_to_item_etype\n",
    "        self.batch_size = batch_size\n",
    "        self.timestamp = timestamp\n",
    "\n",
    "    def recommend(self, full_graph, K, h_user, h_item):\n",
    "        \"\"\"\n",
    "        Return a (n_user, K) matrix of recommended items for each user\n",
    "        \"\"\"\n",
    "        graph_slice = full_graph.edge_type_subgraph([self.user_to_item_etype])\n",
    "        n_users = full_graph.num_nodes(self.user_ntype)\n",
    "        latest_interactions = dgl.sampling.select_topk(graph_slice, 1, self.timestamp, edge_dir='out')\n",
    "        user, latest_items = latest_interactions.all_edges(form='uv', order='srcdst')\n",
    "        # each user should have at least one \"latest\" interaction\n",
    "        assert torch.equal(user, torch.arange(n_users))\n",
    "\n",
    "        recommended_batches = []\n",
    "        user_batches = torch.arange(n_users).split(self.batch_size)\n",
    "        for user_batch in user_batches:\n",
    "            latest_item_batch = latest_items[user_batch].to(device=h_item.device)\n",
    "            dist = h_item[latest_item_batch] @ h_item.t()\n",
    "            # exclude items that are already interacted\n",
    "            for i, u in enumerate(user_batch.tolist()):\n",
    "                interacted_items = full_graph.successors(u, etype=self.user_to_item_etype)\n",
    "                dist[i, interacted_items] = -np.inf\n",
    "            recommended_batches.append(dist.topk(K, 1)[1])\n",
    "\n",
    "        recommendations = torch.cat(recommended_batches, 0)\n",
    "        return recommendations\n",
    "\n",
    "\n",
    "def evaluate_nn(dataset, h_item, k, batch_size):\n",
    "    g = dataset['train-graph']\n",
    "    val_matrix = dataset['val-matrix'].tocsr()\n",
    "    test_matrix = dataset['test-matrix'].tocsr()\n",
    "    item_texts = dataset['item-texts']\n",
    "    user_ntype = dataset['user-type']\n",
    "    item_ntype = dataset['item-type']\n",
    "    user_to_item_etype = dataset['user-to-item-type']\n",
    "    timestamp = dataset['timestamp-edge-column']\n",
    "\n",
    "    rec_engine = LatestNNRecommender(\n",
    "        user_ntype, item_ntype, user_to_item_etype, timestamp, batch_size)\n",
    "\n",
    "    recommendations = rec_engine.recommend(g, k, None, h_item).cpu().numpy()\n",
    "    return prec(recommendations, val_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2fb0a8be-7697-4c75-b38c-5de60ff7699b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:17<00:00,  5.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03642384105960265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:15<00:00,  6.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03658940397350993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:15<00:00,  6.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0390728476821192\n"
     ]
    }
   ],
   "source": [
    "g= dataset['train-graph']\n",
    "val_matrix = dataset['val-matrix'].tocsr()\n",
    "test_matrix = dataset['test-matrix'].tocsr()\n",
    "item_texts = dataset['item-texts']\n",
    "user_ntype = dataset['user-type']\n",
    "item_ntype = dataset['item-type']\n",
    "user_to_item_etype = dataset['user-to-item-type']\n",
    "timestamp = dataset['timestamp-edge-column']\n",
    "device = torch.device(device)\n",
    "# Assign user and movie IDs and use them as features (to learn an individual trainable\n",
    "# embedding for each entity)\n",
    "g.nodes[user_ntype].data['id'] = torch.arange(g.num_nodes(user_ntype))\n",
    "g.nodes[item_ntype].data['id'] = torch.arange(g.num_nodes(item_ntype))\n",
    "# Prepare torchtext dataset and Vocabulary\n",
    "textset = {}\n",
    "tokenizer = get_tokenizer(None)\n",
    "textlist = []\n",
    "batch_first = True\n",
    "for i in range(g.num_nodes(item_ntype)):\n",
    "    for key in item_texts.keys():\n",
    "        l = tokenizer(item_texts[key][i].lower())\n",
    "        textlist.append(l)\n",
    "for key, field in item_texts.items():\n",
    "    vocab2 = build_vocab_from_iterator(textlist, specials=[\"<unk>\",\"<pad>\"])\n",
    "    textset[key] = (textlist, vocab2, vocab2.get_stoi()['<pad>'], batch_first)\n",
    "# Sampler\n",
    "batch_sampler = ItemToItemBatchSampler(\n",
    "    g, user_ntype, item_ntype, batch_size)\n",
    "neighbor_sampler = NeighborSampler(\n",
    "    g, user_ntype, item_ntype, random_walk_length,\n",
    "    random_walk_restart_prob, num_random_walks, num_neighbors,\n",
    "    num_layers)\n",
    "collator = PinSAGECollator(neighbor_sampler, g, item_ntype, textset)\n",
    "dataloader = DataLoader(\n",
    "    batch_sampler,\n",
    "    collate_fn=collator.collate_train,\n",
    "    num_workers=num_workers)\n",
    "dataloader_test = DataLoader(\n",
    "    torch.arange(g.num_nodes(item_ntype)),\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collator.collate_test,\n",
    "    num_workers=num_workers)\n",
    "dataloader_it = iter(dataloader)\n",
    "# Model\n",
    "model = PinSAGEModel(g, item_ntype, textset,hidden_dims, num_layers).to(device)\n",
    "# Optimizer\n",
    "opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# For each batch of head-tail-negative triplets...\n",
    "for epoch_id in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_id in tqdm.trange(batches_per_epoch):\n",
    "        pos_graph, neg_graph, blocks = next(dataloader_it)\n",
    "        # Copy to GPU\n",
    "        for i in range(len(blocks)):\n",
    "            blocks[i] = blocks[i].to(device)\n",
    "        pos_graph = pos_graph.to(device)\n",
    "        neg_graph = neg_graph.to(device)\n",
    "        loss = model(pos_graph, neg_graph, blocks).mean()\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    # Evaluate\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        item_batches = torch.arange(g.num_nodes(item_ntype)).split(batch_size)\n",
    "        h_item_batches = []\n",
    "        for blocks in dataloader_test:\n",
    "            for i in range(len(blocks)):\n",
    "                blocks[i] = blocks[i].to(device)\n",
    "            h_item_batches.append(model.get_repr(blocks))\n",
    "        h_item = torch.cat(h_item_batches, 0)\n",
    "        print(evaluate_nn(dataset, h_item, k, batch_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0967dbf9-3bf0-44e2-8304-bb5dc2729dd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39_torch_cpu",
   "language": "python",
   "name": "py39_torch_cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
